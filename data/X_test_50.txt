The locations in the CoNLL data refer only to cities  , states and countries and the entire data set is composed of news articles. They report identifying location mentions in the CoNLL English data set with 87% precision and recall.
For product entities  , however  , Stanford NER tagger is not able to recognize them  , because the annotated training corpora CoNLL  , MUC and ACE do not contain product entities. The CRF classifier is trained on data from CoNLL  , MUC6  , MUC7  , and ACE.
Ten years later  , the Conference on Computational Natural Language Learning CoNLL started to offer a shared task on named entity recognition and published the CoNLL corpus 43 . Starting in 1993  , the Message Understanding Conference MUC introduced a first systematic comparison of information extraction approaches 42.
In addition   , the Automatic Content Extraction ACE challenge 10  , organized by NIST  , evaluated several approaches but was discontinued in 2008. Ten years later  , the Conference on Computational Natural Language Learning CoNLL started to offer a shared task on named entity recognition and published the CoNLL corpus 43 .
His system is a simple application of Kudo's chunking system KM01 that shows the best performance for the CoNLL-2000 shared task. YKM01 for Japanese NE recognition.
Second  , we retrieve top M documents by applying our retrieval model. To extract these patterns  , we use POS tagged training data used for the CoNLL chunking task 18.
Table 8reports the results for the AIDA/CONLL dataset. The performance for this task can be computed via the relation Me.
The first two data sets come from named entity recognition shared task of CoNLL-200214. We applied our algorithm to three data sets in our experiment.
Temporal entities and percents are recognized with the Alembic system 1. On the CoNLL testing data  , this system obtains an F1 measure of 87.50.
On the testing data from the same evaluation exercise  , our chunker obtains an F1 measure of 95.21. We trained the chunker on the corpora provided by the 2000 CoNLL shared task 15.
BAT allows evaluating the performance of different approaches using five datasets  , namely AQUAINT  , MSNBC  , IITB  , Meij and AIDA/CoNLL. * * indicate that the webservice is not meant to be used within scientific evaluations due to unstable backends.
 Stanford-NER: A NER system based on CRF model which incorporates long-distance information 4. It is reported to have achieved the best result F1-Measure of 0.908 on the CoNLL 2003 test set.
Figure 1shows the learning curves of our method  , SMD  , and L-BFGS for the CoNLL-2000 data set. They achieved an F-score of 94.19% with their own extended features.
TagMe outperforms the other annotators in terms of F1 and recall  , while the highest precision is achieved by AIDA-CocktailParty  , at the cost of very low recall. Table 8reports the results for the AIDA/CONLL dataset.
Of the verified matches  , VIAF's 811 results contain 778 distinct URIs  , whilst the 300 results from MusicBrainz have 272. Such people rarely occur in MusicBrainz  , explaining the threefold difference in matches between the two authorities.
MusicBrainz for musical work may provide a higher probability for discovering the correct match than a general knowledge base. Indeed  , linking a work to a specialized database e.g.
For the 1 ,235 persons identified by either the EMO or the ECOLM database  , 5 ,653 matches were retrieved from VIAF and 1 ,103 from MusicBrainz. Instead  , string searches including date strings  , where available were carried out through the VIAF SRU API and MusicBrainz API.
For our experiments we also consider three different empirical datasets which are described next. For generating hypotheses  , we consult the MusicBrainz musicbrainz.org API as described later.
Over the years  , the Freebase community and Google have maintained the knowledge base. The content of Freebase has been partially imported from various sources such as Wikipedia 1  or the license-compatible part of MusicBrainz 30.
6 The aim of these challenges was to answer natural language questions against data from DBpedia and MusicBrainz. We created a corpus of SPARQL queries using data from the QALD-1 5 and the ILD2012 challenges.
This information was used to populate the two versions of the Music Ontology. MusicMash2 then performed a batch process to retrieve information relating to each artist including other artists from MusicBrainz 18   , DBpedia 19 and Audioscrobbler 20 web services.
The content of Freebase has been partially imported from various sources such as Wikipedia 1  or the license-compatible part of MusicBrainz 30. Most non-CVT objects are called topics in order to discern them from CVTs.
We plan to use structured information from MusicBrainz and DBpedia  , together with semi-structured and unstructured information gathered from SoundCloud  , Last.fm  , Twitter and Facebook. At this moment the context information of the collection is scant.
Such people rarely occur in MusicBrainz  , explaining the threefold difference in matches between the two authorities. Many people named in either resource weren't directly involved with the music  , such as dedicatees or printers  , or authors of secondary literature.
cuses on natural language question-answering over selected RDF datasets  , DBpedia and MusicBrainz 14. To copy otherwise  , or republish  , to post on servers or to redistribute to lists  , requires prior specific permission and/or a fee.
The organizers provide a training set encompassing questions and the corresponding SPARQL queries. 6 The aim of these challenges was to answer natural language questions against data from DBpedia and MusicBrainz.
We determine the date of a track by using the Musicbrainz API and looking for the earliest release date available. Finally  , we specify a hypothesis that believes that successive tracks listened to on Last.fm are close regarding their original publication date e.g.  , someone prefers to only listen to 80s songs.
Next  , we calculate the difference between dates of two songs in years—we only consider track pairs for which we can retrieve a date for both tracks through the API. We determine the date of a track by using the Musicbrainz API and looking for the earliest release date available.
In Section 5.2  , we evaluate the anchor-based retrieval methods   , for which we used only the navigational queries in the NTCIR-4 and NTCIR-5 collections. The average numbers of terms were 2.89 for the NTCIR-3 information topics  , 2.39 for the NTCIR-4 informational topics  , 1.39 for the NTCIR-4 navigational topics  , and 1.35 for the NTCIR-5 navigational topics  , respectively.
However  , the differences between AAM and AAMS were not significant for NTCIR-4 and NTCIR-5. The differences of ADM and AAM for NTCIR-4 and NTCIR-5 were significant at the 5% and 1% levels  , respectively.
For runs with posttranslation pseudo-relevance feedback  , the feedback-related parameters were based on calibration runs using CLEF 2001 topics and the baseline EDICT translation dictionary. With the expanded lexicon  , transliteration is validated either with the NTCIR corpus marked " NTCIR "   , with the LDC corpus marked " LDC "   , or with a combination of the NTCIR corpus and the LDC corpus marked " NTCIR+LDC " .
While the NTCIR- 3 collection includes only informational search topics  , the NTCIR-4 collection includes both informational and navigational search topics. Search topics are also in Japanese.
Table 3shows the numbers in detail. All of the OOV terms appearing in <title> and <desc> from both NTCIR-4 and NTCIR-5 are collected.
The average numbers of terms were 2.89 for the NTCIR-3 information topics  , 2.39 for the NTCIR-4 informational topics  , 1.39 for the NTCIR-4 navigational topics  , and 1.35 for the NTCIR-5 navigational topics  , respectively. For each topic  , we used only the terms in the " TITLE " field  , which consists of one or more terms  , as a query.
The average numbers of correct answers were 75.7 for the NTCIR-3 information topics  , 84.5 for the NTCIR-4 informational topics  , 1.79 for the NTCIR- 4 navigational topics  , and 1.94 for the NTCIR-5 navigational topics  , respectively. We used the highly relevant and relevant documents as the correct answers.
We ran the 100 NTCIR-11 Wikipedia formula queries over the large NTCIR-11 arXiv collection to test retrieval speed for the core engine. Retrieval Times.
The objective of the NTCIR Web Task is 'to research the retrieval of Web documents that have a structure with tags and links  , and that are written in Japanese or English'. This paper describes the evaluation method for the Web Retrieval Task in the Third NTCIR Workshop hereafter 'NTCIR Web Task'1  , which is currently in progress.
Research conducted by participants of NTCIR-12 Lifelog Task was presented at the NTCIR-12 Conference  , June 7- 10  , 2016 at Tokyo  , Japan  , and at a parallel workshop at the University of Glasgow. A document collection and information needs of the NTCIR Lifelog test collection are highly individual and multimodal when compared to conventional test collections.
Because these topics target the same document collection  , we can use them to evaluate our query classification. While the NTCIR- 3 collection includes only informational search topics  , the NTCIR-4 collection includes both informational and navigational search topics.
In this evaluation  , we used Mean Reciprocal Rank MRR as the evaluation measure . In Section 5.2  , we evaluate the anchor-based retrieval methods   , for which we used only the navigational queries in the NTCIR-4 and NTCIR-5 collections.
NTCIR-4 and NTCIR-5 CLIR tasks also provide English and Chinese documents  , which are used as the source and target language corpora  , respectively. Usage of correct translations shall help reveal the necessity of translation.
We use NTCIR-4 and NTCIR-5 English-Chinese tasks for evaluation and consider both <title> and <desc> fields as queries. In this section  , we show the effectiveness of our approach for CLIR.
As a result  , we collected 47 topics from the NTCIR-3 collection and 80 informational topics and 168 navigational topics from the NTCIR-4 collection  , respectively  , and a further 841 topics from the NTCIR-5 collection. We used only topics for which at least one highly relevant or relevant document was found.
Make the content available to the public by uploading it to a website like Flickr  , Blogspot  , or YouTube. I don't publish anything. "
Therefore  , differentiating between legitimate and illegitimate web " monsters " becomes a fundamental task of any crawler. For example  , Yahoo currently reports indexing 1.2 billion objects just within its own domain and blogspot claims over 50 million users  , each with a unique hostname.
These sites instruct the browser to delete the cookie client-side  , but do not invalidate the session server-side. To our surprise we found that many popular websites including Linkedin  , Blogspot  , IMDB  , CNN  , MSN  , eBay and Yahoo!  , do not properly invalidate session tokens on logout see Table 2for a complete list.
We ran the following classification experiments: all features separately  , all features combined  , and all features minus the language modeling feature LM combined.
that really matter  , which is based on the feeds subscribed by all the users on Bloglines. It was also interesting to note that even though Blogspot has had serious splog issues 20  , 13  , based on the Bloglines dataset  , it still contributes to a significant portion of the feeds that really matter on the Blogosphere. The three hosts features behave differently.
It was also interesting to note that even though Blogspot has had serious splog issues 20  , 13  , based on the Bloglines dataset  , it still contributes to a significant portion of the feeds that really matter on the Blogosphere. In particular  , there are a number of users who subscribe to Web 2.0 sites and dynamically generated RSS feeds over customized queries.
To our surprise we found that many popular websites including Linkedin  , Blogspot  , IMDB  , CNN  , MSN  , eBay and Yahoo!  , do not properly invalidate session tokens on logout see Table 2for a complete list. To ensure that Session Juggler's secure logout effectively protects users we tested how web sites handle logout requests.
Thus  , she was proficient with the TaxonX Process and the GoldenGATE Editor before the start of the ZooTaxa Project  , and we can rule out any learning effects in this respect. The biologist who participated in the ZooTaxa Project had participated in the Madagascar Project before.
Figure 2graphs the evolvement of the working time per document page over all documents in the ZooTaxa Project. This number will be our reference point.
This project has used the TaxonX Process Section 2 to generate TaxonX markup for all ant-related documents from the ZooTaxa 1 collection  , i.e.  , 30 documents with over 600 pages in total. Second  , we report on the experiences we have gained with ProcessTron in a real-world markup project  , the ZooTaxa Project.
The biologist who participated in the ZooTaxa Project had participated in the Madagascar Project before. The only difference between the two projects  , apart from the fact that the documents are different  , is that ProcessTron was used in the ZooTaxa Project but not in the Madagascar Project.
Our measurements from the Madagascar Project are well suited to serve as the reference point for the ZooTaxa Project. Its effect is easy to measure.
This project was a real-world markup project in the biosystematics domain. After the favorable laboratory experiment  , we have successfully deployed ProcessTron in the ZooTaxa Project.
The main finding is that ProcessTron is well suited to model markup processes. We further report on the insights we have gained when modeling the markup process of the ZooTaxa Project with ProcessTron.
It took users about 3-5 minutes to mark up a document page in the Madagascar Project. Our measurements from the Madagascar Project are well suited to serve as the reference point for the ZooTaxa Project.
Second  , we report on the experiences we have gained with ProcessTron in a real-world markup project  , the ZooTaxa Project. With a statistical significance of over 90%  , it shows that working with ProcessTron yields a speedup of over 50%.
After the favorable laboratory experiment  , we have successfully deployed ProcessTron in the ZooTaxa Project. Example 7 illustrates this for geo-coordinates; we have used the same approach for dates.
We compare the ZooTaxa Project to another markup project that took place in the biosystematics domain as well  , the so-called Madagascar Project 11 . We have measured how the average working time per page has evolved during the project.
Thus  , using or not using ProcessTron is the only variable. Thus  , she was proficient with the TaxonX Process and the GoldenGATE Editor before the start of the ZooTaxa Project  , and we can rule out any learning effects in this respect.
We have measured how the average working time per page has evolved during the project. Using the GoldenGATE Editor and ProcessTron  , a biologist has created TaxonX markup for all ant-related documents from the ZooTaxa collection  , i.e.  , 30 documents with over 600 pages in total.
The graph implies that it took the user only the first 3 documents to get used to working with ProcessTron  , as the working time per page declines significantly with these initial documents. Figure 2graphs the evolvement of the working time per document page over all documents in the ZooTaxa Project.
Due to the familiarization phase that spans the initial 3 documents  , we also give both averages without these documents  , labeled 'after familiarization'  , with i starting at 4 instead of 1 in the above formulas. Note that the number of documents d = 30 in the ZooTaxa Project.
Using data from HomoloGene 3   , homologs in different species were also combined into a single concept. Entries with matching database identifiers were combined.
MUTATIONS  , CELLS  , and NEOPLASMS For these categories  , we used the appropriate subtrees from the MeSH Medical Subject Headings 4  thesaurus. Using data from HomoloGene 3   , homologs in different species were also combined into a single concept.
First  , we evaluated our method against other associative classifiers and rule-based classification methods on 20 UCI datasets 5. UCI datasets are single-label classification problems.
Table 2shows the number of classes  , instances  , and features for each UCI dataset. The UCI datasets are widely used by the Machine Learning community for evaluating different methods.
As the datsets in the UCI repository do not contain tick data  , in order to be able to perform reasonable experiments  , as preprocessing  , we removed the id values from the UCI datasets if present and sorted the records of the UCI datasets in lexicographical order. In particular  , these datasets were: Adult  , Breast Cancer Wisconsin Diagnostic  , Car Evaluation  , Forest Fires 6 and Poker Hand.
The UCI datasets are widely used by the Machine Learning community for evaluating different methods.  Data: The proposed RSS-ECOC method is validated on 24 multiclass datasets from the UCI machine learning repository 15.
Note that the UCI repository contains two versions of the Mushroom database. In a first experiment  , presented in Table l   , we executed the algorithm on the Mushroom database from the UCI Repositoly of Machine Learning Databases 3.
In our computer simulations  , we used the wine recognition database in the UCI Machine Learning Repository http://www.ics .uci .edu~mleam/MLSummary.html. The consequent class C  , was specified for each combination A , ,
The size of the UCI dataset is huge: it includes 13 ,192 projects  , 2 ,127 ,877 Java source files  , and 20 ,449 ,896 methods. The UCI dataset is a collection of open source software that is open to the public on the Web.
The wine data set is a three-class pattern classification problem with 178 patterns and 13 continuous attributes. In our computer simulations  , we used the wine recognition database in the UCI Machine Learning Repository http://www.ics .uci .edu~mleam/MLSummary.html.
Fourteen data sets were selected from the UCI Repository  11. Table 1*
We used benchmark datasets available from the UCI Machine Learning Repository. Datasets.
Comparison of the three quality measures for UCI and CLBME data sets. 4.
This dataset is again obtained from the UCI college repository for datasets 7. Dataset.
The results on this experiment confirm the results on UCI data set. Discussion.
It includes 8 classes and 1459 samples in total.  UCI yeast dataset.
After sorting  , the values of cells in the same columns and subsequent rows were often equal  , this is the key property of tick data that our approach exploits. As the datsets in the UCI repository do not contain tick data  , in order to be able to perform reasonable experiments  , as preprocessing  , we removed the id values from the UCI datasets if present and sorted the records of the UCI datasets in lexicographical order.
The right column Facebook-Search shows the Facebook model AUC on classifying Search users. The middle column Facebook-Facebook shows the AUC values when we trained and tested on the Facebook dataset.
The middle column Facebook-Facebook shows the AUC values when we trained and tested on the Facebook dataset. ics.
We collected data from several Facebook Pages  , each associated with a commercial entity that uses the Facebook page to communicate with its followers. Facebook Pages.
As our first case study we choose Facebook  , a popular social networking site. Facebook.
Similar observations were reported by Teevan et al. using facebook and facebook login interchangeably to navigate to www.facebook.com.
In addition to Facebook categories  , we also experiment with features derived directly from the liked pages. Facebook Likes L.
We also experiment with n-grams n=1 ,2 ,3 derived from individual Facebook page names  , e.g. Facebook n-grams N.
using facebook and facebook login interchangeably to navigate to www.facebook.com. This is particularly common for popular navigational destinations e.g.
This feature family includes 214 features  , one for each Facebook category in the dataset. Facebook Categories F.
We summarize our findings as follows. For Facebook users  , user content refers to Facebook statuses of the users.
By logging in using a Facebook account  , the topic selection process is additionally influenced by the players' likes on Facebook  , to make it more likely to get topics of interest. The Knowledge Test Game is also available on Facebook.
Unsurprisingly  , only about 5% of the papers presented in ICWSM 2013 were about Facebook  , and nearly all of them were co-authored with Facebook data scientists. Though Facebook public pages are available through its API.
We constructed the Facebook dataset by downloading the entire list of 34 ,370 Facebook apps app names  , IDs  , and developer IDs from SocialBakers 26  , a portal providing the usage statistics of various social media. This constitutes the Facebook all dataset.
In the right column  , the models trained based on Facebook data are tested on search query sample. The numbers in the middle column show the AUC of a model trained on Facebook data for predicting the demographics of Facebook users.
Hence  , when computational cost is concerned  , Facebook categories are more favorable. It is finally worth mentioning that the dimensional space of Facebook likes and n-grams is much larger than that of Facebook categories.
In Table 5  , " content creators " refer to Instagram users who have uploaded media to Instagram. Table 5provides an overview of the Instagram data we observe  , including information about media interactions and users involved in the TDV Instagram network.
Among the handful ones  , McCune investigated people's motivations of using Instagram through a survey study of 23 Instagram users McCune 2011 . 1 We are aware of the small section of research on Instagram.
25% of Instagram video uploads failed and 30% of Youtube uploads failed. Overall  , 504 video files were uploaded to Instagram compared to 444 uploaded to Youtube.
To obtain a sizable sample of Instagram pictures   , we collected data for a random set of 5.1M users whose accounts were public. Instagram.
On the other hand  , researchers have applied visualization and cultural analytics on Instagram photos from different cities in the world to trace their social and cultural differences Hochman and Manovich 2013; Silva et al. Among the handful ones  , McCune investigated people's motivations of using Instagram through a survey study of 23 Instagram users McCune 2011 .
The final crawl includes 963 users with a total of 1 ,035 ,840 posts from Twitter using the Twitter API https:Table 1: Words corresponding to the 5 latent topics from Twitter and Instagram posts from Instagram using the Instagram API https://www. Using its API  , we initially crawled a set of 10 ,000 users and pruned users who do not maintain accounts on both the platforms.
Our analysis based on the Instagram data collected using the Instagram API  , is a qualitative categorization of Instagram photos; and a quantitative examination of users' characteristics with respect to their photos. Below  , we first provide details about the dataset we used  , and later discuss how we develop a coding scheme for categorizing the photos and the coding process.
instagram.com/developer/. The final crawl includes 963 users with a total of 1 ,035 ,840 posts from Twitter using the Twitter API https:Table 1: Words corresponding to the 5 latent topics from Twitter and Instagram posts from Instagram using the Instagram API https://www.
The data includes profile information  , photos  , captions and tags associated with photos  , and users' social network that includes friends and followers . Our analysis based on the Instagram data collected using the Instagram API  , is a qualitative categorization of Instagram photos; and a quantitative examination of users' characteristics with respect to their photos.
2 The depicted trends suggest similar likelihoods for both imdb and instagram  , although the popularity of the latter is rising. In particular  , the frequency distributions for instagram  , and imdb are demonstrated in Figure 1 bottom according to Google Trends.
No dependence models were used in our baselines. Mixture parameters were set to P aquaint = 1 for MASSbaseTRM3 and P bignews = 1 for MASSbaseTEE3.
Dashed lines represent external expansion EE using the BIGNEWS corpus. Pseudo-relevance feedback techniques—which do not use any judgments—are shown for reference.
Notice that the target collections trec12 and robust are subsets of BIGNEWS. The first external collection consists of a union of the GIGAWORD collection   , Tipster disks 1  , 2  , 4  , 5  , and HARD 2004 LDC collections   , which we refer to as BIGNEWS.
For reference  , we draw lines representing RM3 and BIGNEWS-EE depicting performance without user feedback. This figure shows performance after k documents judged.
Dotted lines represent pseudo-relevance feedback using only the target corpus RM3. Dashed lines represent external expansion EE using the BIGNEWS corpus.
For each new index  , we plot the effectiveness of using that index for external expansion. We use the BIGNEWS collection as our external corpus and generate subsets of it by randomly dropping documents during indexing.
indicating that the BIGNEWS corpus has reached the point of diminishing returns and that increasing its size is unlikely to provide substantial improvements. Dotted lines represent pseudo-relevance feedback using only the target corpus RM3.
Our second external collection is the GOV2 corpus consisting of a web crawl of the .gov domain. Notice that the target collections trec12 and robust are subsets of BIGNEWS.
Each of these collections are broken down by runs which solely used the external collection EE and those which combined external and target models MoRM. We break our results down by external collection BIGNEWS  , GOV2  , and WEB.
However  , for the robust and wt10g corpora  , the external collection is a better source and will only stop proving useful once the concept density saturates. This further supports our argument that the trec12 corpus itself is a better source of expansion than the BIGNEWS corpus and that external expansion is unlikely to yield any significant improvements.
We begin by observing that both of our pseudo-relevance feedback techniques outperform receiving feedback on at least the top 2 documents when evaluating using amap. For reference  , we draw lines representing RM3 and BIGNEWS-EE depicting performance without user feedback.
Both groups claimed this form of expansion helped  , although it is not clear from the results how much of an improvement was achieved over expanding on TREC volumes 4 and 5 alone. This was shown experimentally by the fact BIGNEWS is a better source for expansion than GOV2  , despite it being a much smaller collection.
In our experiments  , we investigate mixing models from two collections  , AQUAINT  , and BIGNEWS  , a collection of 6 ,160 ,058 TREC newswire articles we had on site. Now that we have a query model that combines evidence from multiple collections  , we can use it for query expansion by adding the k most likely terms from the distribution P w|Q to the original query.
Second  , we study human trails over successive businesses reviewed by users on the reviewing platform Yelp yelp. Yelp dataset.
The Yelp API only returned the URL for the Yelp listing of a particular venue. Duplicate venues were discarded.
Yelp is a social network in which users can evaluate and post reviews about local businesses. Yelp.
The advantages of using Yelp include: 1 places in Yelp are mainly commercial sites  , which is more align with the relevant candidates in this track; 2 the category structure in Yelp is simple and more direct to identify whether the candidate is similar with any example suggestion. We therefore explored Yelp as the data source for identifying candidates.
We depict the results for comparing the hypotheses at interest for our Yelp dataset business reviews in Figure 5b. Yelp.
The first two are extracted from the public Yelp tip dataset 4 . We used datasets from two popular sites Foursquare and Yelp.
Example businesses in Yelp include restaurants  , shopping malls  , beauty & spas  , etc. Yelp is a business review site and has attracted 47 million reviews to local businesses since 2004.
We observed two shortcomings in utilizing the Yelp categories: First  , the Yelp categories could be very specific  , such as " Sushi Bar " for examples where we would also want to consider the query " Restaurant " . Among these  , the Yelp categories seemed to perform the best as to generating a good query from an example.
are typical examples of LBRSs. Yelp  , Google Places  , Foursquare  , etc.
This problem was solved using a mapping from Yelp categories to more general categories. We observed two shortcomings in utilizing the Yelp categories: First  , the Yelp categories could be very specific  , such as " Sushi Bar " for examples where we would also want to consider the query " Restaurant " .
We queried the Yelp API to extract businesses from the targeted contexts and we extracted the following information for each business: Facebook. Yelp is a social network in which users can evaluate and post reviews about local businesses.
The dimension K in our LRPPM- CF model is set as 50 for Yelp datasets and 20 for Amazon datasets to ensure equal model complexity. We conduct top-5 recommendation on both of the Amazon and Yelp datasets.
Yelp is a business review site and has attracted 47 million reviews to local businesses since 2004. In this study  , we are interested in the business rating prediction problem with business review data from Yelp.
WordNet is an English language dictionary and is organized in so called synsets. For this we use two libraries: WordNet 1 and WordNet Domains 2 .
Integrating WordNet Synsets. Then we use WordNet to establish the hierarchy of classes  , because WordNet offers an ontologically well-defined taxonomy of synsets.
WordNet 2.0 consists of 115 ,424 concepts with a total of 203147 labels. be able to treat WordNet as a SKOS vocabulary  , we use a simple schema mapping: WordNet synsets are mapped to SKOS concepts  , WordNet sense labels to SKOS altLabels and WordNet glosses to SKOS definitions .
WordNet is a large lexical database of English. This time we choose WordNet  , a thesaurus.
It allows the user direct access to the full WordNet semantic lexicon. In addition to the WordNet database itself  , we made use of a Perl interface to WordNet: WordNet::QueryData developed by Jason Rennie http://people.csail.mit.edu/u/j/jrennie/public_html/WordNet/.
In WordNet  , words are organized in a hierarchy of synonyms  , hypernyms and hyponyms. As an example  , 2 WordNet senses.
To demonstrate how WordNet assists the user in defining the community schema  ,  we show the following. Using WordNet.
WordNet offers a machine-readable and comprehensive conceptual system for English words. concepts  , or called synsets is WordNet 14.
WordNet senses are represented by sense keys as string literals. MASC includes semantic annotations with FrameNet and WordNet senses 1.
8 use both WordNet and automatically constructed thesauri to expand queries. Instead of using WordNet alone  , Mandala et al.
WordNet contains about 147k unique single as well as compound terms but no stopwords. The dictionaries used are extracted from WordNet 3.0 14 and GNU Aspell 0.60.6 5  , here on referred to as WordNet and Aspell.
More specifically  , we find semantically similar node pairs i  , j and then the nearest common ancestor node a in WordNet. We next present the WordNet Union Algorithm to build CCO from WordNet by combining semantically similar nodes.
However  , the corresponding WordNet version 3.1 is not yet available in RDF. As sense keys are stable across different WordNet versions  , this annotation can be trivially rendered in URIs references pointing to an RDF version of WordNet.
Currently  , a number of initiatives and efforts in the lexical semantic community have been started to extend WordNet to cover multiple languages; see the Global WordNet Association 9 . WordNet offers a machine-readable and comprehensive conceptual system for English words.
Secondly  , we observe a relative 6.0% gain in performance over tweet expansion using WordNet WordNet: 0.679 vs. WE: 0.638  , a particularly encouraging finding given that WordNet is manually curated whereas our paraphrases are automatically generated. MSR: 0.739 vs. WE: 0.638.
However  , DOUBLETAKE detected a significant number of memory leaks in perlbench and gcc of SPEC CPU2006  , which we verified using Valgrind's Memcheck tool. For the SPEC CPU2006 benchmarks  , DOUBLETAKE did not find any heap buffer overflows and useafter-frees   , which is the same result found with AddressSanitizer.
We evaluate DOUBLETAKE's execution time and memory overhead across all of the C and C++ SPEC CPU2006 benchmarks  , 19 in total. For most benchmarks  , DOUBLETAKE's runtime overhead is under 3%.
VM-based recording alone adds additional runtime overhead  , an average of 5% on the SPEC CPU2006 benchmarks. Aftersight monitors applications running in a virtual machine  , which adds some amount of workload-dependent overhead.
We compare DOUBLETAKE with the previous state-ofthe-art tool  , Google's AddressSanitizer 38. We evaluate DOUBLETAKE's execution time and memory overhead across all of the C and C++ SPEC CPU2006 benchmarks  , 19 in total.
For the SPEC CPU2006 benchmarks  , DOUBLETAKE did not find any heap buffer overflows and useafter-frees   , which is the same result found with AddressSanitizer. For the 12 cases without overflows in SAMATE suite  , DOUBLE- TAKE has no false positives.
Aftersight's dynamic analyses are offloaded to unused processors  , which may not be available in some deployments. VM-based recording alone adds additional runtime overhead  , an average of 5% on the SPEC CPU2006 benchmarks.
We evaluate DOUBLETAKE to demonstrate its efficiency  , in terms of execution time  , memory overhead  , and effectiveness at detecting errors. All evaluations on SPEC CPU2006 are exercised with the " ref " reference input set.
All evaluations on SPEC CPU2006 are exercised with the " ref " reference input set. All programs are built as 64- bit executables using LLVM 3.2 with the clang front-end and -O2 optimizations.
We also evaluate DOUBLETAKE on 19 C/C++ benchmarks from the SPEC CPU2006 benchmark suite. This corpus includes 14 cases with buffer overflows and 12 cases without overflows 17.
For heap overflows  , DOUBLETAKE detects all known overflows in one synthetic test case and 14 test cases of SAMATE suite. We also evaluate DOUBLETAKE on 19 C/C++ benchmarks from the SPEC CPU2006 benchmark suite.
Across the SPEC CPU2006 benchmark suite  , Valgrind degrades performance by almost 17× on average geometric mean; its overhead ranges from 4.5× and 42.8×  , making it often too slow to use even for testing see Table 5. An extreme example is the widely-used tool Valgrind .
Flixster 9 : Flixster is a social network for rating movies. There is no a priori " correct " way.
Flixster contains ratings on movies. We conduct experiments on three real-life  , publicly available rating datasets  , namely: Ciao 1   , Epinions 1   , and Flixster 2 .
The Flixster data set has been crawled by Jamali and Ester 5 and has been made publicly available for the research community 13 . The social relations in Flixster are friendship relations and undirected.
Note that social relations in Flixster are undirected. Flixster is a social networking service in which users can rate movies and add some users to their friend list to create a social network.
I flixter.com: Flixster is a social networking site for movie fans. I facebook.com: Online social network website.
Ciao and Epinions both contain ratings on various categories such as books  , electronics  , movies  , etc. Flixster contains ratings on movies.
We conduct experiments on three real-life  , publicly available rating datasets  , namely: Ciao 1   , Epinions 1   , and Flixster 2 . Datasets.
Friendship. Note that this  , as well as the earlier conclusion  , generally holds for all the annual subsets of Flixster datasets.
in the dominated context rating for Flixster-dominant  , where a context rating is dominated by another one social. The results are for both Epinions and Flixster for the case that a context rating is dominated by another one social or trust.
Flixster is a social networking service in which users can rate movies and add some users to their friend list to create a social network. We use a real life dataset in our experiments which is derived from Flixster 7 website.
I formspring.me: " Formspring helps people find out more about each other by sharing interesting & personal responses " 15. I flixter.com: Flixster is a social networking site for movie fans.
In our experimental results we refer to these data sets as Epinions-dominant and Flixster-dominant. Similarly in Flixster  , if we only consider the movies from the " Horror" genre  , then the ratings 245K will be dominated by the social relations 26M.
We perform 5-fold cross validation in our experiments. While the machine had 64GB memory  , our implementation of HETEROMF consumed only 1GB memory on the largest dataset Flixster.
Items are described by their relevance over 3 topics. The experiment is performed on FLIXSTER  , using 50 different items  , and averaging the results.
This dataset contains 13 ,000 users with 192 ,400 directed edges between them. We obtained the Flixster dataset from Goyal et al's work 18  , 10.
The first reason is the volume of information in Libra. The choice of Libra as a source of citation data has two main reasons.
Section 5 describes the experiments we have done with PopRank and Libra to evaluate the effectiveness of our approach. In Section 4  , we briefly introduce our Libra paper search.
Libra Academic Search  , or simply Libra  , is a digital library focused on the Computer Science field that allows free search of bibliographic data. The results of this crawling is summarized in Table 2.
LIBRA climbs a series of pegs and presents some challenging planning problems such as tight physical constraints. Here the action module planning methodology is applied to the MIT LIBRA laboratory climbing robot to study its practical validity 12.
It currently has more than 3 million documents organized according to the publication venue. Libra Academic Search  , or simply Libra  , is a digital library focused on the Computer Science field that allows free search of bibliographic data.
The fitness function used for LIBRA is: A well designed fitness function is crucial.
libra As the control parameters are difficult to set  , in this paper  , self-regulating control parameters mechanism is used.
The maximum torque required was approximately 1900 oz-in  , occurring at the location of LIBRA in Figure 9a. A human was asked to derive a plan to move the LIBRA from point A to point B in the H task shown in Figure 9while minimizing joint torques.
Libra collects Web information for all types of objects in the research literature including papers  , authors  , conferences  , and journals. Based on the PopRank model  , we have been developing Libra  , an object-level Web search prototype  , to help scientists and students locate research materials.
Ontology negotiation is an attempt to move the LIBRA paradigm into human-to-computer or computer-to-computer com- munications. The idea grew out of our work on the LIBRA methodology  , which is a set of techniques for fostering reuse-oriented knowledge sharing between people Bailin et al  , 2000.
With the concept of Web objects  , the search results of Libra could be a list of papers with explicit title  , author  , and conference proceedings. We have been developing another object-level vertical search system call Libra Academic Search http://libra.msra.cn to help researchers and students locate information for scientific papers  , authors  , conferences  , and journals.
The reasons include: 1 Rexa only supports paper search and author search  , thus cannot make use of dependencies between authors  , papers  , and conferences; 2 Libra supports search of the three objects. Table 3 show that our approach outperforms the existing academic search systems Libra +3.4% in terms of average MAP and Rexa +15.6% by average MAP.
Moreover  , these groups are the ones with more complete information about citations in Libra. two groups are very homogeneous and the conference categories well defined.
Since robots usually work in a dynamic and unpredictable environment  , it is impossible to acquire full knowledge of the world's geometry. libra
In Section 4  , we briefly introduce our Libra paper search. Section 3.2 describes how we select a subgraph of the entire object link graph.
It can be seen that the localization error quickly converges to near zero at the beginning of the navigation. Fig.6shows a typical example  , obtained with the dataset " fr079 "   , the SC feature  , the algorithm ver 2.1 and the parameters N = 10  , 000  , c w = 2.0.
Sina Corporation 10 launched this service in August 2009. Sina Weibo is a Chinese microblogging service.
Sina Weibo is a Chinese microblogging service. Figure 2gives a
Sina Weibo allows up to 140 Chinese characters per post  , which means that one Sina Weibo message could contain more information than in Twitter. Moreover  , Weibo officially supports posting images  , videos  , music via local files or on-line URLs  , and locations.
Furthermore  , unlike Twitter  , Chinese microblogging sites normally support a large amount of " verified accounts " under their real name systems i.e.  , users must link their national identification number to a microblogging account  , for people to declare their professional identities and titles. Sina Weibo allows up to 140 Chinese characters per post  , which means that one Sina Weibo message could contain more information than in Twitter.
Table 1shows the statistics of the six collections. It contains six collections includes: TIPSTER Volume 1-3  , ClueWeb09-T09B  , Tweets2011  , SogouT 2.0  , Baidu Zhidao  , Sina Weibo.
The original question is then pushed to each user via the Sina Weibo Messaging API as a normal Weibo post. For example  , a high voting weight is given to a post that has the following characteristics: a check-in message  , posted recently  , from a  Based on the ranking result  , the Communication Module selects at most fifteen Weibo users to answer the question.
Table 1 shows the statistics of the six collections. We evaluate the proposed method with six corpora TIP- STER Volume 1-3 1   , ClueWeb09-T09B 2   , Tweets2011 Twitter 3   , SogouT 2.0 4   , Baidu Zhidao 5   , Sina Weibo 6 .
 reaches to convergence 12: return R ′ ij = UiVj from Sina Weibo which allows users to follow other users and receive the messages from the followed users similar to Twitter. The sparsity problem is very serious in the retweeting matrix.
In addition  , Weibo users can comment on a post  , which means that the comments will be inserted into the original post rather than into their own timelines. The basic functionalities of Sina Weibo are similar to those of Twitter 11 : messaging  , re-posting  , direct messageing  , mention @  , hashtag #  , and more.
It contains six collections includes: TIPSTER Volume 1-3  , ClueWeb09-T09B  , Tweets2011  , SogouT 2.0  , Baidu Zhidao  , Sina Weibo. The first one is used in 38 CRD for short in the following.
The post body describes that this request is a real request from a real person who needs help  , rather than a spam message. The original question is then pushed to each user via the Sina Weibo Messaging API as a normal Weibo post.
Because Twitter and Weibo serve different communities  , users in different networks may be interested in different kinds of topics. Similarly  , the world's second-largest microblogging system  , Sina Weibo  , boasted more than 176 million active users by 2015  , but because the system's default language is Chinese  , almost all its users come from China or speak Chinese.
Since we lacked labeled data for BibServ  , we used the parameters learned on Cora with appropriate modifications to perform inference on BibServ. This left us with a total of 15 ,954 decisions.
Overall  , even though BibServ and Cora have quite different characteristics  , applying the parameters learned on Cora to BibServ still gives good results. On AUC in venues  , MLNG+C+T and MLNB are the two best performers.
Word stemming was used to identify the variations of the same underlying root word both in Cora and BibServ. Since we lacked labeled data for BibServ  , we used the parameters learned on Cora with appropriate modifications to perform inference on BibServ.
Collective inference is clearly useful  , except for AUC on venues  , where the results are inconclusive. Overall  , even though BibServ and Cora have quite different characteristics  , applying the parameters learned on Cora to BibServ still gives good results.
Table 2 shows the CLL and AUCs for the various algorithms on the BibServ dataset. The results reported below are over these hand-labeled pairs.
BibServ.org is a publicly available repository of about half a million pre-segmented citations. Word stemming was used to identify the variations of the same underlying root word both in Cora and BibServ.
As in the case of Cora  , for citations and authors  , the best-performing models are the ones involving collective inference features. Table 2 shows the CLL and AUCs for the various algorithms on the BibServ dataset.
In the BibServ domain  , because of the absence of labeled data  , the naive Bayes model could not be learned. In the Cora domain  , we performed five-fold cross validation .
The weights of the inverse predicate equivalence rules for each word were fixed in proportion to the IDF of the word. In the BibServ domain  , because of the absence of labeled data  , the naive Bayes model could not be learned.
We experimented on a subset of 10 ,000 records extracted randomly from the user-donated subset of BibServ  , which contains 21 ,805 citations. It is the result of merging citation databases donated by its users  , CiteSeer  , and DBLP.
Since we did not have labeled data for BibServ  , we handlabeled 300 pairs each for citations and venues  , randomly selected from the set where the MAP prediction for at least one of the algorithms was different from the others. Collective inference is clearly useful  , except for AUC on venues  , where the results are inconclusive.
The eBay feedback ratings are added up to determine a client's overall reputation according to the eBay scoring function. Users with higher eBay scores are considered to have higher reputations.
For example  , the auction alerts provided by eBay are powered by NetMind's monitoring technology. These customers include eBay  , RedHerring and Siemens.
The dynamic nature of inventory poses unique challenges for search in a marketplace like eBay. For our study we used click-stream logs from eBay.
There are about 100 million active users on eBay. In this paper  , we study the resale market from data obtained from a leading online marketplace vendor  , i.e.  , eBay.
The dataset analyzed comes from two sources: eBay Sojourner logs and eBay Data Warehouse. The data we employ can be trusted authoritatively.
As the largest online marketplace  , the distribution of eBay items follows the long tail theory. Above arguments are also true for eBay.
The eBay taxonomy comprises of around 25 main categories. We classified around 284 buzz recommendations from the system using eBay taxonomy.
Search on eBay is predominantly boolean. The dynamic nature of inventory poses unique challenges for search in a marketplace like eBay.
ebay  , provides an advertising commissioner   , e.g. An Internet advertiser  , e.g.
10for extending the eBay example of Fig. See Fig.
This overall reputation can be used by potential buyers and sellers to determine whether or not to purchase/sell an item from/to another eBay user. The eBay feedback ratings are added up to determine a client's overall reputation according to the eBay scoring function.
This section and the following explore possible correlations between Facebook information and eBay purchases. It is worth noting that these results refer to our dataset of 13K Facebook-connected eBay users  , and may not generalize to the general population of eBay users or to the whole ecommerce spectrum.
As regards the set of product categories we want to predict  , we use the 35 " eBay metacategories " or " eBay categories " for brevity which form the first and most general layer of the eBay product taxonomy. We will make use of category information throughout the paper.
Examples of meta-categories are Books and Home & Garden . As regards the set of product categories we want to predict  , we use the 35 " eBay metacategories " or " eBay categories " for brevity which form the first and most general layer of the eBay product taxonomy.
The IIT CDIP 1.0 collection consists of 6 ,910 ,192 document records in the form of XML elements. The IIT CDIP project subsequently reformatted the metadata and OCR  , combined the metadata with a slightly different version obtained from UCSF in July 2005  , and discarded some documents with formatting problems  , to produce the IIT CDIP 1.0 collection 8.
The 2008 Legal Track used the same collection as the 2006 and 2007 Legal Tracks  , the IIT Complex Document Information Processing CDIP Test Collection  , version 1.0 referred to here as " IIT CDIP 1.0 "  which is based on documents released under the tobacco " Master Settlement Agreement " MSA. The IIT CDIP 1.0 collection is based on a snapshot ,
The 2007 Legal Track used the same collection as the 2006 Legal Track  , the IIT Complex Document Information Processing CDIP Test Collection  , version 1.0 referred to here as " IIT CDIP 1.0 "  which is based on documents released under the tobacco " Master Settlement Agreement " MSA. See the 2006 TREC Legal Track overview paper for additional details about the IIT CDIP 1.0 collection 3.
IIT CDIP 1.0 has had strengths and weaknesses as a collection for the Legal Track. The IIT CDIP 1.0 collection consists of 6 ,910 ,192 document records in the form of XML elements.
The IIT CDIP 1.0 collection consists of 6 ,910 ,192 document records in the form of XML elements. The IIT CDIP project subsequently reformatted the metadata and OCR  , combined the metadata with a slightly different version obtained from UCSF in July 2005  , and discarded some documents with formatting problems  , to produce the IIT CDIP 1.0 collection .
These problems are being investigated in ongoing work by the IIT CDIP project. 2006.
See the 2006 TREC Legal Track overview paper for additional details about the IIT CDIP 1.0 collection 9. The IIT CDIP 1.0 collection consists of 6 ,910 ,192 document records in the form of XML elements  , that contain both metadata and the results of Optical Character Recognition OCR.
The MSA settled a range of lawsuits by the Attorneys General of several US states against seven US tobacco organizations five tobacco companies and two research institutes. As the Legal Track test collection we chose the IIT CDIP Test Collection  , version 1.0 which we will refer to as " IIT CDIP 1.0 "  which is based on documents released under the tobacco " Master Settlement Agreement " MSA.
The IIT CDIP 1.0 collection consists of 6 ,910 ,192 document records in the form of XML elements  , that contain both metadata and the results of Optical Character Recognition OCR. The IIT CDIP 1.0 collection is based on a snapshot  , generated between November 2005 and January 2006  , of the MSA subcollection of the LTDL.
In this training run  , we extracted all documents that were judged for any of the 2007 queries from the full IIT CDIP collection. To consider this question  , we tested our distributed EDLSI with folding-up system on a subset of the IIT CDIP corpus using the TREC 2007 queries.
The collection statistics are shown in Table 1. We run our experiments on the IIT CDIP 1.0 dataset created for TREC Legal track.
It consists of 6 ,910 ,192 business records from US tobacco companies and research institutes. Legal Track 2006 and 2007 both use the IIT CDIP collection 1 .
We run our experiments on the IIT CDIP 1.0 dataset created for TREC Legal track. The proximity operators like " w/15 " are also dropped.
of Col. "   , and " District of Columbia " in different Data.gov datasets  , so we preserve such labels using skos:altLabel. For example  , Washington DC was referred by " DC "   , " Dist.
For example  , many US government datasets published at Data.gov are organized as data tables. We generate LGD raw data by minimizing human involvement and preserving the structure and content of the original data.
Following is an example data table extracted from the Data.gov catalog dataset 92. Here  , each table entry row has a unique URI  , and each header cell column name yields an auto-generated RDF property.
These labels  , if used within certain context  , can uniquely identify entities; therefore  , developers can use the labels to establish connections across different datasets. of Col. "   , and " District of Columbia " in different Data.gov datasets  , so we preserve such labels using skos:altLabel.
The Tetherless World Constellation TWC Data-gov corpus  , yielded by the Data-gov project at RPI  , publishes LGD over 5 billions RDF triples converted from hundreds of Data.gov datasets covering a wide range of topics e.g. Linked Government Data LGD 2 is introduced to establish links across distributed government datasets and thus facilitate data integration.
With just the LGD raw data converted from Data.gov  , we built mashups that links data via existing Web data APIs to better exhibit the raw data  , including the following applications: Worldwide Earthquakes 1 . This allows users to easily visualize the location and magnitude of earthquakes using a graphical interface.
The Data-gov project 1 investigates the role of semantic web technologies  , especially linked data  , in producing  , enhancing and utilizing government data published on Data.gov and other websites. developers can contribute cross-dataset mappings which are seldom provided by the original LGD datasets; and iii keeping enhancement incremental allows existing LGD applications to remain unchanged when new enhancement data are added.
Simply opening up government data on the Web does not guarantee the data will be ready for mashups. The Data-gov project 1 investigates the role of semantic web technologies  , especially linked data  , in producing  , enhancing and utilizing government data published on Data.gov and other websites.
This uses raw data from the Department of Interior to get the latitude  , longitude and descriptions about earthquakes observed in the past seven days. With just the LGD raw data converted from Data.gov  , we built mashups that links data via existing Web data APIs to better exhibit the raw data  , including the following applications: Worldwide Earthquakes 1 .
Government data catalogues  , such as data.gov and data.gov.uk  , constitute a corner stone in this movement as they serve as central one-stop portals where datasets can be found and accessed. Open data is an important part of the recent open government movement which aims towards more openness  , transparency and efficiency in government.
US government spending  , energy usage and public healthcare. The Tetherless World Constellation TWC Data-gov corpus  , yielded by the Data-gov project at RPI  , publishes LGD over 5 billions RDF triples converted from hundreds of Data.gov datasets covering a wide range of topics e.g.
As a growing number of organizations and companies e.g.  , Europeana  , DBpedia  , data.gov  , GeoNames adopt the Linked Data practices and publish their data in RDF format  , going beyond simple SPARQL endpoints  , to provide more advanced   , effective and efficient search services over RDF data  , has become a major research challenge. For instance  , an effective RDF keyword search method should treat RDF properties edges as first-class citizens  , since properties may provide significant information about the relations between the entities being searched.
However  , working with this data can still be a challenge ; often it is provided in a haphazard way  , driven by practicalities within the producing government agency  , and not by the needs of the information user. Government data catalogues  , such as data.gov and data.gov.uk  , constitute a corner stone in this movement as they serve as central one-stop portals where datasets can be found and accessed.
When data.gov started publishing RDF  , large numbers of datasets were converted using a simple automatic algorithm  , without much curation effort  , which limits the practical value of the resulting RDF. The pioneering LGD efforts in the U.S. and U.K. have shown that creating highquality Linked Data from raw data files requires considerable investment into reverse-engineering  , documenting data elements  , data clean-up  , schema mapping   , and instance matching 8 ,16.
In the U.K.  , RDF datasets published around data.gov.uk are carefully curated and of high quality  , but due to limited availability of trained staff and contractors  , only selected high-value datasets have been subjected to the Linked E. Simperl et al. When data.gov started publishing RDF  , large numbers of datasets were converted using a simple automatic algorithm  , without much curation effort  , which limits the practical value of the resulting RDF.
Each image is segmented into about 10 regions  , and a feature is extracted from each region. Corel: This dataset contains features extracted from 66 ,000 Corel stock images.
COREL is not a good general-purpose  , multiple domain retrieval system. For the COREL prototype  , we limited our attention to the section dealing with things and to the concept of usufruct.
Corel: This dataset contains features extracted from 66 ,000 Corel stock images. These datasets are used in a previous work 12  to evaluate LSH  , and the reader is referred to that study for detailed information on dataset con- struction.
Captions and automatic speech recognition transcripts provide more linguistic clues than the keyword annotation for images in the Corel Data set. Our experiments and observations are based on the Corel Data set.
The CORBL System COREL is an experimental prototype information retrieval system designed to operate in the legal do.main. 2.
 Corel image collection: 528 images distributed over 30 categories. Each vector space is considered as a view.
We would like to highlight several interesting points: 1. Several experiments on Corel database have been systematically performed.
Note that the TREC training set is significantly larger than the COREL  , and hence performance continues to increase even up to 20-Gaussian mixture densities. It confirms the earlier conclusion based on the COREL data that up to a certain point  , increasing model complexity improves performance.
Corel  , there are none that are event photos from personal photo libraries. While there are publicly available datasets e.g.
The example below show~ how COREL exploits the conceptual nature of the legal domam. Each structure represents a legal concept.
The most significant feature of the first version of the base figure 2 is the usage of a network computer  , namely the Corel Netwinder. The Corel netwinuer is very compad  , offers many default interfaces  , and it has a very low power-consumption.
Several experiments on Corel database have been systematically performed. Especially  , out of the three active learning methods SVMactive  , AOD  , LOD  , our proposed LOD algorithm performs the best.
The current COREL system is small. This endeavour is a major undertaking and would constitute a large research project by itself.
For Google Images  , we have included 9000 natural images. For Corel image database  , we have also included 2800 images for natural scenes.
The dataset consists of 5 ,000 images from 50 Corel Stock Photo cds. This also allows us to compare the performance of models in a strictly controlled manner.
For both ClueWeb collections  , the entity context model with window size 8 performs well. This is in not the case for the ClueWeb collections.
Furthermore  , in some cases for ClueWeb  , the Wikipediabased predictors outperform the corpus-based predictors. For the ClueWeb tracks  , the integration often posts statistically significant improvements.
Similar pattern can be observed on the TREC4-Kmeans. We only report b ClueWeb-Wiki Figure 2: High-precision Results of TREC123 and ClueWeb-Wiki with Different Number of Latent Variables TREC123 and ClueWeb-Wiki for these experiments  , and try different configuration of K = {1  , 3  , 5  , 10}.
These findings attest to the redundancy of feature functions when employing ClustMRF for the non-ClueWeb settings and to the lack thereof in the ClueWeb settings. Yet  , there was no statistically significant performance decrease for any of the top-10 feature functions for the non-ClueWeb settings.
We only report b ClueWeb-Wiki Figure 2: High-precision Results of TREC123 and ClueWeb-Wiki with Different Number of Latent Variables TREC123 and ClueWeb-Wiki for these experiments  , and try different configuration of K = {1  , 3  , 5  , 10}. In this section  , we discuss the experimental results when the number of latent variable K changes.
The second step is optional. We made an additional run where Wikipedia pages without Clueweb links are retained in the ranking and a dummy Clueweb page is inserted in the result to make them the right format.
Similarity graphs of ClueWeb and YMusic are denser and Stage 1 can be too aggressive in making the light partitions absorb too much comparison computation. Stage 2 contributes about 4% for Twitter  , 12% in ClueWeb  , and 10% for YMusic .
ClueAF and ClueBF are two additional experimental settings created from ClueWeb following previous work 6. For the ClueWeb Web collection both the English part of Category A ClueA and the Category B subset ClueB were used.
This approach give satisfactory results in the absence of Clueweb dataset. We then used Clueweb's URL-DocId mapping to find urls of target entities present in Clueweb dataset and present corresponding DocID as final results.
Since Clueweb is a subset of actual web  , SEAL could not answer 2 queries which were answered using Google API. Run-1 was using the Google API and Run-2 was using Clueweb API.
Similar to TREC4-Kmeans  , we applied clustering algorithm 33 to divide the dataset into 100 collections.  Wikipedia-100col-Kmeans ClueWeb-Wiki: 100 collections were created from the Wikipedia dataset of the ClueWeb 13.
It is desired to construct a new dataset based on ClueWeb for experiments in federated search. For distributed environment in a different problem setting  , ClueWeb has been used in 15.
The Clueweb Online Services 3 were used to retrieve the top 100 documents for each query  , using the same ranking functions and version of Indri described in Section 3. Results are reported over the Clueweb category B collection.
 Wikipedia-100col-Kmeans ClueWeb-Wiki: 100 collections were created from the Wikipedia dataset of the ClueWeb 13. This testbed comes with 50 queries TREC topics 201-250 with judgments.
For the non-ClueWeb settings  , the feature functions  , in descending order of attributed importance  , are: stdv-qsim  , max-sw2  , geo-qsim  , min-sw2  , max-sw1  , max-qsim  , min-dsim  , geo-sw2  , min-icompress  , min-qsim  , min-sw1  , geo-icompress  , max-dsim  , geo-dsim  , max-icompress  , geo-entropy  , min-entropy  , geo-sw1  , max-entropy. Hence  , we perform the analysis separately for the ClueWeb and non- ClueWeb AP  , ROBUST  , WT10G  , and GOV2 settings.
To illustrate this  , properties in Freebase often have a reverse property that is used in order to be able to traverse the Freebase graph easily in both directions . Freebase encodes its knowledge far more redundantly than Wikidata.
However  , in BingQ  , questions were collected from search engine logs and the knowledge required to answer them does not necessarily exist in Freebase. Questions in WebQ were coined on Freebase and are guaranteed answerable by Freebase.
Freebase. We compare three different methods.
Many of the Freebase entities are contained in Wikipedia. We use a combination of Wikipedia and Freebase as knowledge bases in these experiments.
We use the Freebase schema to map between the two knowledge bases using attributes: /wikipedia/en_title and /wikipedia/en. Many of the Freebase entities are contained in Wikipedia.
Freebase represents the ontology based on the domains and categories given in the Freebase website. NoOntology represents the baseline approach 7   , where no ontology is used.
YAGO represents an external ontology known as YAGO. Freebase represents the ontology based on the domains and categories given in the Freebase website.
In contrast  , our algorithm finds both exact matches and similar matches. b Freebase a DBLP b Freebase 27 finds exact matches in billion-node graphs.
Questions in WebQ were coined on Freebase and are guaranteed answerable by Freebase. These significant differences partly result from the evaluation set construction process.
Freebase allows users to create views using a dedicated query language called MQL. Our query set was based on the user-defined views of Freebase.
Freebase  , a collaborative knowledge graph  , contains more than 43.9 million entities  , interconnected by 2.4 billion facts. Information networks such as Freebase 12 have become massive in recent years.
Edgar companies link to DBpedia companies via Freebase. Finance API.
To use the same evaluation set for the Freebase graph  , we mapped every DBpedia resource from Table 3to a Freebase resource using the owl:sameAs link to Freebase included in the description of the resource 5 . This means that in all these cases there were no common links  , and thus f x  , y was zero.
As shown in Table 4  , the NSWD and its variants also maintain a semantically correct ordering w.r.t. To use the same evaluation set for the Freebase graph  , we mapped every DBpedia resource from Table 3to a Freebase resource using the owl:sameAs link to Freebase included in the description of the resource 5 .
When Google publicly launched Freebase back in 2007  , Freebase was thought of as a " Wikipedia for structured data " . 4 Freebase has as of March 31  , 2015  , gone read-only  , i.e.  , the website no longer accepts edits and the MQL write API was retired.
As shown in Figure 4  , our proposed framework performs better than the model-based clustering approaches. Therefore  , our proposed framework are effective for handling unbalanced datasets such as RCV-Industry and RCV-Topic datasets.
All the constraints are consumed in the " Explore " step to find a neighborhood to initialize each cluster. The reason is that RCV-Industry and RCV-Topic datasets are unbalanced datasets with a large number of clusters.
Therefore  , our proposed framework are effective for handling unbalanced datasets such as RCV-Industry and RCV-Topic datasets. In our proposed framework  , it is not necessary for every cluster to be assigned a set of constraints .
The reason is that RCV-Industry and RCV-Topic datasets are unbalanced datasets with a large number of clusters. 1  does not show better performance with increasing number of constraints.
For the RCV-Industry datasets  , only 39 clusters can be assigned a neighborhood with 1 ,400 constraints . There are nearly no constraints left for the " Consolidate " step.
We evaluate performance on several real-world text data sets. Additional data sets were derived from the Reuters RCV
The RCV is capable of log polar sampling  , multi resolutional pyramid sampling  , and any number of other sampling schemes that can be devised. While there are many excellent features in all these systems  , the prime advantage of the RCV is flexibility.
The disadvantage of the RCV relative to other systems is that this flexibility  , combined with the use of a general purpose microcontroller  , results in a slower system than what dedicated hardware can deliver. The RCV is capable of log polar sampling  , multi resolutional pyramid sampling  , and any number of other sampling schemes that can be devised.
RCV: The overall performances of different methods except Simple Margin on the RCV data are presented in Table 2. In contrast  , the TED methods set the optimization goal as finding data to well preserve the rest of other data  , and are thus unlikely to find those outliers.
For both scenarios we use LSH sketches specific to the similarity measure of the data points. m RCV ,cosine n RCV ,cosine o WikiLinks ,cosine p WikiLinks ,cosine explicit representation of the data points and ii instead of the original dataset only a small sketch of the data is available and similarity needs to be approximately estimated.
This flexibility allows the RCV to be used in a large number of applications  , primarily as " quick vision " for prototype mobile robots. The RCV is capable of arbitrarily sampling an image  , grabbing only those portions of the visual field that may be of interest in a given situation.
Data Preprocessing. We experimented with several background corpora: the Brown corpus  , the Gütenberg corpus  , the Reuters RCV-1 corpus  , as well as combinations.
Otherwise  , an interrupt to robot processes is generated. If a message of type CR is received  , a variable CR-RCV  , shared with the transmitting service  , is set.
Table 4shows the test set error rates for the four classification datasets. On MCAT and RCV  , we chose 500 to keep the training times reasonble.
The next generation of the RCV is currently being designed and will make use of newer chipsets designed for video digitization  , such as the Samsung KS0116 series which is a 3-chip set capable of decoding  , digitizing and encoding color NTSC  , SVHS  , and PAL video. The recent reduction in price of these VLSI chips will keep the cost of the RCV near the current level  , while greatly enhancing its capabilities.
Prominent examples of Dcat adopters include data.gov.uk and semantic.ckan.net. Dcat has been adopted by a number of government catalogues.
It is used by data.gov.uk to track provenance of data published by the U.K. government. OPMV is a lightweight provenance vocabulary based on OPM 18.
Currently   , Dcat development is pursued under the W3C Government Linked Data Working Group 6 . Prominent examples of Dcat adopters include data.gov.uk and semantic.ckan.net.
The core ontology of OPMV can be extended by defining supplementary modules . It is used by data.gov.uk to track provenance of data published by the U.K. government.
Government data catalogues  , such as data.gov and data.gov.uk  , constitute a corner stone in this movement as they serve as central one-stop portals where datasets can be found and accessed. Open data is an important part of the recent open government movement which aims towards more openness  , transparency and efficiency in government.
However  , working with this data can still be a challenge ; often it is provided in a haphazard way  , driven by practicalities within the producing government agency  , and not by the needs of the information user. Government data catalogues  , such as data.gov and data.gov.uk  , constitute a corner stone in this movement as they serve as central one-stop portals where datasets can be found and accessed.
In the U.K.  , RDF datasets published around data.gov.uk are carefully curated and of high quality  , but due to limited availability of trained staff and contractors  , only selected high-value datasets have been subjected to the Linked E. Simperl et al. When data.gov started publishing RDF  , large numbers of datasets were converted using a simple automatic algorithm  , without much curation effort  , which limits the practical value of the resulting RDF.
Like SimilarWeb  , StumbleUpon also presents the users with visual aids either thumbnails or images from the web page to help users find interesting pages Figure 1. Another tool suggesting web pages to users is StumbleUpon 16.
23 used tags to identify users across social tagging systems such as Delicious  , StumbleUpon and Flickr. Iofciu et al.
From this data  , we identified distinct interests in the corresponding communities. Our analysis was based on time series that were collected from Google Insights  , Delicious  , Digg  , and StumbleUpon.
Another tool suggesting web pages to users is StumbleUpon 16. In addition to the thumbnail preview  , SimilarWeb also shows the page title and URL below the thumbnail.
Toolbar  , and Windows Live Toolbar  , as well as those targeted at users with specific interests e.g.  , StumbleUpon and eBay Toolbar. Examples of popular toolbars include those affiliated with search engines e.g.  , Google Toolbar  , Yahoo!
Our analysis was based on time series that were collected from Google Insights  , Delicious  , Digg  , and StumbleUpon. In this paper  , we investigated the temporal dynamics and infectious properties of 150 famous Internet memes.
The usefulness of miniature versions of web pages -web page thumbnails -along with or instead of textual summaries in presenting search results has been a topic of research for a decade. Like SimilarWeb  , StumbleUpon also presents the users with visual aids either thumbnails or images from the web page to help users find interesting pages Figure 1.
Other utilities like Delicious  , StumbleUpon usually need an explicit user actions to mark the page on the user profile and to rate the page. Second  , the extent to which they are aimed at providing recommended content and views into browsing trends are challenged by the lack of tightly organized  , closed communities .
8 CrunchBase consists of structured information about organizations including companies  , people  , products  , investments  , and several other items  , and is edited by a Web community. KB: As KB we used CrunchBase 7 in combination with DBpedia.
We built an RDF KB using the CrunchBase API and integrated owl:sameAs-relations to the corresponding entities in DBpedia. 8 CrunchBase consists of structured information about organizations including companies  , people  , products  , investments  , and several other items  , and is edited by a Web community.
Our CrunchBase RDF KB contains 16 ,706 entities of type organization and 26 ,468 entities of type person – in both cases with corresponding owl:sameAs-links to DBpedia . 5.
In our evaluation  , we focus on relations between organizations and on relations between persons and organizations see Fig. This approach is more robust than using simple string matching based on the CrunchBase entity labels.
By bridging to DBpedia  , we can use the existing entity linking module 4 ,5 for mapping the textual subjects and objects. We built an RDF KB using the CrunchBase API and integrated owl:sameAs-relations to the corresponding entities in DBpedia.
This approach is more robust than using simple string matching based on the CrunchBase entity labels. By bridging to DBpedia  , we can use the existing entity linking module 4 ,5 for mapping the textual subjects and objects.
There are 16 ,509  , 60 ,936  , 151 ,722  , and 83 ,470 distinct facts regarding the KB properties cb:acquired  , cb:competesWith  , cb:founded  , and cb:isBoardMemberOrAdvisor  , respectively. Our CrunchBase RDF KB contains 16 ,706 entities of type organization and 26 ,468 entities of type person – in both cases with corresponding owl:sameAs-links to DBpedia .
The numbers also indicate that CrunchBase misses a significant number of facts. The high precision of the novel facts of the novelty classes B 1 ,i   , B 1 ,ii   , and B 2 on average 281/293 = 0.959 shows that especially in cases where the entities are already known as it is usually the case  , our system is able to retrieve high-quality novel facts.
Regarding the 89 extracted facts about acquisitions which were classified as known Class A  , we found out that four of them would have been detected by our tool before they were added to CrunchBase. the fact :Facebook :acquired :Oculus VR extracted from the news of March 25–28  , 2014. the announcement dates  , our system was able to retrieve known and novel facts as presented in Table 2.
To study scalability we created several datasets using varying subsets from 10 million to 80 million triples of the UniProt data. UniProt proteinandannotationdatainRDFformat14isusedfor this experiment.
Links are attached with Chinese protein names in search results and users can click these links to access relevant protein information in UniProt database.  Link generation: In order to integrate identified proteins with UniProt database  , links that follow linking rules of UniProt database are generated based on their accession numbers or names.
UniProt proteinandannotationdatainRDFformat14isusedfor this experiment. This experiment characterizes RDF_MATCH performance for querying large-scale data.
 Link generation: In order to integrate identified proteins with UniProt database  , links that follow linking rules of UniProt database are generated based on their accession numbers or names. More features can also be annotated if necessary.
Notably  , UniProt Q1  , Q2 had a high number of initial triples and the join results were quite unselective . Queries of the first type are – Q1  , Q2 of UniProt and Q1  , Q2  , Q3 of LUBM.
In S we can see the term columbus  , which comes from the description field of UniProt for the entry HBG2 HUMAN and refers to a position variant. We could also have used the various fields of EntrezGene  , UniProt and OMIM more carefully .
The dictionary is based on standardized nomenclature and controlled vocabularies of UniProt database  , since UniProt database makes use of the official nomenclature defined by international committees while still providing the published synonyms8. An English-Chinese Controlled-vocabulary dictionary is essential for mining protein information efficiently and accurately.
The work starts with downloading relevant data from UniProt database. SICLS also invited Professor Weimin Zhu of EBI to Shanghai and his comprehensive introduction to UniProt database and detailed technical information required for text mining are very helpful for our collaboration.
Section 6 concludes with a summaryandfuturework. Section 5 describes the performance experiments conducted using RDF data for WordNet and UniProt.
UniProt and KEGG  , to allow for automatic synonym retrieval. We combined the information from several sources  , e.g.
Then detailed protein information and annotation can easily be obtained from UniProt database  , such as protein sequences  , sequence annotations  , functional annotations  , sequence blast  , alignment and homologous analysis  , etc. Protein names in search results can be highlighted by users and if they want to know more about these proteins  , just click on them and follow links to access UniProt databases.
Besides protein names  , the dictionary also includes relevant information needed for text mining. The dictionary is based on standardized nomenclature and controlled vocabularies of UniProt database  , since UniProt database makes use of the official nomenclature defined by international committees while still providing the published synonyms8.
Figure 2shows a screenshot of our system which we presented to the domain experts from the HZI. UniProt and KEGG  , to allow for automatic synonym retrieval.
excellentphotographeraward  and Flickr groups e.g. d60  , Flickr awards e.g.
For locations with no Flickr coverage  , it is not possible to predict the location. This makes Flickr ideal for localizing POIs in the places where Flickr has coverage.
We crawl the images with GPS locations through Flickr API 1 . We evaluate the proposed models on Flickr dataset.
Similar to Wikipedia categories  , Flickr user groups are also overlapping. Flickr user groups are collected as ground truth.
Flickr group is a collection of images created by users with certain common interest.  Flickr group recommendation.
This makes Flickr ideal for localizing POIs in the places where Flickr has coverage. Many of the images that are geotagged and uploaded to Flickr portray POIs and tourist destinations.
Flickr users also have the option to create and join groups  , where people can post images and comments. Flickr: Flickr is an image hosting website  , where users can endorse an entity image by including it in their 'Favorites'.
The list of contacts for each user is publicly visible in Flickr  , and we used the Flickr API to reduce the necessary bandwidth for crawling. In this way  , we collected a " snowball" sample of the Flickr social network .
Both English and German tag networks are mined from Flickr. The tag networks needed for the proposed methods are generated from the on-line Flickr folksonomy via Flickr APIs.
Flickr allows users to create two types of links: links to favorite photos called favorites in Flickr and links to other users called contacts in Flickr. Flickr provides users with privacy control over photos they upload  , allowing photos to be classified as either private  , visible only to their friends  , or  , the default  , public.
Two known vulnerabilities in the Flickr Web application 9   , namely Flickr Visibility FV and Flickr Comment FC  , are used to evaluate the effectiveness of our approach. Flickr 4 is a Web application allowing users to upload and share their photos on the Web.
Our snowball sample is part of a large weakly connected component of the entire Flickr network that is reachable from the seed user; we call our sampled data the Flickr social network graph. The list of contacts for each user is publicly visible in Flickr  , and we used the Flickr API to reduce the necessary bandwidth for crawling.
In this section we provide a brief overview of some of the Flickr and non-Flickr works that we consider to be most related. The scope of research on Flickr is rather broad and includes analyses of the Flickr browsing patterns  , development of new interfaces  , image recommendations and many others.
Flickr allows authorized users to add comments to private and family photos; for public photos  , all users are allowed to comment. Flickr Comment FC.
MovieLens is collected from users of MovieLens website 2 . Netflix is the training data set for the Netflix Prize competition .
We further compare MudRecS against the 20 systems that participated in the Netflix contest in 2008. Netflix.
Netflix provides RMSE values to competitors that submit their predictions for the Quiz set. However  , we do not know the true ratings for the Quiz set  , which are held by Netflix being the subject of the Netflix Prize contest.
Netflix publicly provides the relevance judgments of 480 ,189 anonymous customers. Netflix Data: This data set was constructed by combining documents about movies crawled from the web with a set of actual movie rental customer relevance judgments from Netflix19.
The Netflix-48k dataset has a total of 10 ,015 ,137 ratings and the Netflix-24k dataset has 5 ,038 ,411 ratings. This creates one dataset of 48 ,090 users and 17 ,770 movies  , which we will refer to as Netflix-48k  , and one dataset of 24 ,047 users and 17 ,770 movies  , which we will refer to as Netflix-24k.
Table 3imation based CF methods on the MovieLens 10M and Netflix datasets. The Movie- Lens 10M and Netflix datasets are used in this study.
We use the full dataset that contains 480 ,189 users  , 17 ,770 items and 100 ,480 ,507 ratings. Netflix: This is the classic movie rating dataset used in the Netflix challenge 5 .
de-anonymized Netflix movie ratings 3. Likewise  , Narayanan et al.
We take the Netflix dataset for illustration. 4.3.
Comparison of recall on Movielens and Netflix datasets 2.
Specifically  , the first subsample of the Netflix dataset was produced by selecting 100 users from the set of all the Netflix users ranked between 2000 to 2100 based on the total number of ratings they gave. For our experiments we used three random subsamples of the Netflix Prize dataset 7 .
The Netflix domain has under 100 ,000 DVDs  , and Netflix users provide a large number of explicitly given ratings on the videos. The closest related studies have come from the Netflix challenge  , in which a system must recommend DVDs to subscribers based on their previous ratings 11.
These results are comparable to recently reported results for the Netflix data by methods that require an extensive training phase 2  , 13. Their mixtures achieve solutions with RMSEs around 0.8900 6.45% improvement over Netflix system  , which would currently rank high on the Netflix Prize Leaderboard 1   , even before mixing with the useroriented approach to which we now turn.
The open competition was held by Netflix  , an online DVDrental service  , and the Netflix Prize was awarded to the best recommendation algorithm with the lowest RMSE score in predicting user ratings on films based on previous ratings. We further compare MudRecS against the 20 systems that participated in the Netflix contest in 2008.
The third scenario covers university courses from Thalia 4. Schemas come from the XCBL 1 .
The data set represents the university course catalogues of 45 computer science departments around the world. The thalia testbed is specifically aimed at benchmarking rdb2rdf tools 11.
The Humboldt-Merger is implemented as a standalone Java application. Additionally   , we plan to show examples taken from the recent THALIA benchmark for information integration 5.
Furthermore  , for the majority of cases  , a notable improvement in Precision was measured with up to +37.9%  , +55.8% and +36% improvement for the Web-froms  , Thalia and OAEI datasets  , respectively. Comparing CEM with MWBM  , we confirm the ability of MCD to regulate the decision making of the former in majority of the cases  , with up to +31.2%  , +35.1% and +25% improvement in F1- Score for the Web-forms  , Thalia and OAEI datasets  , respectively.
Therefore  , the average maximum number of matches explored during a single CEM run is about 180 ,000 and 120 ,000 for the Web-forms and Thalia datasets  , respectively. Recall that  , on each iteration t  , CEM samples N = 10  , 000 matches.
The Thalia dataset 2 is a publicly available dataset of relational database tables representing University course catalogs from computer science departments around the world. Exact matches for each schema pair was manually crafted by several judges.
Additionally   , we plan to show examples taken from the recent THALIA benchmark for information integration 5. For the demonstration we provide sample data from all scenarios mentioned in the introduction.
The thalia testbed is specifically aimed at benchmarking rdb2rdf tools 11. Additionally  , the queries used in the benchmarks are more simplistic than those found in the scientific domain  , with few or only simple selection conditions.
Schemes exist for classifying more detailed forms of relationships  , these include Correspondence Patterns 5  , and the THALIA framework 10. Current semantic matching tools focus on discovering equivalence ≡ relationships   , and less commonly less general ⊑  , more general ⊒ and disjointness ≢ relationships.
A full enumeration of matches  , on the other hand  , has an exponential time-factor in the similarity matrix M dimensions. Therefore  , the average maximum number of matches explored during a single CEM run is about 180 ,000 and 120 ,000 for the Web-forms and Thalia datasets  , respectively.
The OAEI dataset contains ontologies from the 2011 competition in the comparison track  , using the bibliographic references domain. The Thalia dataset 2 is a publicly available dataset of relational database tables representing University course catalogs from computer science departments around the world.
The EDOAL 6  language provides a method for describing more complicated correspondences   , using an OWL-like syntax  , and was developed in conjunction with correspondence patterns. Schemes exist for classifying more detailed forms of relationships  , these include Correspondence Patterns 5  , and the THALIA framework 10.
Comparing CEM with MWBM  , we confirm the ability of MCD to regulate the decision making of the former in majority of the cases  , with up to +31.2%  , +35.1% and +25% improvement in F1- Score for the Web-forms  , Thalia and OAEI datasets  , respectively. On average  , CEM matches have a significantly better Precision with up to +36% boost over the second best 2LM  , slightly less Recall  , and an overall better F1-Score.
On average  , CEM matches have a significantly better Precision with up to +36% boost over the second best 2LM  , slightly less Recall  , and an overall better F1-Score. Overall  , for most of the cases  , independently of a given 1LM  , CEM 2LM has produced on average a better quality match  , with up to +14.6%  , +9.8% and +25% improvement in F1-Score over the second best 2LM baseline for the Webforms   , Thalia and OAEI datasets  , respectively.
However  , it achieves comparable performance in FRGC database. Compared to FDA  , NDPE obtains superior performance in ORL and PIE databases.
Table 2describes fundamental information about these benchmark datasets. In detail  , we used one toy dataset  , four UCI datasets  , and the FRGC dataset as our experimental testbeds.
Experiments are performed on a subset of facial images selected from FRGC version 2 9. Here we verify our subspace algorithms for the intensively studied topic  , face recognition.
However  , NDPE still outperforms NPE in all dimensions as depicted in Figure 5. As anticipated  , the overall AERs increase in FRGC database compare to ORL and PIE databases  , as it is a considerably difficult database.
The supervised version of NPE -NDPE  , consistently outperforms NPE in the all experiments in three face databases  , namely ORL  , PIE and FRGC. From the experiments above  , we notice that: 1.
In detail  , we used one toy dataset  , four UCI datasets  , and the FRGC dataset as our experimental testbeds. We also contrast with state-of-the-art semi-supervised learning approaches.
Table IV  , V and VI show the performance comparisons of all feature extractors in ORL  , PIE and FRGC databases  , respectively. Specifically  , we use 2 training samples and 5 training samples for comparisons.
Nevertheless  , when smaller training sample set 2 samples per class is used  , the performance degradation of NDPE is not obvious compared to FDA  , ie. However  , it achieves comparable performance in FRGC database.
For FRGC  , every subject has two types of images which were taken in controlled and uncontrolled environments. However  , we only select 10 images for each subject which have the same pose but with significant illumination variations for the experiments.
As we can observe from the tables  , NDPE delivers lower AER when 5 training samples are adopted for three databases. Table IV  , V and VI show the performance comparisons of all feature extractors in ORL  , PIE and FRGC databases  , respectively.
For each subject  , we randomly select 10 samples and they are partitioned into training and testing sets with 5 samples for each. We utilize all the subjects in the ORL and PIE and a subset of FRGC.
NDPE relies on the intuition that class information integration in the process of neighborhood selection could lead to the enhancement of classification capability over original NPE and this assumption is testified by the experimental results. The supervised version of NPE -NDPE  , consistently outperforms NPE in the all experiments in three face databases  , namely ORL  , PIE and FRGC.
As anticipated  , the overall AERs increase in FRGC database compare to ORL and PIE databases  , as it is a considerably difficult database. However  , the performance of NDPE AER=0.48% is still much better than FDA AER=6.37% on PIE database with 5 samples per subject used in training stage.
The controlled one was taken in the studio with controlled environment; while the uncontrolled images were taken at the outdoors  , hallways or atria  , with varying " natural " illumination conditions. For FRGC  , every subject has two types of images which were taken in controlled and uncontrolled environments.
For the baseline method  , we simply perform face matching without using any feature extractor. A few samples images for ORL  , PIE and FRGC are shown in Figure 2We first show the effectiveness of NDPE compare to NPE and baseline for three face databases in Figure 3  , Figure 4and Figure 5.
Using the extracted geonames  , a ML algorithm predicts the tweet's implicit state location and adds it to the tweet's list of geonames. Geonames are found through geospatial name extraction from preprocessed tweets using a named entity recognizer NER.
Between DBpedia and GeoNames  , in positive a DBpedia—GeoNames. Figure 2shows the total number of undecided and erroneous decisions  , divided into positive and negative cases.
Combined information geonames and predicted state is sent to the geocoding API to get the location of the tweet. Using the extracted geonames  , a ML algorithm predicts the tweet's implicit state location and adds it to the tweet's list of geonames.
Object classes whose coordinates are really close to those in Geonames include: church  , school  , tower or monument. The highest concentration of results is to be found in the first sector of 200 meters around the Geonames coordinates.
Geonames is generally richer than our database but a hefty chunk of its content points towards administrative regions around 50% and hotels. For example  , Geonames contains only 5 records for Timisoara  , while we obtain 30 results in Gazetiki.
From the instances of Geonames  , we found commonly used properties such as geo-onto:alternateName  , wgs84 pos:alt  , and geoonto:countryCode   , etc. However  , for Geonames and NYTimes  , we only retrieved commonly used properties in the data sets.
Speficically  , the weights of tags that appear in the English part of GeoNames are boosted. To determine whether a tag is geographic in nature  , in 21  GeoNames 3 is exmployed   , a large gazetteer of geographic entities.
GeoNames returns the country code that corresponds to the search keywords. We can determine the user's country by searching GeoNames with the information in the location field of the user's Twitter profile 1.
The intersection between Geonames and Gazetiki  , around 15%  , shows that the two structures are complementary. Geonames is generally richer than our database but a hefty chunk of its content points towards administrative regions around 50% and hotels.
All the decisions made by two subjects were excluded from the results because they unusually made two or more erroneous decisions when using NOSUMM i.e. Between DBpedia and GeoNames  , in positive a DBpedia—GeoNames.
Aleda 15 is a French entity knowledge base extracted from the French Wikipedia and Geonames. Other efforts were employed to automatically build entity knowledge bases from Wikipedia  , Wordnet  , and GeoNames such as YAGO and DBPedia.
Geonames are found through geospatial name extraction from preprocessed tweets using a named entity recognizer NER. Combined information geonames and predicted state is sent to the geocoding API to get the location of the tweet.
If a sensor detects a tsunami  , GeoNames can provide all geographical places within a certain radius from the sensor location. The prediction is calculated by the following formula: Further on  , we have utilized GeoNames as a worldwide geographical knowledge base.
The news stories of Dataset2 are from the sports channel of SOHU http://sports.sohu.com/ on April 22  , 2007. Dataset2 is used to test our topic detection and tracking algorithm.
Because our aim is to classify blog articles  , those articles selected from Sohu site are only used as a part of the training data. They are proper to the definition of informative articles.
We divide those data labeled by human into training and testing parts. Because our aim is to classify blog articles  , those articles selected from Sohu site are only used as a part of the training data.
Dataset2 is selected to verify our guess. The sentence " 搜狐直播 员： "  " SOHU live reporter "   , separated as a sentence by other contents with a colon  , appears 73 times in the news story but in fact has nothing to do with the topic.
It supports web browsers  , iOS and Android apps  , and community-built apps based on the SoundCloud API. SoundCloud has about 10 million users and is usually among the world's top 200 web sites 3 .
The SoundCloud architecture started as a single Rails application now called mothership but since 2011 has gradually been split into separate services. It supports web browsers  , iOS and Android apps  , and community-built apps based on the SoundCloud API.
and most are vertical  , i.e.  , in charge of one functional area completely. Any SoundCloud development team consists of developers only there are no architects   , testers  , etc.
SoundCloud has around 80 developers overall. and most are vertical  , i.e.  , in charge of one functional area completely.
Any SoundCloud development team consists of developers only there are no architects   , testers  , etc. New services are realized in a variety of technologies.
Team Pay is responsible for the Buckster service that contains all payments-related functionality such as user subscriptions   , fraud detection  , and reporting. SoundCloud has around 80 developers overall.
New services are realized in a variety of technologies. The SoundCloud architecture started as a single Rails application now called mothership but since 2011 has gradually been split into separate services.
We plan to use structured information from MusicBrainz and DBpedia  , together with semi-structured and unstructured information gathered from SoundCloud  , Last.fm  , Twitter and Facebook. At this moment the context information of the collection is scant.
For #H1N1  , #FIERFOX and #SOUNDCLOUD  , the uninformed forecasts are rather uncertain of the values of the mean  , the standard deviation  , and the second moment   , leading to significantly lengthened violin plots. Whereas SED accurately forecasts the average CCDFs and all three statistics  , as observed by contrasting SED forecast violin plots against those of the ground truth Future.
SoundCloud is a music sharing service: artists can present themselves and upload their own music 3 hours for free  , more against payment; other users can browse this music  , listen to it  , comment on it  , and share it on social networks. Pay.PO/I5-15:10.
To this end  , we will take information from structured Wikipedia  , WordNet  , MusicBrainz  , semi-structured Freesound  , SoundCloud  , Last.fm  , Internet Archive and unstructured sources Facebook  , Twitter. We will focus our research in the extraction of knowledge from music-related context information sources in the Web.
The ultimate end of this PhD work is contribute to the improvement of Music Information Systems  , exploiting structured context information with semantic meaning. We plan to use structured information from MusicBrainz and DBpedia  , together with semi-structured and unstructured information gathered from SoundCloud  , Last.fm  , Twitter and Facebook.
We use the LibraryThing dataset which was crawled by Zubiaga et al. LibraryThing dataset: LibraryThing is a social tagging system which allows to tag books.
LibraryThing dataset: LibraryThing is a social tagging system which allows to tag books. The original dataset is available online 4 .
For the user study on the LibraryThing data  , we asked five colleagues to register with LibraryThing  , tag at least 20 books there  , and choose some friends among other users. This process yielded 150 queries for delicious and 184 queries for LibraryThing; note that this method cannot be applied to the Flickr data because there is almost no overlap in the pictures users tag.
They then suggested 28 queries related to the books they tagged. For the user study on the LibraryThing data  , we asked five colleagues to register with LibraryThing  , tag at least 20 books there  , and choose some friends among other users.
LibraryThing www.librarything.org is a web service for managing book collections  , allowing  , among other things  , to tag books. In the case study presented here we will concentrate on tags for books from LibraryThing.
Examples of social-community platforms are del.icio.us  , Flickr  , LibraryThing  , LinkedIn  , MySpace  , Facebook  , and YouTube. contents.
As users in LibraryThing typically use tags that consist of multiple terms  , we extracted the terms from the tags and considered the set of terms used by a user for a book as if she tagged the book with the terms.  LibraryThing: We have partly crawled the social book catalog LibraryThing http://www.librarything.com with a total of 9 ,986 users  , 6 ,453 ,605 books with 14 ,295 ,693 tags  , and 17 ,317 friendship edges.
However  , people tagging streams in Twitter stabilize slightly slower and less heavily than the tagging streams in Delicious and LibraryThing where imitation behavior is encouraged. Interestingly  , our results show that despite the difference in the user interfaces  , the people tagging streams in Twitter exhibit similar stabilization patterns as the book and website tagging streams in Delicious and LibraryThing.
The highest i.e.  , high k values and fastest i.e.  , low t values overall tag stabilization can be observed for Delicious and LibraryThing which both encourage imitation behavior by suggesting previously assigned tags see Delicious and by making previously assigned tags visible during the tagging process see LibraryThing. Contrary  , we can see that real-world tagging systems exhibit much higher stability.
 LibraryThing: We have partly crawled the social book catalog LibraryThing http://www.librarything.com with a total of 9 ,986 users  , 6 ,453 ,605 books with 14 ,295 ,693 tags  , and 17 ,317 friendship edges. The participant then marks each result as highly relevant  , relevant  , or nonrelevant in the context of the query initiator without knowing which configuration generated the result.
The tags have similar frequencies 440 and 439 occurrences resp. As an example  , consider the tags must read and mysteries in the LibraryThing dataset.
The use of metadata of real and recent books until March 2009 from Amazon/LibraryThing was pointed out positively. The search system was well received by the participants.
As an example  , consider the tags must read and mysteries in the LibraryThing dataset. We use the subscript ι to indicate that the co-occurrence distribution is computed using the co-occurrence on items.
Note that the bars for the LibraryThing baseline experiment have been cut at 350 ,000. Figure 4shows abstract cost for the same setup  , but now with tag expansion up to a limit of 10 similar tags.
UK edition  , translation  , the large majority of tags in LibraryThing refers either to the work or to the exemplar. While we find some tags related to the levels of expression and manifestation e.g.
The second approach builds entirely new tools which are complementary to the PubMed search interface. For example   , PubMed advanced search feature 3  , 4  , PubMed auto query suggestions 5  , PubMed automatic term mapping 6  , and PubMed related article feature 7  are some of the PubMed supplementary tools to enhanced information retrieval from PubMed.
It finds articles similar to a chosen PubMed citation from the entire PubMed database. PMRA probabilistic similarity measure was used to develop the related article feature in PubMed.
PubMed real world log analysis shows that roughly a fifth of all non-trivial PubMed sessions used the related article feature 18. It is capable of finding relevant articles from the entire PubMed database for a given PubMed citation.
It is capable of finding relevant articles from the entire PubMed database for a given PubMed citation. PMRA is a wellestablished tool in PubMed.
Pubmed creates chunks from this query which represent entries in MeSH 4 . Firstly  , the query is submitted to PubMed 3 .
2 Note that PERSIVAL uses a distributed search engine that is not limited to PubMed alone. nlm.nih.gov/PubMed/.
When downloaded  , the PubMed citations are in the XML format. First  , all the PubMed citations for the given 50 topics were downloaded from the PubMed using the Entrenz utilities provided by the NCBI 29.
 Using the content-similarity algorithm in PubMed 20  , accessed through the PubMed eUtils API. Settings used: the Krovetz stemmer  , Dirichlet prior smoothing with µ = 1500.
Pubmed and Wikipedia.   , medical research databases e.g.
As a final experiment  , we ran our strategy simulations on actual PubMed results. Thus  , browsing related articles appears to be an integral part of PubMed searchers' activities.
Finally  , the corpus contains multiple view metrics from PLOS and PubMed Central PMC. The sources taken into account for citation metrics are CrossRef  , PubMed  , Scopus  , Nature and Postgenomic.
PubMed provides access to over 23 million citations in the field of biomedicine 2. In 1996  , National Center for Biotechnology Information NCBI at NLM introduced PubMed citation database.
Pubmed creates chunks from this query which represent entries in MeSH Johnston et al.  , 2002 . The query is submitted to PubMed 4 .
The BASE1 and BASE10 are baseline collections of the WT100g. The Web Track collection WT100g consists for 100 GB of spidered web data and was originally used in last years VLC2 track 6.
Because of various internal limits in the experimental SearchServer version used for WT100g  , we indexed WT100g in 12 tables more than proved to be necessary. The second table contained CD3  , CD4 and the last 4 directories of CD5 WTX101-WTX104.
The same pattern with respect to elapsed time ratios found with VLC2 Title Only runs was found with Web Track runs: BASE1 to the other collections yielded sub-linear ratios  , while BASE10 to WT100g yielded super-linear ratios. Measuring Scalability over both the text and Inverted file we found that the figures for BASE1 to WT100g were very good  , while the corresponding figures from BASE10 to WT100g were acceptable.
No approximate search index was built; however  , some Large Web Task runs re-used the approximate search index of WT10g as a spell-corrector. Because of various internal limits in the experimental SearchServer version used for WT100g  , we indexed WT100g in 12 tables more than proved to be necessary.
Further  , we also made use of a medium sized collection wt50g  , obtained by randomly sampling half of the documents from wt100g. We have used two collections wt10g and wt100g from the TREC Web Track 8 coupled with queries from Excite logs from the same c. 1997 period.
Measuring Scalability over both the text and Inverted file we found that the figures for BASE1 to WT100g were very good  , while the corresponding figures from BASE10 to WT100g were acceptable. Having to do I/O for list elements when weighting inverted list could reduce the search performance dramatically.
Why was the response time for Web Track queries higher than Title Only on WT100g nearly double despite the fact that Web Track Queries are only slightly larger than Title Only Queries ? A frequent query was the single term " a " .
Similar results hold for the wt100g collection  , where a model of about 10 Mb offers substantial space and time savings over no model at all  , but returns diminish as the model size increases. improvement with larger models.
Anecdotally  , the increase is due to inclusion of sophisticated navigational and interface elements and the JavaScript functions to support them. In wt100g  , the average web page size is more than half the current Web average 11.
Table 13– Scalability Results for VLC2 experiments The Scalability results are given in table 13. The ratio from BASE10 to WT100g shows super-linear growth for response times for all runs.
All three collections were distributed as evenly as possible across the 16 nodes of the Cluster by linear assignment i.e. The BASE1 and BASE10 are baseline collections of the WT100g.
The first two rows of Table 1give the number of documents and the size in Mb of these collections. Further  , we also made use of a medium sized collection wt50g  , obtained by randomly sampling half of the documents from wt100g.
The static cache performs surprisingly well squares in Figure 4  , but is outperformed by the LRU cache circles. With 1% of documents cached  , about 222 Mb for the wt100g collection  , around 80% of disk seeks are avoided.
The Index time scaleup from the baselines to WT100g and from BASE1 to BASE10 are good  , with very little deterioration in time per index unit. The load balancing for all Indexing experiments on the Web Track is good with only slight levels of imbalance recorded: this confirms that the strategy used for distributing the collection to nodes was a good one.
The final merge took only about 10 minutes a total of 1.5% of total indexing time: this represents a significant improvement on previous single processor experiments. This compares favourably with our previous web track experiments with WT100g 2  , in which indexes were 11% of the collection size.
Census is a data set from the U.S. census database  , also down-loaded from U.C. Census.
The Italian National Institute for Statistics ISTAT provides  , under open data licences  , information gathered via the 2011 Italian Census 5 . Census Data.
In the former data sets we discover causal relationships and nonrelationships between census categories such as gender and income. In Section 5 we test this algorithm on a variety of real-world data sets  , including census data and text data.
Census data has categorical data that we divided into a number of boolean variables. The census data set consists of n = 126229 baskets and m = 63 binary items; it is a 5% random sample of the data collected in Washington state in the 1990 census .
In addition to the Census database  , we generated a number of synthetic databases with different data distributions . We refer to this database as the Census database.
Odum Institute for Research in Social Science provides public access to an extensive collection of U.S. Census data  , North Carolina Census data  , and the National Network of State Polls NNSP. The data archive of the H.W.
For instance  , we divided the census question about marital status into several boolean items: MARRIED  , DIVORCED  , SEPARATED  , WIDOWED  , NEVER-MARRIED.~ Every individual has 4The census actually has a choice  , " Never married or under 15 years old. " Census data has categorical data that we divided into a number of boolean variables.
We used the census' variables related to people and buildings Table 1  , which were defined at various geographic levels. The Italian National Institute for Statistics ISTAT provides  , under open data licences  , information gathered via the 2011 Italian Census 5 .
Census data generation from left and right images is done in parallel. The census algorithm requires 15 8-bit comparisons and this is very easily done in parallel with little logic.
In this experiment  , we tested various attributes from census data compiled by U.S. Census Bureau 17 . The experiment results indicate that our algorithms can successfully identify spatial outliers ignored by the z algorithm and can avoid detecting false spatial outliers.
As a particular numerical example  , we examined the Boston Housing data  , which contains 506 census tracts in Boston from the 1970 census. As shown in the boxplot  , taking ε = 0.1  , most of the ratios lie within interval
The experimental results of all models are shown in Table  1. The results can be known through submitting the results in Kaggle.
For each querydocument pair  , personalization features are computed based on the information available prior to the moment this query was issued including the information from the first 26 days of the Kaggle dataset. This evaluation dataset S consists of 280K queries uniformly sampled from the 27th day in the original raw Kaggle log for training and testing purposes.
During that time there were two main opportunity to obtain a feedback: 1 external  , through Kaggle LeaderBoard  , and 2 internal  , using homogeneous ensembling with CV-passports  , see Section 4. lasted for more than 4 months.
At the conference  , we hope to gather more lessons learned from teams. To encourage teams to jump into the data early  , we may borrow from Kaggle by offering a web service that scores runs before the final submission deadline.
To encourage teams to jump into the data early  , we may borrow from Kaggle by offering a web service that scores runs before the final submission deadline. We are investigating possible centralized cluster resources for future KBA tasks.
The results can be known through submitting the results in Kaggle. 20% ratings are used in the public leaderboard evaluation and the left 80% ratings are used in the private leaderboard evaluation.
There does exist a data set for insults which was used in a Kaggle competition. 3 To our knowledge this is the first publicly available abusive language data set with multiple annotations and the different subcategories.
This evaluation dataset S consists of 280K queries uniformly sampled from the 27th day in the original raw Kaggle log for training and testing purposes. Namely  , we use the evaluation dataset provided by team pampampampam  , the winner of the Personalized Web Search Challenge.
For example  , in Kaggle contests  , there are typically at least dozens and usually hundreds of participants  , so in such a contest setting the optimal threshold is likely to have little effect on incentives to exert effort. We note that both regimes with large numbers of players and small numbers of players are relevant empirically.
A number of other such contests abound. A third example is the contest platform Kaggle  , which hosts contests where the innovation being procured is a data-mining algorithm: again  , submissions are typically evaluated on a test dataset provided by the requesters and ranked according to the score they obtain.
The goal of the challenge was to use these listening histories to recommend 500 songs for a subset of 110K test users. The MSD dataset was used in the Kaggle Million Song Dataset Challenge 18 and consists of listening histories for 1.2M users and 380K songs.
Selection of the parameters was a consequence of both feedbacks  , combined together. During that time there were two main opportunity to obtain a feedback: 1 external  , through Kaggle LeaderBoard  , and 2 internal  , using homogeneous ensembling with CV-passports  , see Section 4.
We briefly enlist the types of features extracted from the Yandex dataset. For each querydocument pair  , personalization features are computed based on the information available prior to the moment this query was issued including the information from the first 26 days of the Kaggle dataset.
For instance  , the additional improvement of 0.12% over the baseline e.g.  , the same as the WRSS achieved over RSS according to MRR with 'sat' gain would allow the team that took the 3rd place in Kaggle competition become a winner 3 . To conclude this section  , we would like to mention that web search personalization is known as one of the most challenging learning tasks.
The difference in performance between neighbor-and model-based methods on binary data became especially evident during the Million Song Dataset Challenge MSD which was recently conducted by Kaggle 18 . Recently  , several model-based approaches have been developed specifically for binary feedback 13  , 20  , however results on CF challenges have shown that these models don't perform as well as neighbor-based approaches.
The MovieLens-100K data set is a classic movie rating data set collected through the MovieLens web site. Our experiments are performed on three real data sets: MovieLens-100K 4   , MovieLens-1M and Lastfm.
The MovieLens dataset is from GroupLens Research. A series of experiments were conducted on four real-world datasets: MovieLens-100K  , MovieLens-1M  , DianPing and Yahoo!Music.
Among these datasets  , MovieLens-100K and MovieLens-1M are from the well-known MovieLens dataset. Music to validate the effectiveness of the proposed framework.
MovieLens is collected from users of MovieLens website 2 . Netflix is the training data set for the Netflix Prize competition .
MovieLens-1M is a large data set with more distinct users than items. Overall  , the performance of our approach on MovieLens-1M is similar to that on MovieLens-100K  , since they are from the same web source.
People are invited to visit MovieLens in order to rate and ask for recommendations for movies. The MovieLens data set is a real movie database from the web-based recommender system MovieLens www.movielens.umn.edu.
Further details of MovieLens and the MovieLens tagging system can be found in 29. Since we introduced tagging features to MovieLens in January 2006  , MovieLens members have created 84 ,155 tag applications resulting in 13 ,558 distinct tags a tag is a particular word or phrase  , a tag application is a three way relationship between a user  , tag  , and item.
This dataset 3 is an extension of MovieLens dataset  , which links movies from MovieLens with their corresponding IMDb webpage and Rotten Tomatoes review system. Movie.
The second set of movies are selected from the top-200 movies in MovieLens. The first set of movies contains the top-40 most popular movies in MovieLens.
We only report the simulation results of MovieLens due to the space limitation . Two datasets from movie rating are applied in our experiments: MovieLens 3 and EachMovie 4 .
Tag Searches: Tag searches are textual searches for tags  , or clicks on tag hyperlinks. Further details of MovieLens and the MovieLens tagging system can be found in 29.
Music to validate the effectiveness of the proposed framework. We conducted a series of experiments on four real-world datasets: MovieLens-100K  , MovieLens-1M  , Dianping and Yahoo!
Comparison of recall on Movielens and Netflix datasets 2.
However   , in this study  , we want to capture the consumed movies which were recommended;  we want to give users sufficient time to learn how to use MovieLens;  we want to give MovieLens enough time to understand users' preferences better  , in order to improve the quality of recommendations. This is potentially due to the fact that these users had watched many movies before joining MovieLens – they rated these movies to help MovieLens understand their preference better.
Overall  , the performance of our approach on MovieLens-1M is similar to that on MovieLens-100K  , since they are from the same web source. That means in MovieLens-1M data set  , by using MCoC  , the rate of hit items in NPCA's recommendation list is reduced  , whereas the positions of hit items are moved up.
@anderson.com addresses. This includes Enron @enron.com and non-Enron e.g.
The Enron email dataset 8 is the archive email from many of the senior management of Enron Corporation  , and is now the public record. Enron email dataset.
It contains about 0.5M emails from about 150 users  , mostly senior management of Enron. ENRON This dataset is from the Enron email collection 3 .
 fb1912: an ego-network obtained from Facebook.  enron: an e-mail communication network by Enron employees.
Enron email dataset. 3.1.1.
on Enron email dataset. Performance of GSP-PCL  , GSP-means and Kmeans on Enron email dataset in terms of F1.
The Enron corpus contains e-mail messages belonging to 159 users. The Enron corpus – a large set of e-mail messages – has been made publicly available during the legal investigation concerning the Enron corporation 13.
 The Downfall of Enron – Enron collapsed dramatically from one of the most respected U.S. corporations into bankruptcy over the last quarter of the year 2001. The grey line denotes the relative volume of references to Enron.
The TREC Enron corpus covers 104 custodians while Enron had a worldwide staff count of 22 ,000 at the end of 2001. While we were not able to identify dates for all custodians  , the It should be noted that while the TREC Enron Corpus has more email than the CMU Enron Corpus  , many are duplicates and the collection is still a small collection when compared to the actual email at Enron or other large organizations.
Each employee in Enron is identified by an email address. We preprocessed the Enron email dataset by removing the common stop words.
 enron: an e-mail communication network by Enron employees.  astro: a co-authorship network among arXiv Astro Physics publications.
Next  , we present some statistical data about the results of C-Rank on the 131 mailboxes of the Enron data set. Enron data set statistics.
The Enron data set is a collection of email messages exchanged among 149 executives at Enron. The resulting network had 3 ,725 nodes and 58 ,123 links.
While the TREC Enron Corpus had 104 custodians  , we took a broader view and considered the number of custodians identified in other distributions of the Enron Corpus  , namely the CMU Enron Corpus which is also based on the FERC release. For this analysis  , we sought to confirm that the number of custodians selected by the exclusionary ED team would reflect the percentage of custodians collected during the Enron litigation.
CiteSeer-API was design not only to allow interaction between CiteSeer-like services but also with other DL systems. We introduced CiteSeer-API  , a SOAP/WSDL-based API to CiteSeer-like services.
The CiteSeer Plus system architecture is highly modular. An experimental prototype of a CiteSeer Plus deployment is publicly available 5 .
We introduced CiteSeer-API  , a SOAP/WSDL-based API to CiteSeer-like services. We encourage research groups to take advantage in their own projects of the functionalities and data available through CiteSeer-API.
CiteSeer extracts metadata items using customized regular expressions. OAI compliance requires additional metadata items than currently available from CiteSeer.
Our system works even when CiteSeer is down or overloaded. from the ACM Digital Library that CiteSeer cannot provide.
Our motivations for CiteSeer-API are therefore 1 to provide programatical access for all the functionalities supported by CiteSeer-like systems; 2 to enable interoperability of CiteSeer services with distributed and heterogeneous DL systems. Still  , CiteSeer servers feature many functionalities that cannot be accommodated by OAI-PMH  , including full text document and citation search and citation-based document discovery.
The addition of semantics concepts to CiteSeer-API using OWL-S will enable automated agents to discover  , register and seamlessly exploit CiteSeer-like services. While CiteSeer-API turns CiteSeer-like niche search engines into actual web-services  , it still requires developers to have an understanding of the service in order to make use of it.
After comparing available services provided both by CiteSeer and Google Scholar  , some unique services of CiteSeer are excluded. Originally  , CiteSeer has more than 40 request types recorded in its logs.
As such  , when dealing with records present in CiteSeer  , we save a potential query. Note that the CiteSeer mirror that we host already contains the cached PDF files downloaded by the original CiteSeer crawler.
Registration to CiteSeer-API is available at 1. We provide an overview of the current functionalities supported by CiteSeer-API and outline what we feel are the next necessary steps for enabling CiteSeer services on the Semantic Web.
CiteSeer servers have been brought to OAI-PMH compliance so that their metadata collection can be accessed by metadata harvesters 8. Our motivations for CiteSeer-API are therefore 1 to provide programatical access for all the functionalities supported by CiteSeer-like systems; 2 to enable interoperability of CiteSeer services with distributed and heterogeneous DL systems.
CiteSeer also uses PageRank and HITS algorithm. CiteSeer is a popular digital library on the Web for research publication search and citation analysis.
CiteSeer catalogs scientific publications available in full-text on the web in PostScript and PDF formats. The data for our experiments was taken from CiteSeer 24.
We provide an overview of the current functionalities supported by CiteSeer-API and outline what we feel are the next necessary steps for enabling CiteSeer services on the Semantic Web. We introduce CiteSeer-API  , a SOAP/WSDL based API to CiteSeerlike servers which we envision will serve as the corner stone for seamless interoperability with and between CiteSeer services.
If a document falls into the paper category according to CiteSeer i.e. The CiteSeer module takes care of the download phase  , converting and parsing each document.
Each newsgroup contains roughly about 1000 messages. The newsgroup dataset includes 20 ,000 messages from 20 different newsgroups.
It consists of 20 ,000 newsgroup articles from 20 different newsgroups. The 20 Newsgroup dataset is a document-words dataset.
It consists of the same set of 20 newsgroup topics  , but each topic contains only 100 articles. The third subset is the mini 20 newsgroup dataset which is a reduced version of the full 20 newsgroup dataset.
In this newsgroup data  , different emails or posts may have the same subject. To obtain multiple co-occurrence data  , we use the 20-Newsgroup data set.
Searching on newsgroup message is searching on unknown content. However  , people have little idea of the contents and authors of the newsgroup messages.
The third subset is the mini 20 newsgroup dataset which is a reduced version of the full 20 newsgroup dataset. Interested readers can refer to technical report 7.
In this paper  , we analyzed the reasons that cause the text-based search engine to fail on newsgroup search and also analyzed the thread tree structure of newsgroup messages  , then we proposed that the performance of newsgroup search can be improved by analyzing the context information of the messages within the message thread tree. Newsgroup search is a very important service for web communities; however  , the performance of a traditional text based search engine  , or a search engine for web pages  , when used on newsgroups  , is not adequate due to unique attributes of newsgroup message threads.
A user submits a query to a newsgroup search engine because he believes somebody might already have answered the same question or a very similar question in the newsgroup. A user typically posts a question to a newsgroup because he thinks the question is unique or he wants a very detailed answer to the question.
No matter whether a user posts a new question to the newsgroup or submits a query to the search engine  , he is expecting other people's answers to his question  , not other people's similar question messages. A user submits a query to a newsgroup search engine because he believes somebody might already have answered the same question or a very similar question in the newsgroup.
Although significant improvement on the performance of newsgroup search has been achieved by combining multiple features from the context of the message in the message thread tree  , there are still more ways to improve the performance of newsgroup search left unexplored. In the future  , we plan to study to what extent our results and conclusions are applicable to all newsgroup search in general.
A newsgroup collection can be regarded as a collection of thread trees  , and there is no explicit semantic relationship between different thread trees. Figure 1is an example of a newsgroup message thread tree.
The reason is that the length of texts has great variations It's worth noting that variations of similarity scores in the 20-newsgroup corpus are more evident than the Reuters corpus   , even though 20-newsgroup is a fairly balanced corpus.
This leads to two different kinds of behavior: Some people visit a newsgroup to ask questions  , others come to a newsgroup to answer other people's questions or discuss some topics.
In effect  , a newsgroup thread can be seen as an already grouped topically related set of messages. Her work is related to ours because newsgroup thread trees can also be considered as a set of locally concentrated discussions  , but different to high quality traditional literature used in Hearst's research  , newsgroup message threads contain messages authored by different people and are not linear in structure.
A newsgroup discussion on a topic typically consists of some seed postings  , and a large number of additional postings that are responses to a seed posting or responses to responses. Newsgroup postings tend to be largely " discussion " oriented.
The streamcorpus is about 4.5TB and just over 500 million documents appearing in the period from October 2011 through February 2013. In this task  , we are given a list of target entities from Wikipedia and Twitter  , and we aim at identifying from the given streamcorpus 1   , which stream items documents are worth citing when updating the target entity profile e.g.  , Wikipedia article.
A specially filtered subset of the full TREC 2014 StreamCorpus 2 was provided for use in the 2014 TREC KBA Track. its Wikipedia article.
We use the streamcorpus toolbox 4 to parse the files.  Parsing: The SC files are then converted to TXT files.
Our corpus reader was developed based on the streamcorpus toolkit provided by the TREC KBA track. Each file contains around 100–300 JSON documents of the same type  , and is serialized with Apache Thrift and compressed with XZ Utils.
It consists of about 20 million timestamped documents from several sources News  , Social  , Forum  , Blog  , etc. A specially filtered subset of the full TREC 2014 StreamCorpus 2 was provided for use in the 2014 TREC KBA Track.
This stripped corpus is about 4.5TB and just over 500M StreamItems. We have used the KBA 2013 " English-and-unknown-language " streamcorpus with all non-English documents removed and the StreamItem.body.raw text set to "".
We then applied the verb model to generate a document prior for each item in the streamcorpus. The quantity nw  , H is simply the number of headlines in which the word w occurs.
The corpus is divided into 11948 folders  , each one represents one distinct hour. The streamcorpus is about 4.5TB and just over 500 million documents appearing in the period from October 2011 through February 2013.
On the other hand  , only use the named entities to measure the importance of sentence may be not appropriate. This could be in part because we have used " StreamItem.body " 1 to filter the streamcorpus  , but it is always incomplete  , which led to many relevant documents not included.
We have used the KBA 2013 " English-and-unknown-language " streamcorpus with all non-English documents removed and the StreamItem.body.raw text set to "". For the purpose of the TS track  , the corpus of time-stamped documents is considered a stream and documents should be iterated over in temporal order.
We identified Web site domains that frequently mentioned the KBA entities from these towns  , and then generated an excerpt from the TREC StreamCorpus containing all documents from those domains. As an initial small step in this direction  , we helped the KBP organizers select a couple of the small town " groups " of KBA entities as the basis for the Cold Start input corpus.
A specially filtered subset of the full TREC 2014 StreamCorpus 1 was provided. This year  , the track run only one task which requires systems to iterate over a stream corpus in a chronological order and filter relevant and novel sentences to a developing event.
In order to retrieve the content of a document  , we need to perform these operations in reverse order: after decompressing each file  , we use the tools provided by the StreamCorpus 2 toolbox to extract large streams of text from each StreamItem and store its content in a custom format. Flat files have been serialized into batches of documents called Chunks  , and further compressed.
However  , none of records have any genre labels. As the largest currently available dataset  , the Million Song Dataset MSD is a collection of audio features and metadata for a million contemporary popular music tracks.
As the largest currently available dataset  , the Million Song Dataset MSD is a collection of audio features and metadata for a million contemporary popular music tracks. In music information retrieval  , many methods see song genre as important metadata for retrieving songs.
Compared to GNMID14  , MSD differs most notably in its absence of geotemporal information and smaller scale. The Million Song Dataset MSD 1 uniquely provides audio features among large music datasets as well as some complementary datasets for satellite applications .
In this challenge   , song listening histories were made available for 1.2M users and 380K songs  , and the goal was to predict the next 500 songs for a subset of 110K users. The difference in performance between neighbor-and model-based methods on binary data became especially evident during the Million Song Dataset Challenge MSD which was recently conducted by Kaggle 18 .
In more details:  Taste Profile Subset TPS: contains user-song play counts collected by the music intelligence company Echo Nest. Throughout this study we use four medium to large-scale user-item consumption datasets from various domains: 1 taste profile subset TPS of the million song dataset 1; 2 scientific articles data from arXiv 5 ; 3 user bookmarks from Mendeley 6 ; and 4 check-in data from the Gowalla dataset 4.
The MSD dataset was used in the Kaggle Million Song Dataset Challenge 18 and consists of listening histories for 1.2M users and 380K songs. In all MSD experiments we follow the same set-up that was used during the competition and tune models on the 10K public leaderboard set  , then evaluate on the 100K private leaderboard set.
The goal of the challenge was to use these listening histories to recommend 500 songs for a subset of 110K test users. The MSD dataset was used in the Kaggle Million Song Dataset Challenge 18 and consists of listening histories for 1.2M users and 380K songs.
Throughout this study we use four medium to large-scale user-item consumption datasets from various domains: 1 taste profile subset TPS of the million song dataset 1; 2 scientific articles data from arXiv 5 ; 3 user bookmarks from Mendeley 6 ; and 4 check-in data from the Gowalla dataset 4. We further pre-process the dataset by only keeping the users with at least 20 songs in their listening history and songs that are listened to by at least 50 users.
Of the 58 ,649 overlapping songs in both collections  , we selected 24 ,436 songs with at least five user interpretations to have sufficient text for our analysis. For the experiment  , we used songs that appear in both songmeanings.com and the Million Song Dataset MSD 1   , a freely available music collection with a variety of useful audio features and metadata.
For the experiment  , we used songs that appear in both songmeanings.com and the Million Song Dataset MSD 1   , a freely available music collection with a variety of useful audio features and metadata. We collected interpretations of lyrics from songmeanings.com  , where music listeners share and discuss their understanding of lyrics by posting comments about millions of songs.
The difference in performance between neighbor-and model-based methods on binary data became especially evident during the Million Song Dataset Challenge MSD which was recently conducted by Kaggle 18 . Recently  , several model-based approaches have been developed specifically for binary feedback 13  , 20  , however results on CF challenges have shown that these models don't perform as well as neighbor-based approaches.
The Million Song Dataset MSD 1 uniquely provides audio features among large music datasets as well as some complementary datasets for satellite applications . However  , it is about 100 times smaller in size and is subject to similar issues with Twitter text matching.
wikiHow with a click ratio of 1.3  , maps MapQuest 1.75; appearances of " maps " within all clicked domain strings with a ratio of 1.74  , weather AccuWeather 2.22; " weather " string 1.48  , dictionary dictionary.reference.com 1.43; thefreedictionary .com 1.75  , " dictionary " 1.35  , recipes allrecipes.com 1.58  , " recipe " 1.53  , video streaming screen.yahoo.com 1.97; " screen " 1.77  , and music iTunes 1.88. Further inspecting the lists of top clicked domains revealed differences towards voice queries in other CQA sites e.g.
where m 3 is an authority conformation function that conforms DrugBank 's authority to the Example authority 8 . For Jamendo we deploy manually one experimental pipeline:
We selected this dataset as it contains a substantial amount of embedded information hidden in literal properties such as mo:biography. From the music domain  , we chose the Jamendo 9 dataset.
This result is particularly interesting  , because we do not always generate the manually created reference pipeline described in the previous subsection. The results shown in Table 2suggest that when faced with data as regular as that found in the datasets Drugbank  , DBpedia and Jamendo  , our approach really only needs a single example to be able to reconstruct the enrichment pipeline that was used.
The results shown in Table 2suggest that when faced with data as regular as that found in the datasets Drugbank  , DBpedia and Jamendo  , our approach really only needs a single example to be able to reconstruct the enrichment pipeline that was used. measured the F-measure achieved by our approach on the datasets at hand.
As an example  , in case of M 1 Jamendo the manual configuration was just one enrichment function  , i.e.  , NLP-based enrichment to find all locations in mo:biography. However  , in some cases 4.7 % our algorithm generated a longer pipeline to emulate the manual configuration.
The goal of our enrichment process is to add a geospatial dimension to Jamendo  , e.g.  , the location of a recording or place of birth of a musician. We selected this dataset as it contains a substantial amount of embedded information hidden in literal properties such as mo:biography.
For Jamendo we deploy manually one experimental pipeline: To this end  , we deployed a manual enrichment pipeline  , in which we enrich Jamendo's music data by adding additional geospatial data found by applying the NLP enrichment function 4 DrugBank is the Linked Data version of the DrugBank database  , which is a mo:biography.
Our algorithm learns this single manually configured enrichment as 1 an NLP enrichment function that extracts all named entities types and then 2 a filter enrichment function that filters all non-location triples. As an example  , in case of M 1 Jamendo the manual configuration was just one enrichment function  , i.e.  , NLP-based enrichment to find all locations in mo:biography.
To this end  , we deployed a manual enrichment pipeline  , in which we enrich Jamendo's music data by adding additional geospatial data found by applying the NLP enrichment function 4 DrugBank is the Linked Data version of the DrugBank database  , which is a mo:biography. The goal of our enrichment process is to add a geospatial dimension to Jamendo  , e.g.  , the location of a recording or place of birth of a musician.
