For all the SVM models in the experiment  , we employ the linear SVM. This is because the LETOR data set offers results of Linear Ranking SVM. , 7. The relevancy judgments provided in OHSUMED are scored 0  , 1 or 2 and there are 45 features for each querydocument pair. We have described an experimental method in which learnt uncertainty information can be used to guide design choices to avoid overfitting  , and have run a series of experiments on the benchmark LETOR OHSUMED data set for both types of model. Although it is a continuous timeline  , we split it into two segments to follow the traffic trends seen in Fall and Spring semesters.  LETOR: Using only statistical features associated with matched terms features L1−10 and H1−3 in Tab. We use the centroid-based approach 23  since it is a popular scheme for compact clusters which are similar to the clusters we see in the AlgoViz DSN. For all the SVM models in the experiment  , we employed Linear SVM. For our accuracy studies we primarily use the well-known LETOR benchmark 14  , version 3. Multiple LETOR methods have been tried  , which are different in many ways and we expect them to be complimentary during the final fusion. However  , GERBIL is currently only importing already available datasets. In LETOR  , there are a total of 16 ,140 query-document pairs with relevance judgments  , and 25 extracted features. Again  , and with the exception of Datahub D  , the other data sets exhibit a similar trend. 28 The extensibility of the datasets in GERBIL is furthermore ensured by allowing users to upload or use already available NIF datasets from DataHub. Here we only conjecture that this may be related to the consideration of both presence and absence of terms in the context of personalized spam classification. However  , as witnessed in the popular dataset registry DataHub 2   , dataset descriptions are often missing entirely  , or are outdated  , for instance describing unresponsive endpoints 7. To facilitate search and reuse of existing datasets  , descriptive and reliable metadata is required. For WebKB dataset we learnt 10 topics. As with our first batch of results presented for Ro- bust04  , we again assume the user provides correct feedback. , biblio. Approaches such as point-based measures or cluster centroids are often used to assign newly arriving points to an existing cluster. Considering the large amount of resources per dataset  , we investigate samplebased strategies as follows: SPARQL endpoint from DataHub in step i  , step ii extracts resource types and instances via SPARQL queries 5 that conform to the definition of resource types and instances in Section 2. Feature examples include TF  , IDF  , LMIR and BM25 considering  , result title  , abstract  , body  , url and pagerank values. Each data set is partitioned on queries to perform 5 fold cross-validation. Figure 1shows DSNs based on AlgoViz log data for the months of September and October 2010 with a connection threshold of 10. The KC4 dataset has been taken from the NASA data metrics program http://mdp.ivv.nasa.gov/. SPARQL endpoint from DataHub in step i  , step ii extracts resource types and instances via SPARQL queries 5 that conform to the definition of resource types and instances in Section 2. The optimal parameters for the final GBRT model are picked using cross validation for each data set. TD2004 have more relevant documents per topic than other LETOR collections  , relevant documents remain relatively sparse. The idea is similar to that of sitemap based relevance propagation 24. Figure 8top left shows the accuracy of the classifier for the AlgoViz Fall 2009 dataset. Topics 1  , 2  , 4  , and 5 are mostly related to AlgoViz catalog entries  , These topics are prominent in clusters 2  , 4 and 5. The results of the state-ofthe-art algorithms are provided in the LETOR 3.0. We determine the effectiveness of our algorithm in relation to semi-supervised text classification algorithm proposed in 5 NB-EM.