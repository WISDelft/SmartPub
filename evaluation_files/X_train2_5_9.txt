Then we provide analysis of the importance of features and fields    , and the influence of different query types on LeToR models.To evaluate the effectiveness of the proposed method    , we performed a systematic set of experiments using the LETOR benchmark collections OHSUMED    , TD2004    , and TD2003 and several evaluation measures MAP    , NDCG and precision .For instance    , users prefer to go to a furniture store to buy furniture rather than to a general purpose store such as Walmart.,b1n .On FriendFeed users can comment and start discussions on the aggregated content    , similar to functionalities provided by typical OSNs.We have described an experimental method in which learnt uncertainty information can be used to guide design choices to avoid overfitting    , and have run a series of experiments on the benchmark LETOR OHSUMED data set for both types of model.We preprocessed the OAIster collection to produce the bag-of-words representation as follows: Starting with the 668 repositories in the 9/2/2006 harvest    , we excluded 163 primarily non-English repositories    , and 117 small repositories containing fewer than 500 records    , leaving 388 repositories.In our experiments with retail store data from Walmart    , we generated ranges by sliding    , over the time period    , a window of size 5 days with a step of 3 days.Collaborating with other projects that could benefit from using OAIster    , e.g.For each of these datasets    , we conduct 5-fold cross-validation experiments    , using the default partitions in LETOR.OAIster    , a union catalog of digital resources    , harvests from over seven hundred OAI repositories i.e.The ten largest repositories by size in MB from our 9/2/2006 OAIster harvest are listed in 
98626
The metadata OAIster collects is in Simple Dublin Core format.By extracting a generic query for each theme defined as the most frequent terms of that theme    , we then characterize sentences in the latter by taking 12 features used in the Letor datasets 
EXPERIMENTAL RESULTS
We carried out experiments on DUC 2006 and DUC 2007 datasets 2 .We would like to improve the search and discovery experience on OAIster by allowing users to restrict search results by subject.This paper studies the FriendFeed service    , with emphasis on social aggregation properties and user activity patterns.EXPERIMENT DESIGN
 For our experiments    , we use version 3.0 of LETOR package provided by Microsoft Asia 
EXPERIMENT RESULTS
Comparison of NDCG-Annealing Algorithm with Baselines in LETOR 3.0
We compare our proposed NDCG-Annealing algorithm with those baselines provided in LETOR 3.0.The Topic Model
In this paper we use the topic model for subject metadata enrichment of the OAIster collection.LETOR 2 challenge datasets.Datasets
For the Relevance Feedback experiment    , we used the LETOR testbed 
Experimental Setup
Algorithms
To examine the effectiveness of the proposed algorithm for ranking refinement    , we compared the following ranking algorithms: Base Ranker: It is the base ranker used in the ranking refinement.As also indicated in 
Parameter Sensitivity Study on LETOR 3.0
 As discussed before    , the starting temperature of the Simulated Annealing algorithm must be hot enough.To the best of our knowledge    , this is the first formulation in the context of the standard set of LETOR features 
simtq    , t d  := maxcossgtq    , sgdq    , 0     , 
where sgt is the word embedding vector of term t learned by the SkipGram algorithm 
bm d tq = arg max t d ∈d simtq    , t d  bmqt d  = arg max tq ∈q simtq    , t d  δst    , d = simt    , bm d t 
δsq    , t = simbmqt    , t     , 4 Term repetition is avoided since the number of occurrences of the term t in d is already counted in fL i .Features of relevance view were exactly the same as those in traditional documents ranking    , as were reported in LETOR
The features of intrinsic view were query-independent    , and those social attributes of tweets such as @ mentions    , # hashtags    , and retweeted count were incorporated.The intuition behind depth-pooling is that most relevant documents appear at the top of the ranked list and therefore depth-k pools contain most of them 
 StatAP sampling stratified random sampling: StatAP sampling 
 When the properties of the above document selection methodologies are considered    , one can see that infAP creates a representative selection of documents    , statAP and depthk pooling aim at identifying more relevant documents utilizing the knowledge that retrieval systems return relevant documents at higher ranks    , the LETOR-like method aims at selecting as many relevant documents according to BM25 as possible    , hedge aims at selecting only relevant documents    , and MTC greedily selects discriminative documents.bl1  ,bl2  ,.We have tried using Support Vector Regression RankSVM with linear kernel for pairwise LETOR    , and were trained on a set of error pairs collected using the " web2013 " relevance judgments file.Note that FriendFeed being an aggregation service enables us to study different services from one common observation point    , and allows us to get a unique " sneak peek " on how these social networking and content sharing services are being used by a common set of users.We would also like to thank the University of Michigan for the sample of OAIster transaction log data used in our analyses.The second group of datasets corresponds to well-known LETOR 3.0 Topic distillation tasks    , TD2003 and TD2004 a.k.a.This is a rather surprising result given the wide usage of the LETOR datasets as it suggests that using the same judgment effort    , better collections could be created via other methods.Thus    , creating consistent enriched subject metadata is one of the biggest challenges of the OAIster collection.Finally    , we note that it appears that less active users are less likely to join an aggregation service such as FriendFeed.OAIster can be found online at http://www.oaister.org/    , with over a million records available from over 140 institutions..The results of RankSVM    , RankBoost    , AdaRank and FRank are reported in the Letor data set.Experimental results    , obtained using the LETOR benchmark    , indicate that methods that learn to rank at query-time outperform the state-ofthe-art methods.Results for the analysis of the 2  ,404 OAIster query strings are given in Tables 4 and 5 below.The unique feature of OAIster is that it provides access to metadata pointing to actual digital resources.Suppose that a user interested in comparative shopping wishes to find popular cellphones that have been manufactured in the " USA " and are listed on two distinct data sources: Best Buy and Walmart with at least 300 reviews at each source.Please consult 
Characterization Results 
Network Properties 
Subscription to Services and Aggregation 
This section dives into the social aggregation properties of FriendFeed.This functionality is only possible if we have reliable    , consistent and appropriate subject metadata for each of the ten million records in OAIster.FriendFeed www.friendfeed.com is one such service.Bias-Variance Decomposition of Error 
According to the bias-variance decomposition of error 
METHODS
Data sets
For our experiments we work with three public data sets: TD2004 and MQ2007 from LETOR data sets 
Evaluation Metrics
For model comparison we use two information retrieval metrics: Normalized Discounted Cumulative Gain NDCG 
N DCG@k = N −1 k j=1 grjdj    , 
 where N −1 is a normalization factor chosen so that a perfect ordering of the results will receive the score of one; rj denotes the relevance level of the document ranked at the j-th position; grj is a gain function: 
grj = 2 r j − 1; and dj denotes a discount function.INTRODUCTION
Combining evidence from multiple sources has been studied in various contexts 
.Simple K-nearest neighbour KNN with K set to 20 and Regression Tree was used to perform point-wise LETOR.The tool that transforms OAIster metadata from Simple Dublin Core to our native DLXS Bibliographic Class was modified so that it could ingest the file from the first step    , and output a transformed metadata record.User query strings were extracted by automated means from a sample of OAIster transaction logs recorded a few days each month over several months in 2003 and 2004.FriendFeed allows users either to filter by people or to use a form-based search tool 1 .  , JCPenney    , Best Buy    , and Walmart.Harvested metadata that has no corresponding digital resource is not indexed in OAIster.BBJoin Cost costBBJoin / BOJoin Cost cost
Products Dataset Experiments
In this section    , we evaluate the efficacy of our approaches on a real electronic products dataset collected from two different data sources: Best Buy and Walmart.Our experiments on LETOR 3.0 benchmark dataset show that the  NDCG-Annealing algorithm outperforms the state-of-theart algorithms both in terms of performance and stability.This is because the LETOR data set offers results of linear RankSVM.Suppose that the analyst chooses two such data sources: Best Buy denoted by BB and Walmart denoted by WM.Future Directions for OAIster
The University of Michigan intends to continue researching the use of OAI in a variety of ways.For RSVM    , we can make use of its results provided in LETOR.The statistics of two data sets are summarized in 
Setup
With LETOR data    , since HP and NP are similar tasks but TD is rather different    , we conducted experiments on HP03- to-NP04 and NP03-to-TD04 adaptation    , where the former setting is for adapting to a similar domain and the latter for adapting to a distinct one.,bln Ra Features Regressor 
EXPERIMENTS
To evaluate our ranker selection approach    , we use the LETOR 3.0 dataset 
 In terms of MAP    , RankBoost is the best individual ranker    , followed by FRank and Regression.A friend on FriendFeed is a unidirectional relationship.Commenting on aggregated content facilitates information dissemination in the FriendFeed network.However    , we have found little evidence    , at least for the LETOR OHSUMED data set    , that explicit use of the uncertainty information can improve model performance in terms of NDCG.LETOR Results
 In §7.1.1    , we compare BARACO and MT on the Switching Problem ; in §7.1.2    , we compare BARACO and the EM-based approach 
Switching Problem Results
To address RQ1    , we compare the ROC curves of BARACO and MT on the Switching Problem.  , Walmart.A FriendFeed user can choose to aggregate content from among the supported services into the user's FriendFeed profile page.We have shown very competitive results relative to the LETOR-provided baseline models.In LETOR 3.0 dataset    , each query can only belong to only one category.Aggregator b11  ,b12  ,.For example    , given a new query    , " walmart credit card "     , assume the set of unigrams    , bigrams and trigrams contained in unit vocabulary includes { " walmart "     , " credit "     , " card "     , " credit card " }    , then we only keep " walmart " and " credit card " in the unit set.All presented NDCG    , Precision and MAP results are averaged across the test queries and were obtained using the evaluation script available on the LETOR website.This enriched metadata could then be distributed to meet the needs of access services    , preservation repositories    , and external aggregation services such as OAIster.We are also interested in understanding the characteristics of the FriendFeed social network and how they relate to the characteristics of the social network services that it aggregates.Then using FriendFeed 5 data    , we identified users who also have FriendFeed accounts.For comparative purposes    , considering that the Microsoft and LETOR datasets were designed for a folded cross-validation procedure    , we applied this same strategy to the YA- HOO!The first phase captured the network of FriendFeed users    , while the second phase captured the activity of the users identified in the first phase over a period of five weeks.Those features are then piped into different LETOR algorithms to produce several rank lists    , and eventually all the rank lists are merged using the conventional Reciprocal Rank based data fusion method.We used LETOR 
OHSUMED: Pseudo Relevance Feedback
We compared the performances of Relational Ranking SVM and several baseline methods in Pseudo Relevance Feedback using the OHSUMED data set in LETOR.Data Description
We used the Letor 2 data collection 
Evaluation Measures
 In order to evaluate the performance of the proposed algorithms     , three evaluation measures are applied: Precision    , Mean average precision and Normalized Discount Cumulative Gain 
18 
Mean Average Precision.FriendFeed allows aggregation of information from a number of services that include popular social networking     , video sharing    , photo sharing    , and blogging services.A FriendFeed user can " follow " the activity of other users of this service by subscribing them as " friends " .As an example    , a search performed in OAIster for " double-well Duffing oscillator " retrieves two records    , exactly the same    , but one was harvested from the arXiv.org Eprint Archive repository an original repository and one harvested from the CiteBase repository an aggregator.In §7.1    , we analyse the performance of BARACO and MT on the LETOR data; in §7.2    , we analyse their performance on the WSDM data.On average    , our strategies converge at about 15 iterations on the LETOR datasets    , and around 5 to 10 iterations on the multi-relevance judgment datasets.  , Walmart    , Home Depot    , Subway and McDonald's.Many modem manufacturers and retailers - Walmart is a particularly well known example have found extending the companies boundaries in just this way are central to the 'whole concept of Just in Time and process reengineering.It is likely that monitoring all items for sale at Walmart    , say    , is not of interest.The retrieval performance achieved was at least as good as the LETOR 4.0 baselines.For example    , when taking a random sample of all product items in the Walmart catalog    , more than 40% of the items in the sample are from the segment " Home & Garden " .META SEARCH EXPERIMENTS
For meta search aggregation problem we use the LETOR 
WWW 
NDCGπ    , L@K = 1 GK L K X i=1 2 Lπ −1 i − 1 logi + 1 12 where Lπ −1 i
 is the relevance level of the document with rank i in π    , and GK L is a normalizing constant that ensures that a perfect ordering has an NDCG value of 1.Creating individual preprocessing rules for each repository in the collection is not a scalable solution for OAIster    , or any other large metadata collection.Multiple LETOR methods have been tried    , which are different in many ways and we expect them to be complimentary during the final fusion.This realization has led various retail giants such as WalMart 
RELATED WORK
An attempt has been made to make the process of hiring an auto simpler by an initiative launched in Bangalore by the city police and the transport authority    , called Easy Auto 4 .The experimental results provided in the LETOR collection also confirm this.The introduction of the well-known retrieval models introduced in the past decades can be found in many well written literatures such as 
General Pipeline
Our goal is set to design a system as simple as possible    , without using any external processing engine or resources    , other than the standard Indri toolkit and a third party LETOR toolkit.