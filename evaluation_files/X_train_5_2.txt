There are 16 ,140 query-document pairs with relevance labels. As a result  , the NDCG-Annealing algorithm is more stable and pronounced compared to the baselines in LETOR 3.0 dataset. We also analyze the results of our approach on a different dataset; OHSUMED 5 which is also available in Letor 16. When we use only similarity between the page titles to build the model  , the recommendation framework does not perform well. Table 1summarizes the properties of these data sets.  Easy integration of datasets: We also provide means to gather datasets for evaluation directly from data services such as DataHub. For Movie and SRAA data sets  , we give the mean and standard deviation of the classification accuracies over five runs of the classifiers with each run using randomly chosen examples for training and testing. LETOR: For comparison purposes  , a LETOR-like document selection methodology is also employed. Since a lot of features of LETOR we cannot get  , we droped those columns and then trained the ranking model. The results of RankSVM  , RankBoost  , AdaRank and FRank are reported in the Letor data set. To avoid the aforementioned implication  , these extra documents with low BM25 scores were dropped in the latest LETOR release 13. In LETOR  , data is partitioned in five subsets. The datasets provided in the LETOR There are 106 queries in the OSHUMED dataset. Figure 4shows the results on Letor OHSUMED dataset in terms of MAP and NDCG  , averaged over five trials. When the LETOR collection was built  , the fact that documents with low BM25 score were selected only if they were relevant resulted in BM25 being negatively correlated with relevance in the LETOR collection. As the histogram shows  , relevant documents per topic are quite sparse  , restricting the number of feedback iterations possible with stable evaluation. We use the 5-fold cross validation partitioning from LETOR 10. For these datasets  , there are 64 features extracted for each query-document pair and a binary relevance judgment for each pair is provided. In AlgoViz we used the results in two ways: 1 within the content recommendation block that suggests a list of entries based on the DSN analysis results and 2 within the ranking function that generates the ordered list of entries for users during browse and search operations. Both hedge and LETOR-like document selection methodology   , by design  , select as many relevant documents as possible . In this section  , we provide an overview of the processing steps for generating structured dataset profiles. There are a number of ways in which graphs can be analyzed  , graph partitioning being one. In the first experiment  , we used the Letor benchmark datasets 18: OHSUMED  , TD2003  , and TD2004. The co-occurrence matrices are computed on low level categories thus clearer blocks means better clustering performance. These data sets were chosen because they are publicly available  , include several baseline results  , and provide evaluation tools to ensure accurate comparison between methods. Some users are mainly interested in bibliography entries. For our experiments we work with three public data sets: TD2004 and MQ2007 from LETOR data sets 24 and the recently published MSLR-WEB10K data set from Microsoft Research 1. in two different ways. We justify why  , for typical ranking problems  , this approximation is adequate. We have shown very competitive results relative to the LETOR-provided baseline models. For meta search aggregation problem we use the LETOR 14  benchmark datasets. The SVMRank 5 algorithm was used in this task and five-folds cross validation was done. One example of a project that combines an educational portal with online community is the AlgoViz Portal http: //algoviz.org. Since we are only training on a single topic  , resulting accuracy is far lower than what typically published LETOR results. Combining each time different subsets to make the training  , the validation and the test set  , the LETOR authors create 5 different arrangements for five-fold cross validation. From the extracted dataset metadata i.e. Hence  , we only compare the proposal algorithm with Ranking-SVM  , but not Rank-Boost. For Spam data set  , we give classification accuracies for each user inbox. The collection can be sorted by author  , title  , publication type  , or publication year. We begin by constructing DSNs based on AlgoViz log data from Fall 2009 August 1 to December 31 and Spring 2010 January 1 to May 31. 1. Features in Letor OHSUMED dataset consists of 'low-level' features and 'high-level' features. The model takes into account a user's page viewing history  , page viewing trends captured using DSNs  , and text similarity between page titles. Letor OHSUMED dataset consists of articles from medical journals . This is a highly counterintuitive outcome. definitely  , possibly  , or not relevant. The Datahub data set shows a far more balanced behaviour. The training features are the ones used in LETOR benchmark 2 and are described in 2. The error bars are standard errors of the means. There are 106 queries in the collection. We evaluate our method on three data sets belonging to three different application areas -spam filtering  , movie review   , and SRAA. The number of sampling iterations for the topic model of each month was 200. MAP is then computed by averaging AP over all queries. An exception is the Datahub data set D  , where the distribution of resources in type sets and property sets seems comparable. In both cases  , for any given time span  , if an entry E in AlgoViz received a certain number of views within a cluster whose topics were highly related to that of E  , then E would be weighted more compared to other entries of similar type. Two users were connected only if they viewed at least 10 similar pages within a month. The clustering results along with the topics highlighted in the previous section indicate that AlgoViz users have clusters of interests when it comes to using online resources related to algorithm visualizations. Knowing the groups  , their interests  , and size gives us leverage on better serving the target audience. All presented NDCG  , Precision and MAP results are averaged across the test queries and were obtained using the evaluation script available on the LETOR website. Note that it is commonly believed that Rank-Boost performs equally well as Ranking SVM. According to this methodology  , documents in the complete collection are first ranked by their BM25 scores for each query and the top-k documents are then selected for feature extraction. Hedge finds many relevant documents " common " to various retrieval systems   , thus documents likely to contain many of the query words. The sessions are the nodes and an edge between two sessions indicate they share k common pages. For SRAA dataset we learnt 10 topics on the complete dataset and labeled these 10 topics for all the three classification tasks. The empirical results indicate that even with sparse models  , the ranking performance is still comparable to that of the standard gradient descent ranking algorithm. Data sets. These users are referred to as Anonymous users and have a default user ID of 0. The first evaluation  , based on the LETOR datasets 17  , uses manual relevance assessments as ground-truth labels and synthetic clicks as feedback to BARACO. Figure 11 left shows the performance of the recommendation for the AlgoViz Fall 2009 dataset. The stream-based approach is also applicable to the full data crawls of D Datahub , In Letor  , the data is represented as feature vectors and their corresponding relevance labels . In total  , there are 44 features. Following LETOR convention  , each dataset is divided into 5 folds with a 3:1:1 ratio for training  , validation  , and test set. Despite a small number of registered users  , AlgoViz project leaders are interested in understanding the trends of its overall user base. In the case of SRAA dataset we inferred 8 topics on the training data and labeled these 8 topics for all the three classification tasks discussed above. Our goal is set to design a system as simple as possible  , without using any external processing engine or resources  , other than the standard Indri toolkit and a third party LETOR toolkit. We conducted 5-fold cross validation experiments  , following the guideline of Letor. There are 106 queries in the collection split into five folds. Figure 8 and Figure 9show the experimental results for the two DSNs. We use two AlgoViz DSNs created from log data captured in Fall 2009 and Spring 2010. So instead of IDs  , we rely on other methods to identify users whether registered or unregistered. For various subsets of the datasets discussed above  , we choose number of topics as twice the number of classes. For our empirical analysis  , we use the different segments of the data set provided for the Billion Triple Challenge BTC 2012. Actually  , the results of Ranking SVM are already provided in LETOR. For SVM  , we use the implementation provided by SV M Light 15. Table 1shows the statistics of the datasets included in the LETOR 3.0 benchmark. P -perfect user model setting  , I -informational  , N -navigational LETOR eval- uation. Figure 5and Figure 6show the results on the Letor TD2003 and TD2004 datasets.  LETOR: For comparison purposes  , a LETOR-like document selection methodology is also employed. Thus  , the results reported here refer to non-normalized data. 16  , here we investigate whether a simple unweighted average is sufficient to give improve- ments. We also see from Figure 4 that our NDCG-Annealing algorithm outperforms all the other baseline algorithms on this dataset. In Ranking SVM plus relation  , we make use of both content information and relation information. We compare our proposed NDCG-Annealing algorithm with those baselines provided in LETOR 3.0. If yes  , which one of these methods is better for this purpose ? " We compare our new proposals against several competitive systems  , including structured max-margin learners and RANKBOOST 6. Our method is a hybrid generative-discriminative method where the term weights represent a generative model and the linear discriminant represents a discriminative model of the classification problem . Step i uses the CKAN API to extract dataset metadata for datasets part of the LOD-Cloud group in DataHub. Density 20 for a network with edges E and vertices V is defined as: We employ five different document selection methodologies that are well studied in the context of evaluation  , along with the method used in LETOR for comparison purposes. Two of the top-most topics in the September 2010 DSN include words related to AlgoViz bibliography entries i.e. We also tried different strategies to normalize our feature vectors  , including L2-norm  , z-score and the LETOR normalization procedure 17  , with no improvements. The Ohsumed data set is available from the LETOR website 1 . However  , at very different levels: the probability of knowing the type set for a given property set ranges between 15.15% and 54.85%. One of the prominent collections of AlgoViz is the bibliography of publications related to algorithm visualizations . As shown in Figure 2  , the documents selected by the two methods also exhibit very high similarity to each other. This is because the LETOR data set offers results of linear RankSVM. We compare the NDCG-Annealing algorithm with linear ranking function described in section 3 with baselines provided in the LETOR 3.0 datasets. We tried treating 'partially relevant' as 'irrelevant'  , it did not work well for SVM map . How to optimize towards diversity under the context LETOR is yet another problem to be studied in future. Simple K-nearest neighbour KNN with K set to 20 and Regression Tree was used to perform point-wise LETOR. As our testbed we use the AlgoViz Portal 1 which collects metadata on Algorithm Visualizations and provides community support. The first evaluation is based on the LETOR datasets 17  , which include manual relevance assessments. In the absence of adequate explicit user feedback  , AlgoViz usage data has helped us to generate networks and find common usage patterns. Therefore  , in the case where hundreds of raw features are employed  , ranking functions may need more than 1% of the complete collection to achieve optimal performance. To analyze the impact from various numbers of auxiliary corpora  , we discard Sraa-1 ,2 from Multi-1 ,2 and then applying the C-LDA. Nevertheless  , in a setup similar to LETOR setup  , as in our experiments  , we show that substantially less documents than the ones used in LETOR can lead to similar performance of the trained ranking functions. Finally  , we evaluate the proposed method on LETOR 3.0 benchmark collections1. We report the classification accuracy for spam data set  , and the mean and standard deviation of classification accuracy for movie and SRAA data sets calculated over 5 runs of the algorithms. The results on seven datasets in LETOR 3.0 show that the NDCG-Annealing algorithm can outperform the baselines and it is more stable. The main steps shown in Figure 1are the following: i dataset metadata extraction from DataHub; ii resource type and instance extraction; iii entity and topic extraction; iv topic filtering and ranking; and v dataset profile representation. In particular  , we integrated 6 additional annotators not evaluated against each other in previous works e.g. We randomly split SRAA and WebKB datasets such that 80% is used as training data and remaining 20% is used as test data. The stream-based approach is also applicable to the full data crawls of D Datahub  , As small data sets  , we used A the full Rest subset 22 ,328 ,242 triples  , B an extract of the Datahub subset 20 ,505 ,209 triples and C an extract of the Timbl subset 9 ,897 ,795 triples 7 . Experimental results show that DSN-based recommendation performs better compared to when only text similarity is used. In LETOR 3.0 package  , each dataset is partitioned into five for five-fold cross validation and each fold includes training   , testing and validation sets. The rankers are compared using the metric rrMetric 3. This issue is partially due to the lack of automated mechanisms for generating reliable and up-to-date dataset metadata  , which hinders the retrieval  , reuse or interlinking of datasets. can observe the tendency that the property sets convey more information than type sets. Similar observations can be made for the data set A  , F and G  , though to a lower extent. The experimental results provided in the LETOR collection also confirm this. In Section 5 we describe experiments with the wellknown public ranking data set LETOR  , from Microsoft. A connection threshold of size k for an edge indicates that two users have viewed at least k common pages. We conduced 5-fold cross validation experiments  , using the partitions in LETOR. Given an aggregate ranking Ï€  , and relevance levels L  , NDCG is defined as: The distribution of training and testing sets are similar for the Movie and the SRAA data sets.