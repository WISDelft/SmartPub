We have shown very competitive results relative to the LETOR-provided baseline models.We have tried using Support Vector Regression RankSVM with linear kernel for pairwise LETOR    , and were trained on a set of error pairs collected using the " web2013 " relevance judgments file.In LETOR 3.0 dataset    , each query can only belong to only one category.Aggregator b11  ,b12  ,.We first describe the Thrift-based API    , followed by the DataHub Notebook.Then we provide analysis of the importance of features and fields    , and the influence of different query types on LeToR models.To evaluate the effectiveness of the proposed method    , we performed a systematic set of experiments using the LETOR benchmark collections OHSUMED    , TD2004    , and TD2003 and several evaluation measures MAP    , NDCG and precision .To address these use cases    , and many more similar ones    , we propose DataHub    , a unified data management and collaboration platform for hosting    , sharing    , combining and collaboratively analyzing diverse datasets.Relevant graph partitioning techniques have been studied in areas such as web science 
APPLICATIONS
The clustering results along with the topics highlighted in the previous section indicate that AlgoViz users have clusters of interests when it comes to using online resources related to algorithm visualizations.,b1n .For SRAA dataset we learnt 10 topics on the complete dataset and labeled these 10 topics for all the three classification tasks.All presented NDCG    , Precision and MAP results are averaged across the test queries and were obtained using the evaluation script available on the LETOR website.We have described an experimental method in which learnt uncertainty information can be used to guide design choices to avoid overfitting    , and have run a series of experiments on the benchmark LETOR OHSUMED data set for both types of model.Bias-Variance Decomposition of Error 
According to the bias-variance decomposition of error 
METHODS
Data sets
For our experiments we work with three public data sets: TD2004 and MQ2007 from LETOR data sets 
Evaluation Metrics
For model comparison we use two information retrieval metrics: Normalized Discounted Cumulative Gain NDCG 
N DCG@k = N −1 k j=1 grjdj    , 
 where N −1 is a normalization factor chosen so that a perfect ordering of the results will receive the score of one; rj denotes the relevance level of the document ranked at the j-th position; grj is a gain function: 
grj = 2 r j − 1; and dj denotes a discount function.INTRODUCTION
Combining evidence from multiple sources has been studied in various contexts 
.For SRAA dataset we infer 8 topics on the training dataset and label these 8 topics for all the three classification tasks.DataHub has three key components    , designed to support the above use data collaboration use cases: I: Flexible data storage    , sharing    , and versioning capabilities.For comparative purposes    , considering that the Microsoft and LETOR datasets were designed for a folded cross-validation procedure    , we applied this same strategy to the YA- HOO!Collections currently available through Ensemble include the existing collections of AlgoViz Algorithm Visualization    , CITIDEL computing education resources 
Tools and Services
 Existing resources and tools only cover some of the patron's needs.Simple K-nearest neighbour KNN with K set to 20 and Regression Tree was used to perform point-wise LETOR.SPARQL endpoint from DataHub in step i    , step ii extracts resource types and instances via SPARQL queries 5 that conform to the definition of resource types and instances in Section 2.Those features are then piped into different LETOR algorithms to produce several rank lists    , and eventually all the rank lists are merged using the conventional Reciprocal Rank based data fusion method.bl1  ,bl2  ,.We used LETOR 
OHSUMED: Pseudo Relevance Feedback
We compared the performances of Relational Ranking SVM and several baseline methods in Pseudo Relevance Feedback using the OHSUMED data set in LETOR.For each of these datasets    , we conduct 5-fold cross-validation experiments    , using the default partitions in LETOR.In both cases    , for any given time span    , if an entry E in AlgoViz received a certain number of views within a cluster whose topics were highly related to that of E    , then E would be weighted more compared to other entries of similar type.Finally    , in step 5 the user then decides to document their analysis in the DataHub Notebook see Section 3.3 for details in order to share it with their team.Data Description
We used the Letor 2 data collection 
Evaluation Measures
 In order to evaluate the performance of the proposed algorithms     , three evaluation measures are applied: Precision    , Mean average precision and Normalized Discount Cumulative Gain 
18 
Mean Average Precision.A new DataHub app can be written and published to the DataHub App Center using our SDK via thriftbased APIs see Section 3.3.In the case of SRAA dataset we inferred 8 topics on the training data and labeled these 8 topics for all the three classification tasks discussed above.By extracting a generic query for each theme defined as the most frequent terms of that theme    , we then characterize sentences in the latter by taking 12 features used in the Letor datasets 
EXPERIMENTAL RESULTS
We carried out experiments on DUC 2006 and DUC 2007 datasets 2 .The second group of datasets corresponds to well-known LETOR 3.0 Topic distillation tasks    , TD2003 and TD2004 a.k.a.In §7.1    , we analyse the performance of BARACO and MT on the LETOR data; in §7.2    , we analyse their performance on the WSDM data.Our experiments on LETOR 3.0 benchmark dataset show that the  NDCG-Annealing algorithm outperforms the state-of-theart algorithms both in terms of performance and stability.On average    , our strategies converge at about 15 iterations on the LETOR datasets    , and around 5 to 10 iterations on the multi-relevance judgment datasets.The SRAA corpus contains 73  ,218 UseNet articles from four discussion groups: simulated auto racing    , simulated aviation    , real autos    , and real aviation.EXPERIMENT DESIGN
 For our experiments    , we use version 3.0 of LETOR package provided by Microsoft Asia 
EXPERIMENT RESULTS
Comparison of NDCG-Annealing Algorithm with Baselines in LETOR 3.0
We compare our proposed NDCG-Annealing algorithm with those baselines provided in LETOR 3.0.Version Comparisons and Merging
DataHub allows datasets to be forked and branched    , enabling different collaborators to work on their own versions of a dataset and later merge with other versions.This is a rather surprising result given the wide usage of the LETOR datasets as it suggests that using the same judgment effort    , better collections could be created via other methods.This is because the LETOR data set offers results of linear RankSVM.However    , we have found little evidence    , at least for the LETOR OHSUMED data set    , that explicit use of the uncertainty information can improve model performance in terms of NDCG.The introduction of the well-known retrieval models introduced in the past decades can be found in many well written literatures such as 
General Pipeline
Our goal is set to design a system as simple as possible    , without using any external processing engine or resources    , other than the standard Indri toolkit and a third party LETOR toolkit.LETOR 2 challenge datasets.For RSVM    , we can make use of its results provided in LETOR.The statistics of two data sets are summarized in 
Setup
With LETOR data    , since HP and NP are similar tasks but TD is rather different    , we conducted experiments on HP03- to-NP04 and NP03-to-TD04 adaptation    , where the former setting is for adapting to a similar domain and the latter for adapting to a distinct one.An exception is the Datahub data set D    , where the distribution of resources in type sets and property sets seems comparable.The retrieval performance achieved was at least as good as the LETOR 4.0 baselines.In AlgoViz we used the results in two ways: 1 within the content recommendation block that suggests a list of entries based on the DSN analysis results and 2 within the ranking function that generates the ordered list of entries for users during browse and search operations.As also indicated in 
Parameter Sensitivity Study on LETOR 3.0
 As discussed before    , the starting temperature of the Simulated Annealing algorithm must be hot enough.Datasets
For the Relevance Feedback experiment    , we used the LETOR testbed 
Experimental Setup
Algorithms
To examine the effectiveness of the proposed algorithm for ranking refinement    , we compared the following ranking algorithms: Base Ranker: It is the base ranker used in the ranking refinement.LETOR Results
 In §7.1.1    , we compare BARACO and MT on the Switching Problem ; in §7.1.2    , we compare BARACO and the EM-based approach 
Switching Problem Results
To address RQ1    , we compare the ROC curves of BARACO and MT on the Switching Problem.As part of DataHub    , we are building a version browser to browse and examine versions    , as well as a version graph displaying how versions have evolved for both purposes: differencing and analysis of how versions have evolved    , and for merging versions.,bln Ra Features Regressor 
EXPERIMENTS
To evaluate our ranker selection approach    , we use the LETOR 3.0 dataset 
 In terms of MAP    , RankBoost is the best individual ranker    , followed by FRank and Regression.META SEARCH EXPERIMENTS
For meta search aggregation problem we use the LETOR 
WWW 
NDCGπ    , L@K = 1 GK L K X i=1 2 Lπ −1 i − 1 logi + 1 12 where Lπ −1 i
 is the relevance level of the document with rank i in π    , and GK L is a normalizing constant that ensures that a perfect ordering has an NDCG value of 1.Multiple LETOR methods have been tried    , which are different in many ways and we expect them to be complimentary during the final fusion..The results of RankSVM    , RankBoost    , AdaRank and FRank are reported in the Letor data set.If there are no conflicts    , merging can be done automatically    , otherwise DataHub will need to walk the user through the differences.DataHub has already been used by data scientists in industry    , journalists    , and social scientists    , spanning a wide variety of use-cases and usage patterns.1. sim auto vs sim aviation vs real auto vs real aviation 2. auto sim auto + real auto vs aviation sim aviation + real aviation 3. simulated sim auto + sim aviation vs real real auto + real aviation We randomly split SRAA dataset such that 80% is used as training data and remaining is used as test data.Experimental results    , obtained using the LETOR benchmark    , indicate that methods that learn to rank at query-time outperform the state-ofthe-art methods.To the best of our knowledge    , this is the first formulation in the context of the standard set of LETOR features 
simtq    , t d  := maxcossgtq    , sgdq    , 0     , 
where sgt is the word embedding vector of term t learned by the SkipGram algorithm 
bm d tq = arg max t d ∈d simtq    , t d  bmqt d  = arg max tq ∈q simtq    , t d  δst    , d = simt    , bm d t 
δsq    , t = simbmqt    , t     , 4 Term repetition is avoided since the number of occurrences of the term t in d is already counted in fL i .Features of relevance view were exactly the same as those in traditional documents ranking    , as were reported in LETOR
The features of intrinsic view were query-independent    , and those social attributes of tweets such as @ mentions    , # hashtags    , and retweeted count were incorporated.The experimental results provided in the LETOR collection also confirm this.This way    , DataHub enables many individuals or teams to collaboratively analyze datasets    , while at the same time allowing them to store and retrieve these datasets at various stages of analysis.The intuition behind depth-pooling is that most relevant documents appear at the top of the ranked list and therefore depth-k pools contain most of them 
 StatAP sampling stratified random sampling: StatAP sampling 
 When the properties of the above document selection methodologies are considered    , one can see that infAP creates a representative selection of documents    , statAP and depthk pooling aim at identifying more relevant documents utilizing the knowledge that retrieval systems return relevant documents at higher ranks    , the LETOR-like method aims at selecting as many relevant documents according to BM25 as possible    , hedge aims at selecting only relevant documents    , and MTC greedily selects discriminative documents.In February 2012    , we extracted the list of 220 URIs available on the DataHub site under the " LOD cloud " group    , offering entry points for most of the datasets listed in the LOD cloud.As small data sets    , we used A the full Rest subset 22  ,328  ,242 triples    , B an extract of the Datahub subset 20  ,505  ,209 triples and C an extract of the Timbl subset 9  ,897  ,795 triples 7 .