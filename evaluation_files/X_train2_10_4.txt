Although none of these sites are represented in the WT2g dataset    , we had to take this possibility into account.In the Shop.com dataset    , however    , we have both the product price information and the quantity that a consumer purchased in each record.Aleph is unable to find a good clause even after evaluating the maximum 500K clauses    , thus resulting in relatively worse performance on the WebKB-Department task than the WebKB-Student task.bl1  ,bl2  ,.By extracting a generic query for each theme defined as the most frequent terms of that theme    , we then characterize sentences in the latter by taking 12 features used in the Letor datasets 
EXPERIMENTAL RESULTS
We carried out experiments on DUC 2006 and DUC 2007 datasets 2 .The intuition behind depth-pooling is that most relevant documents appear at the top of the ranked list and therefore depth-k pools contain most of them 
 StatAP sampling stratified random sampling: StatAP sampling 
 When the properties of the above document selection methodologies are considered    , one can see that infAP creates a representative selection of documents    , statAP and depthk pooling aim at identifying more relevant documents utilizing the knowledge that retrieval systems return relevant documents at higher ranks    , the LETOR-like method aims at selecting as many relevant documents according to BM25 as possible    , hedge aims at selecting only relevant documents    , and MTC greedily selects discriminative documents.Sibling relationships were only identiied if the siblings and the parent that links to them were all present in the WT2G collection.WWW2004    , 
Previous Work
Whereas search engines locate relevant documents in response to a query    , web-based Question Answering QA systems such as Mulder 
KNOWITALL was inspired    , in part    , by the WebKB project 
KNOWITALL
 KNOWITALL is an autonomous system that extracts facts    , concepts     , and relationships from the web.GERBIL is not just a new framework wrapping existing technology.In the rest of this paper    , we present and evaluate GERBIL.To this end    , we use GERBIL v1.1.4 and evaluate the approaches on the D2KB i.e.The Gerbil platform already integrates the methods of Agdis- tis 
Results
Results of the experiments run on the Gerbil platform are shown in 
Discussion.Datasets
For the Relevance Feedback experiment    , we used the LETOR testbed 
Experimental Setup
Algorithms
To examine the effectiveness of the proposed algorithm for ranking refinement    , we compared the following ranking algorithms: Base Ranker: It is the base ranker used in the ranking refinement.OKAPI BM25 function is utilized as TF part of weighting function 
Passage Retrieval
Since some pages are extremely long in the wt2g data set    , we became aware of using passages rather than whole pages as the indexing unit is appropriate for the sake of retrieval effectiveness.The WT2G collection is a 2G size crawl of Web documents.LETOR Results
 In §7.1.1    , we compare BARACO and MT on the Switching Problem ; in §7.1.2    , we compare BARACO and the EM-based approach 
Switching Problem Results
To address RQ1    , we compare the ROC curves of BARACO and MT on the Switching Problem.The method of choosing the WT2g subset collection was entirely heuristic.LETOR 2 challenge datasets.Data Set and Evaluation Metrics
Data sets
In this paper    , we use the data sets from the KDDCUP 2005 competition which is available on the Web 1 .We have described an experimental method in which learnt uncertainty information can be used to guide design choices to avoid overfitting    , and have run a series of experiments on the benchmark LETOR OHSUMED data set for both types of model.Rel Doc Densities 
WT2g Link Densities 
Connectivity data
Nick Craswell developed software for extracting hyper-link connectivity information from WT2g.In order to prepare our dataset for OSPC    , we chose the dataset of the TAC KBP 2009 Entity Linking competition    , as this dataset have been extensively used in Entity Linking evaluation.We have tried using Support Vector Regression RankSVM with linear kernel for pairwise LETOR    , and were trained on a set of error pairs collected using the " web2013 " relevance judgments file.Multiple LETOR methods have been tried    , which are different in many ways and we expect them to be complimentary during the final fusion.Most notably    , we have only reported MAP scores for the MoviePilot data.Due to the community effort behind GERBIL    , we could raise the number of published annotators from 5 to 9.EXPERIMENT DESIGN
 For our experiments    , we use version 3.0 of LETOR package provided by Microsoft Asia 
EXPERIMENT RESULTS
Comparison of NDCG-Annealing Algorithm with Baselines in LETOR 3.0
We compare our proposed NDCG-Annealing algorithm with those baselines provided in LETOR 3.0.The experimental results provided in the LETOR collection also confirm this.In the following    , we present current state-of-the-art approaches both available or unavailable in GERBIL.The third dataset is the second largest in Wikia    , Muppet    , whose articles are about the TV series " The Muppet Show " .LIF achieved better recall and F1 than TF*IDF did on WebKB 
DISCUSSION AND RELATED WORK
In the various experiments presented here    , the proposed term weighting methods based on least information modeling performed very strongly compared to TF*IDF.In this section    , inspired by KDDCUP 2005    , we give a stringent definition of the QC problem.Data Description
We used the Letor 2 data collection 
Evaluation Measures
 In order to evaluate the performance of the proposed algorithms     , three evaluation measures are applied: Precision    , Mean average precision and Normalized Discount Cumulative Gain 
18 
Mean Average Precision.For the WebKB task    , QuickFOIL explored on average 28K literals    , whereas Aleph constructed more than 10M clauses.The KDDCUP 2005 winning solution included two kinds of base classifiers and two ensemble classifiers of them..For each of these datasets    , we conduct 5-fold cross-validation experiments    , using the default partitions in LETOR.Then we provide analysis of the importance of features and fields    , and the influence of different query types on LeToR models.For our experiments    , we use two real-life datasets WebKB and HIV    , and synthetic datasets Bongard    , which are summarized in WebKB 
Comparisons.The " Open Knowledge Extraction " challenge at ESWC 7 and frameworks such as GERBIL 
Conclusions
The primary focus of this research proposal is to gain event understanding through employing automated tools and collecting diverse crowd semantic interpretations on different data modalities    , sources and event-related tasks.In LETOR 3.0 dataset    , each query can only belong to only one category.We conclude with a discussion of the current state of GERBIL and a presentation of future work.Zhu    , Kraut    , and Kittur 2014 examine community survival as a function of multiple memberships within Wikia communities.The evaluation of our framework by contributors suggests that adding an annotator to  GERBIL demands 1 to 2 hours of work.We focus in particular on how annotators and datasets can be added to GERBIL and give a short overview of the annotators and tools that are currently included in the framework.The winning solution in the KDDCUP 2005 competition    , which won on all three evaluation metrics precision    , F1 and creativity    , relied on an innovative method to map queries to target categories.Moreover    , all developers reported they felt comfortable—4 points on average on a 5-point Likert scale between very uncomfortable 1 and very comfortable 5—implementing the annotator in GERBIL.This is a rather surprising result given the wide usage of the LETOR datasets as it suggests that using the same judgment effort    , better collections could be created via other methods.However    , we have found little evidence    , at least for the LETOR OHSUMED data set    , that explicit use of the uncertainty information can improve model performance in terms of NDCG.Our experiments on LETOR 3.0 benchmark dataset show that the  NDCG-Annealing algorithm outperforms the state-of-theart algorithms both in terms of performance and stability.The pages in Wikia sum up to more than 33 million .More information can be found at our project webpage http:// gerbil.aksw.org and at the code repository page https: //github.com/AKSW/gerbil.EXPERIMENTAL DESIGN AND RESULT
 Since this paper focuses on the recommendation in ecommerce sites    , we collect a dataset from a typical e-commerce website    , shop.com    , for our experiments.This dataset was also used in the prior work 
3 WEBKB Data.The performance difference between the two is subtle: UP-bm25 was shown superior in MAP on Disks 4 & 5 but inferior in P@10 on WT2G.Starting in 2009 the NIST Text Analysis Conference TAC began conducting evaluations of technologies for knowledge base population KBP.BM25 slightly outperforms LM with Dirichlet prior on the WT2G collection.For RSVM    , we can make use of its results provided in LETOR.Since GERBIL is based on the BAT-framework    , annotators of this framework can be added to GERBIL easily.Even though small    , this evaluation suggests that implementing against GERBIL does not lead to any overhead.To evaluate the effectiveness of the proposed method    , we performed a systematic set of experiments using the LETOR benchmark collections OHSUMED    , TD2004    , and TD2003 and several evaluation measures MAP    , NDCG and precision .With GERBIL    , we aim to push annotation system developers to better quality and wider use of their frameworks.The retrieval performance achieved was at least as good as the LETOR 4.0 baselines.Aleph suffers from this problem starkly on the WebKB- Department task.The WebKB dataset consists of 8275 web-pages crawled from university web sites.WebKB 4 Universities Data WebKB: This data set contains 8    , 282 web pages collected in 1997 from computer science departments of various universities    , which were manually categorized into seven categories such as student    , faculty    , and department.We choose this language pair because its ground-truth Entity Linking annotations are available through the TAC-KBP program .For WebKB dataset we learnt 10 topics.  , 
 Extensibility: GERBIL is provided as an open-source platform 2 that can be extended by members of the community both to new tasks and different purposes.On average    , our strategies converge at about 15 iterations on the LETOR datasets    , and around 5 to 10 iterations on the multi-relevance judgment datasets.In addition    , we created a dataset to study OSPC based on the TAC KBP Entity Linking 2009 task dataset    , which is publicly available 8 .,b1n .Overall    , the developers reported that they needed between 1 and 4 hours to achieve this goal 4x 1-2h    , 1x 3-4h    , see  either the same or even less time to integrate their annotator into GERBIL.Note that in all the results reported    , mentions that contain NIL or empty ground truth entities are discarded before the evaluation; this decision is taken as well in Gerbil version 1.1.4.We used LETOR 
OHSUMED: Pseudo Relevance Feedback
We compared the performances of Relational Ranking SVM and several baseline methods in Pseudo Relevance Feedback using the OHSUMED data set in LETOR.In §7.1    , we analyse the performance of BARACO and MT on the LETOR data; in §7.2    , we analyse their performance on the WSDM data.We bootstrapped this system by transferring the learned model from TAC KBP 2010 thereby circumventing the need for training examples.To the best of our knowledge    , this is the first formulation in the context of the standard set of LETOR features 
simtq    , t d  := maxcossgtq    , sgdq    , 0     , 
where sgt is the word embedding vector of term t learned by the SkipGram algorithm 
bm d tq = arg max t d ∈d simtq    , t d  bmqt d  = arg max tq ∈q simtq    , t d  δst    , d = simt    , bm d t 
δsq    , t = simbmqt    , t     , 4 Term repetition is avoided since the number of occurrences of the term t in d is already counted in fL i .We use both methods in our TAC-KBP evaluation.A full list of features and a complete description of the entity linking system is provided in our TAC KBP notebook paper.While developing GERBIL    , we spotted several flaws in the formal model underlying previous benchmarking frameworks which we aim to tackle in the future.For the free parameters in our Sequential Dependence SD sub-models we estimate the parameters using training data from the TAC KBP 2010 entity linking data    , resulting in settings 
λ T D = 0.29    , λ O D = .21    , and λ U D = 0
 .50.Are the best methods for retrieval over the ad hoc data also the best for the WT2g collection  ?E-commerce Dataset Description
We adopt the consumer purchasing records dataset from Shop.com 1 for model evaluation    , because an important information source leveraged in our framework is the quantity of product that a consumer purchased in each transaction     , which is absent in many of the public datasets.Experimental results    , obtained using the LETOR benchmark    , indicate that methods that learn to rank at query-time outperform the state-ofthe-art methods.Automatic knowledge base population by extracting entity information from large-scale unstructured text data has been shown to be a very challenging task in the recent TAC KBP program 1 .In Section 7.1 we directly compare the approaches on the basis of its results achieved with GERBIL.Aggregator b11  ,b12  ,.The WT2G collection is a general Web crawl of Web documents    , which has 2 Gigabytes of uncompressed data.These values are depicted inside a rectangle in 
Spreading activation
In a first link-based strategy    , we chose the spreading activation SA approach 
RSVD i  = SIMD i     , Q + λ · SIMD j   ,Q j=1 k ∑ Using 
all the incoming and outgoing links    , and for different values of the parameter λ    , in most cases did not result in retrieval improvement within the WT2g corpus 
RSVD 4  = SIMD 4     , Q + λ · SIMD 2     , Q + λ · SIMD 8     , Q = 90 + 0.1 · 60 + 0.1 · 100 = 106 
 The similarity value of non-retrieved documents e.g.Features of relevance view were exactly the same as those in traditional documents ranking    , as were reported in LETOR
The features of intrinsic view were query-independent    , and those social attributes of tweets such as @ mentions    , # hashtags    , and retweeted count were incorporated.Our approach was based on using the WT2g dataset    , consisting of 247  ,491 HTML documents at 2GB storage requirements.The online version of GERBIL can be accessed at http://gerbil.aksw.org/gerbil.Currently    , GERBIL offers 9 entity annotation systems with a variety of features    , capabilities and experiments.The second group of datasets corresponds to well-known LETOR 3.0 Topic distillation tasks    , TD2003 and TD2004 a.k.a.To this end    , we provide two main approaches to evaluating entity annotation systems with GERBIL.As a second future work    , we plan use our motif framework as a way to analyze other evolving collaborative systems    , such as non- Wikimedia Wikis    , such as Wikia and Conservapedia    , which have very different editing policies and user bases.Their work found that higher levels of joint memberships between Wikia communities was correlated with success.The results of this experiment are shown in 
CONCLUSION AND FUTURE WORK
In this paper    , we presented and evaluated GERBIL    , a platform for the evaluation of annotation frameworks.Experiments on the KDDCUP 2005 data set show that the bridging classifier approach is promising.The statistics of two data sets are summarized in 
Setup
With LETOR data    , since HP and NP are similar tasks but TD is rather different    , we conducted experiments on HP03- to-NP04 and NP03-to-TD04 adaptation    , where the former setting is for adapting to a similar domain and the latter for adapting to a distinct one.,bln Ra Features Regressor 
EXPERIMENTS
To evaluate our ranker selection approach    , we use the LETOR 3.0 dataset 
 In terms of MAP    , RankBoost is the best individual ranker    , followed by FRank and Regression.Those features are then piped into different LETOR algorithms to produce several rank lists    , and eventually all the rank lists are merged using the conventional Reciprocal Rank based data fusion method.We use the Gerbil testing platform 
Evaluation metrics.Ensemble of Classifiers
The winner of the KDDCUP 2005 competition found that the best result was achieved by combining the exact matching method and SVM.The introduction of the well-known retrieval models introduced in the past decades can be found in many well written literatures such as 
General Pipeline
Our goal is set to design a system as simple as possible    , without using any external processing engine or resources    , other than the standard Indri toolkit and a third party LETOR toolkit.We compare our approach to the University of Washington submission to TAC-KBP 2013 
 F 1  over this submission    , evaluated using a comparable approach.To achieve this goal    , we surveyed the workload necessary to implement a novel annotator into GERBIL compared to the implementation into previous diverse frameworks.This is because the LETOR data set offers results of linear RankSVM.WebKB This dataset contains webpages from computer science departments at around four different universities 7 .WebKB: The WebKB dataset 3 contains 8145 web pages gathered from university computer science departments.All presented NDCG    , Precision and MAP results are averaged across the test queries and were obtained using the evaluation script available on the LETOR website.Accordingly    , the connectivity data was also distributed by ftp in a highly compressed format based on WT2g document numbers.The second collection is the largest provided by the Wikia service    , Wookieepedia    , about the Starwars universe.Simple K-nearest neighbour KNN with K set to 20 and Regression Tree was used to perform point-wise LETOR.EXPERIMENTAL RESULTS
For evaluating our methods    , we used WebKB datasets
We also test the accuracy of SimFusion algorithm.We evaluate our algorithm on the purchase history from an e-commerce website shop.com.The other two measures are defined according to the standard measures to evaluate the performance of classification     , that is    , precision    , recall and F1-measure 
F 1 = 2 × P × R/P + R 11 
" performance " adopted by KDDCUP 2005 is in fact F1.Some examples of such data include organizational and personal web pages e.g    , the WebKB benchmark data set    , which contains university web pages    , research papers e.g.The study was performed through a webpage mimicking the look-and-feel of the moviepilot website    , on this page users were presented with a random selection of movies they had previously rated    , with the ratings withheld.Those functions    , however    , tend to overfit the given rating set R and are likely to degrade on the complement of R. 
USER STUDY
In order to empirically estimate the magic barrier    , a user study on the real-life commercial movie recommendation community moviepilot 4 was performed.META SEARCH EXPERIMENTS
For meta search aggregation problem we use the LETOR 
WWW 
NDCGπ    , L@K = 1 GK L K X i=1 2 Lπ −1 i − 1 logi + 1 12 where Lπ −1 i
 is the relevance level of the document with rank i in π    , and GK L is a normalizing constant that ensures that a perfect ordering has an NDCG value of 1.To address this problem    , we aim to develop/implement novel measures into GERBIL that make use of scores e.g.Thereafter    , we present the GERBIL framework.More information about GERBIL and its source code can be found at the project's website.Knowledge Base Population
As a result of our participation in the 2015 TAC KBP Slot Filler Validation Task    , we have accumulated an interesting dataset of 69 automatically extracted knowledge bases from all participating systems.ACKNOWLEDGEMENTS
 Introduction
 The goal of the Text Analysis Conference Knowledge Base Population TAC-KBP Slot Filling SF task 
1 Supervised classification.Results
The average classification accuracies for the WebKB data set are shown in 
SIGIR 2007 Proceedings 
The Number of Factors
As we discussed in Section 3    , the computational complexity of each iteration for solving the optimization problem is quadratic to the number of factors.moviepilot provides its users with personalized movie recommendations based on their previous ratings.This result in itself is of high practical significance as it means that by using GERBIL    , developers can evaluate on currently 11 datasets using the same effort they needed for 1    , which is a gain of more than 1100%.Bias-Variance Decomposition of Error 
According to the bias-variance decomposition of error 
METHODS
Data sets
For our experiments we work with three public data sets: TD2004 and MQ2007 from LETOR data sets 
Evaluation Metrics
For model comparison we use two information retrieval metrics: Normalized Discounted Cumulative Gain NDCG 
N DCG@k = N −1 k j=1 grjdj    , 
 where N −1 is a normalization factor chosen so that a perfect ordering of the results will receive the score of one; rj denotes the relevance level of the document ranked at the j-th position; grj is a gain function: 
grj = 2 r j − 1; and dj denotes a discount function.We have shown very competitive results relative to the LETOR-provided baseline models.To alleviate this problem    , GERBIL allows adding additional measures to evaluate the results of annotators regarding the heterogeneous landscape of gold standard datasets.The collocations were extracted from the TAC KBP collection 
One entity per discourse
In order to estimate OSPD we divided the number of times a mention string referred to different entities in the document with the number of times a mention string occurred multiple times in the document.As also indicated in 
Parameter Sensitivity Study on LETOR 3.0
 As discussed before    , the starting temperature of the Simulated Annealing algorithm must be hot enough.Next    , we experiment with the extent that the algorithms can produce quality recommendations for groups    , using the MoviePilot data.The results of RankSVM    , RankBoost    , AdaRank and FRank are reported in the Letor data set.In 2013    , Jiaul H. Paik 
w ′′ q i     , d = log pq i |d= log dl dl + µ p ml q i |d + µ dl + µ p ml q i |c 4 
EXPERIMENTAL SETTING
We conduct experiments on eight standard collections    , which include AP88-89 with queries 51-100    , AP88-90 with queries 51-150    , FBIS with queries 351-450    , FT91-94 with queries 301-400    , LA with queries 301-400    , SJMN1991 with queries 51-150    , WSJ87-92 with queries 151-200 and WT2G with queries 401-450.For example    , for the category " staff " of the WebKB dataset    , the F 1 measurement is only about 12% for all methods.INTRODUCTION
Combining evidence from multiple sources has been studied in various contexts 
.The two datasets are the WebKB data set
Methods
The task of the experiments is to classify the data based on their content information and/or link structure.KddCUP: The KddCup database is quite large    , but it contains large clusters of identical objects.Some of these queries have produced quite impressive results using the WT2g dataset and associated connectivity data.OKAPI BM25 function is utilized as the TF part of weighting function 
Passage Retrieval
Since some pages are extremely long in the wt2g data set    , we became aware that using passages rather than whole pages as the indexing unit is appropriate for the sake of retrieval effectiveness.GERBIL aims to be a central repository for annotation results without being a central point of failure: While we make experiment URLs available    , we also provide users directly with their results to ensure that they use them locally without having to rely on GERBIL.For comparative purposes    , considering that the Microsoft and LETOR datasets were designed for a folded cross-validation procedure    , we applied this same strategy to the YA- HOO!For each input URL the server would respond with a list of incoming links from other WT2g documents and outgoing links.