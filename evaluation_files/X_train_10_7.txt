Table 3 shows the various statistics about the datasets. The purpose of the MESUR project is to study usage behavior in the scholarly process and therefore  , usage modeling is a necessary component of the MESUR ontology. An interesting ontology-based approach was developed by the Ingenta MetaStore project 19. use  , it is designed at a level of generality that does not directly support the granularity required by the MESUR project. use  , it is designed at a level of generality that does not directly support the granularity required by the MESUR project. 2. People with different mobility patterns significantly differ in the topics they talk about and terms they use  , indicating a fruitful area of further study. In general  , such a set of features is based on datasets and vocabularies used in some LOD collection  , e.g. The assumptions we make on the considered dataset are as follows. While the frequency function of walmart may not appear unusual  , showing only that it is more popular during the day than at night  , it is in fact distinctive enough such that it correlates very well with other large retailers. OpenStreetMap. The MESUR project makes use of a triple store to represent and access its collected data. 3. The distribution of training and testing sets are similar for the Movie and the SRAA data sets. The poor performance of SVM-DBSCAN is mainly due to the small number of attributes used when compared with the original proposed method described in 17. The requirement to handle a variety of semantic relationships publishes  , cites  , uses and different types of content bibliographic data  , citation data  , usage data  , led MESUR to define a context-centric OWL ontology that models the scholarly communication process 19 3 . In certain cases  , the usage data is provided by the source in an anonymized form  , in other cases MESUR is responsible for the required processing. Hence  , we envision some extensions to Triplify such as a more external annotation of the SQL views in order to allow optionally SPARQL processing on Triplify endpoints. Additionally  , we employed Triplify to publish the 160GB of geo data collected by the OpenStreetMap project. The dataset is the Billion Triple Challenge 2009 collection. In order to publish the OpenStreetMap data  , we performed some preprocessing of the data structures. All figures are generated by our modified version of Java OpenStreetMap Editor 2 which is a map editor for OpenStreetMap 3 written in Java. The ultimate answer to this question depends on the exact data and queries used  , though based on our experimental analysis above  , we believe that an adaptive materialization strategy provides the best trade-off for running provenanceenabled queries over Web Data in general. On the BDBComp collection  , SAND outperforms all methods under all metrics by more than 60%. It is meaningful to compute the similarity between every two cameras  , but not so meaningful to compute that for each camera and each TV  , as an overall similarity between cameras and TVs should be sufficient. Defining a model of the scholarly communication process represented as an RDF/OWL ontology 3. This reference data set forms the basis for a program aimed at the identification  , validation and characterization of a range of usage-based metrics. This may be true for a certain point-feature representation of the cities but is not correct for all points inside the city boundaries. To examine as many different implementations and hosts as possible  , we noted that the Billion Triple Challenge 2014 13 dataset consisted of a 4 GTriple corpus of spidered Web data. BDBComp has been designed to be OAI compliant and adopts Dublin Core DC as its metadata standard. , products  , organizations  , locations  , etc. The most general class in OWL is owl:Thing. We consider the area of Central London  , which consists of 3 ,368 street segments. The proposed MESUR ontology is practical  , as opposed to all encompassing  , in that it represents those artifacts and properties that  , as previously shown in 4  , are realistically available from modern scholarly information systems. Table 1lists the five highest-ranked journals according to their usage 5 at LANL  , one of the initial usage data sets in the MESUR reference data set. Given both usage and bibliographic data  , it will be possible to generate and validate metrics for understanding the 'value' of all types of scholarly artifacts. The match between geolocation and language improves when we compare location breakdown with the language breakdown for blogs collected by BlogPulse in October 2006. However  , the timeconsuming process of aggregation  , filtering  , parsing  , and deduplicating 1 billion usage events was terminated only recently . The BTC dataset contains 10 million quadruples  , but we used smaller excerpts containing 100  , 250 and 500 thousand unique quadruples. Table 2summarizes the most popular point-of-interest annotations currently found in the OpenStreetMap data. author  , and action e.g. For example  , consider the hierarchical categories of merchandise in Walmart. In a medium sized business or in a company big as Walmart  , it's very easy to collect a few gigabytes of data. So far  , MESUR reached agreements for the exchange of usage data with 14 parties  , and as a result has compiled a data set covering over 1 billion article-level usage events  , as well as all associated bibliographic and citation data. , a huge collection of RDF graphs that was crawled by a Linked Data crawler like the Billion Triple Challenge dataset. The MESUR project was started in October of 2006 and thus  , is still in its early stages of development. The breakdown of usage data sources is as follows 2 : Publishers Six major international scholarly publishers. Usage instructions and further information can be also found at http://LinkedGeoData.org. Example 2. Among the blog document set 100 ,649 feeds 38GB  , 2.8 million permalinks 75GB  , and 325 ,000 homepages 20GB  , only the permalinks were used in our experiment. on the basis of scholarly usage. In conjunction with the widespread use of smartphones and GPS enabled devices  , this has resulted in a large number of RDF datasets containing geospatial information  , which is of high importance in several application scenarios  , such as navigation  , tourism  , and location-based social media. Standard economic literature users Euclidean distance and location games to model this phenomena; one of our contributions is suggesting that Jacquard distance is a more accurate model to capture the nuances of user tastes. We also find this to be true for queries in many other areas; for example  , newspapers  , airlines  , and banks among others also tend to have high correlation among themselves. Still  , the mapping can be inhomogeneous some zones can be more detailed annotated than others. 2 How would you grade your knowledge about the Dublin Core metadata standard ? The spatial data is collected by the OpenStreetMap 5 project and it is available in RDF format. The Hilliness. As a result  , one can assume that substantial usage data sets must be aggregated from a variety of sources in order to derive conclusions that have global reach 3 . This is not surprising  , as the BlogPulse blog data was used as a source set of blog urls for harvesting blog author profiles. IW3C2 reserves the right to provide a hyperlink to the author's site if the Material is used in electronic media. This article presents  , the OWL ontology 17 used by MESUR to represent bibliographic  , citation and usage data in an integrated manner. The dynamic of the OpenStreetMap project will ensure a steady growth of the dataset. , OpenStreetMap or Open Government Data data  , a restaurant guide  , etc. , product recommendation on shopping websites  , collaborator and patent recommendation in academia  , friend recommendation on social networks  , and personalized web search. The paper is structured as follows: We motivate the need for a simple RDB-to-RDF mapping solution in Section 2 by comparing indicators for the growth of the Semantic Web with those for the Web. We filter out those points which are either outside of the city boundary or in the ocean. To describe those segments  , we rely on data gathered and distributed for free by OpenStreetMap OSM a global group of volunteer cartographers who maintain free crowdsourced online maps and by Ordnance Survey the national mapping agency for Great Britain. In order to do this  , the MESUR project makes use of a representative collection of bibliographic  , citation and usage data. Large Linked Datasets. As a result  , in order to improve triple store query efficiency  , MESUR stores such data in a relational database  , and the MESUR ontology does not explicitly represent these literals. The results strongly point towards the imminent feasibility of usage-based metrics of impact. Mining such a vast data set in an efficient  , performing  , and flexible manner presents significant challenges regarding data representation and data access. Session-based grouping: Usage data is typically recorded and hence provided to MESUR as a time-sequential list of individual events recorded by an information system; different events generated by the same agent in the course of a certain time span are not grouped. Opinion identification is accomplished by combining the four opinion modules that leverage various evidences of opinion e.g  , Opinion Lexicon  , Opinion Collocation  , Opinion Morphology. Furthermore  , the extended ontology includes the mappings resulted by the schema matching. Figure 1provides a general overview of the the various stages of the MESUR project. RDF 15 triple databases are the natural habitat for data represented in this manner  , and they provide great flexibility for data analysis without the need for extensive upfront application design. This collection was created by us and contains the 10 largest ambiguous groups found in BDBComp. With the advent of the Web and mobile devices  , we are observing a boom in local search: that is  , searching local businesses under geographical constraints. The principles espoused by the OntologyX 5 ontology are inspiring. On the BDBComp collection  , SAND outperformed two unsupervised methods in more than 36% under the pF1 metric and in more 4% under the K metric. Furthermore  , the program prioritizes mutations based on their potential functional significance synonymous vs. non-synonymous substitutions as well as frequency. For SRAA dataset we learnt 10 topics on the complete dataset and labeled these 10 topics for all the three classification tasks. However  , the default crawler may end up spidering many pages of the catalog at the cost of possibly missing pages in categories of interest to subscribers  , such as investor relations or press release pages. In addition to applications in retail and distribution  , RFID technology holds the promise to simplify aircraft maintenance  , baggage handling  , laboratory procedures  , and other tasks. Figure 6 presents the complete taxonomy of the MESUR ontology. For Spam data set  , we give classification accuracies for each user inbox. Information about trees and parks is extracted from OpenStreetMap. Table 1 shows more detailed information about the collections and its ambiguous groups. The Blog06 test collection includes a crawl of feeds XML  , associated permalinks HTML  , retrieval units  , and homepages during Dec 2005 through early 2006. OntologyX uses context classes as the " glue " for relating other classes  , an approach that was adopted for the MESUR ontology. Future work will present benchmark results of the MESUR triple store. This enhancement enables a variety of new Linked Data applications such as geo data syndication or semantic-spatial searches. IDF was calculated on the corpus of all 429 ,183 blog posts from the 4th July that were contained in the original Blogpulse corpus. OntologyX also helped to determine the primary abstract classes for the MESUR ontology. and provide similar products and services e.g. For SVM  , we use the implementation provided by SV M Light 15. We report the classification accuracy for spam data set  , and the mean and standard deviation of classification accuracy for movie and SRAA data sets calculated over 5 runs of the algorithms. This is represented in Figure 5where an edge denotes a rdfs:subClassOf relationship. The stream-based approach is also applicable to the full data crawls of D Datahub , Experiments are performed on Web data taken from the Billion Triple Challenge and the Web Data Commons datasets. First  , we observe that the degree distributions are greatly affected by the existence of splogs. The BTC data set has been crawled from the web in a typical web spider fashion and contains about 1.44 billion triples. The MESUR ontology provides three subclasses of owl:Thing. A significant amount of data processing must be performed to turn the heterogeneous usage data collections obtained from a variety of sources into a reference data set that provides a solid basis to perform cross-source analysis: 1. Researching sampling bias: MESUR examines the effects of sampling biases on its reference data set to determine whether and how a usage data set can be compiled that is representative of global scholarly us- age. We evaluate our approach using the evaluation framework used in the Semantic Search Challenge 2010 3 . We extract a set of tourist attractions in the metadata of OpenStreetMap. The dataset for the ELC task is the Billion Triple Challenge dataset 2 . No holonymy/meronymy composite class definitions are used at this stage of the ontology's development. The Indian middle class represents a huge burgeoning market. While several services exist with similar characteristics  , few  , if any  , comprehensive studies of such services have been reported in the DL literature. In particular  , OpenStreetMap OSM is an initiative for crowdsourcing map information from users. We present a high-level * This work was partly supported by the National Science Foundation with grants IIS-9984296 and IIS-0081860. Opinion modules require opinion lexicons  , which are extracted from training data. This assumption seems to be confirmed by the pattern that emerges as the MESUR reference data set grows and becomes more diverse over time. Our method is a hybrid generative-discriminative method where the term weights represent a generative model and the linear discriminant represents a discriminative model of the classification problem . Without considering the context  , Baseline2 recommends the homepage of Sears as the first choice. are identifiers typically generated for maintaining referential links. Thus  , we find English  , Chinese and Russian languages to be strongly represented as the location segmentation implies. The ranking is based on about 1.5 million usage events. The presence of known SNPs derived by scanning dbSNP within each individual DNA are also noted on this viewer  , thus commonly occurring polymorphisms can be quickly eliminated from further analysis. In contrast  , during the second quarter in 2014  , the second user is interested in " center  , partner  , WalMart  , game  , player  , Oklahoma " that are about business   , politics and some sports. Other applications demand tags with enhanced capabilities. Most of the proposed systems for this task see for example 6 exploit IR indexing and ranking techniques over the RDF dataset used at the Billion Triple Challenge 2009. Another significant component of the MESUR project is the development of a scholarly ontology that represents bibliographic  , citation  , usage concepts  , along with concepts for expressing different artifact metrics. We constructed 20 training topics from BlogPulse http://www.blogpulse.com/ and Technorati search http://www.technorati.com/ archives and manually evaluated the search results of the training topics to generate the training data set of 700 blogs. Using various data sources of substantial size gives the opportunity to find intended POIs  , which may fall into multiple concepts ranging from rather generic to more detailed ones such as " restaurant " vs. " pizzeria. " Therefore  , the MESUR project uses a combination of a relational database to store and query item e.g. Creating a reference data set: MESUR has invested significant energy to compile a large-scale col- 1 Pronounced " measure "   , an acronym for " Metrics from Scholarly Usage of Resources " . The MESUR project will proceed according to the following project phases: 1. Note that the connection between the bibliographic record and the usage event occurs through the doc id bolded properties. We imported the Shapefiles into a PostGIS database and created virtual geospatial RDF views on top of them using Ontop-spatial  , as described at https://github. To account for potential measurement errors when matching social media data with streets  , we add a buffer of 22.5 meters around each street's polyline. in the triple store  , as done by Ingenta  , is not essential. For various subsets of the datasets discussed above  , we choose number of topics as twice the number of classes. After excluding splogs from the BlogPulse data  , we 14 for the BlogPulse dataset  , we replicate the result that the cumulative in-degree and out-degree distributions show smoother curves  , as shown in Figure 3. This ontology forms the basis for the representation of the reference data set in the MESUR infrastructure. For this year's task is based on Billion Triple Challenge 2009 dataset. Unfortunately  , again  , the Ingenta ontology does not support expressing usage of scholarly documents  , which is a primary concern in MESUR. Defining and validating usage-based metrics: MESUR defines a wide range of usage-based metrics  , calculates them for the established reference data set  , and assesses their validity and reliability. This resulted in a list of 312 endpoints. We also perform a dataset analysis and develop a cost model that provide insight into why particular strategies are effective for Web Data. After queries have been represented by time series  , our goal is to analyze the underlying structure of query logs. The results are the worst for Gene data source  , because the classifier has poor performance  , as we had shown earlier in Table II. Here we only conjecture that this may be related to the consideration of both presence and absence of terms in the context of personalized spam classification. Many modem manufacturers and retailers - Walmart is a particularly well known example have found extending the companies boundaries in just this way are central to the 'whole concept of Just in Time and process reengineering. The MESUR reference data now consists of 1 billion individual usage events that were recorded at the documentlevel and processed as described above. 1 Crawled during February/March 2009  , it comprises about 1.14 billion RDF statements. We took SPARQL Endpoints from the SPARQLES survey 3  , vocabularies from Linked Open Vocabularies LOV 2 and prefix.cc  , and we augmented these data with spidered data from the Billion Triple Challenge BTC 2014 13 dataset. The simplest RFID tag stores only a 96-bit identifier called the EPC. OpenStreetMap OSM maintains a global editable map that depends on users to provide the information needed for its improvement and evolution. We evaluate our method on three data sets belonging to three different application areas -spam filtering  , movie review   , and SRAA. As a result  , all usage data in the MESUR reference data set is anonymized both regarding individual and institutional identity. For instance  , users prefer to go to a furniture store to buy furniture rather than to a general purpose store such as Walmart. a5 derives from the observation that because of the rich context of blogs  , captured for example in hyperlinked sources  , important terms may not actually be frequent in the post itself  , such that their being unusual high IDF creates a better indicator of importance 10. Our experimental results also show that: 1 there is some sensitivity of the method to the choice of the user-defined parameter  , Ï†max  , although there are some ranges of values in which the results are very stable and 2 the combination of the first step of our method with other supervised ones does not produce good results as we obtained with SAND. We use the Billion Triple Challenge BTC collection 3   , a publicly available Semantic Web crawl; we consider this collection as a reasonable sample of Linked Open Data LOD. This design choice was a major factor that prompted the engineering of a new ontology for bibliographic and usage modeling. 1: 1. Thus  , we aimed at augmenting folksonomy-style tagging by more standard ways of assigning metadata. For instance  , the MESUR ontology does not have a direct relationship between an article and its publishing journal. 7 The MESUR website offers detailed information on metric definitions and abbreviations: http://www.mesur.org/ This results in irregular shapes for the cumulative degree distributions  , which represent the proportion of blogs having at least k in-links or out-links. Future analysis will focus on determining which request types most validly represent user interest. The goal of LinkedGeoData is to add a spatial dimension to the Semantic Web. After deduplication   , there are about 886 million triples  , 175 million resources  , and 296 million literals. However  , it was more convenient for us to download the most up-todate original OpenStreetMap data about Bremen  , available as Shapefiles 10 . OpenStreetMap datasets are available in RDF format from the LinkedGeoData project 9 . Overall  , the project had produced a 160GB database of geo data until July 2008  , in some regions surpassing commercial geo data providers in terms of precision and detail. We tested and evaluated Triplify by integrating it into a number of popular Web applications. LinkedGeoData uses the information collected by the OpenStreetMap project with the aim of providing a rich integrated and interlinked geographic dataset for the Semantic Web. In BlogPulse  , according to the splog detection methodology presented in 14  , the percentage of splogs is 7.48%. The proposed poster is divided into two primary components . This poster provides an overview of the MESUR project's workplan and architecture  , and will show preliminary results relating to the characterization of its semantic network and a range of usage-based impact metrics. Not all nodes in this Semantic Web graph are entities; identifying the nodes which refer to an entity is one of the challenges introduced by the task. This indicates that our validation algorithm can recognize the true schema attributes with a high accuracy. For Movie and SRAA data sets  , we give the mean and standard deviation of the classification accuracies over five runs of the classifiers with each run using randomly chosen examples for training and testing. The Web Data Commons project extracts all Microformat  , Microdata and RDFa data from the Common Crawl Web corpus  , the largest and most up-to-data Web corpus that is currently available to the public  , and provides the extracted data for download in the form of RDF-quads and also in the form of CSV-tables for common entity types e.g. In the BDBComp collection  , SAND outperforms the KWAY and SVM-DBSCAN methods by more than 36% under the pF1 metric. Their similarity   , if needed  , is derived based on the similarity information stored in the tree path. Generating maps of science: MESUR produces maps of science on the basis of its reference data set. The MESUR ontology was engineered to make a distinction between required base-relationships and those  , that if needed  , can be inferred from the baserelations . The most common indicator of journal status is Thomson Scientific's journal Impact Factor IF that is published every year for a set of about 8 ,000 selected journals. Unique identifiers for these items are shared among these storage infrastructures and allow jumping from one to the other as needed. The final project outcome will be the publication of guidelines with regards to the properties of various usage-based impact metrics  , and how they can be appropriately applied. This diagram primarily serves as a reference. This open-source alternative mapping service also publishes regular database dumps. 3 How would you grade your knowledge of bibliographic self-archiving after using the BDBComp service ? It aims to pave the way for an inclusion of usage-based metrics into the toolset used for the assessment of scholarly impact and move the domain beyond the longestablished and often disputed IF. a vector  , to represent the query " Walmart " which is showed in Figure 1as follows: The application of opinion modules is similar to on-topic retrieval optimization in that opinion scores generated by modules act as opinion reranking factors to boost the ranks of opinionated blogs in the topic-reranked results. Along with novel models of scholarly evaluation  , advances in semantic network analysis algorithms and large-scale data management techniques have and will continue to be produced. However  , before making this service available it was necessary to collect some data to construct its " seed " collection. by better interlinking the data with other Linked Data datasets and providing a proper ontology for querying. In all other four situations there is some drop in effectiveness . It is based on a large and active community contributing both data and tools that facilitate the constant enrichment and enhancement of OSM maps. A novel approach to data representation was defined that leverages both relational database and triple store technology. on whether the street is in or near a park. The MESUR project attempts to fundamentally increase our understanding of usage data. As future work  , we intend to evaluate the impact of the service in the expansion of BDBComp as well as on its sustainability. It is likely that monitoring all items for sale at Walmart  , say  , is not of interest. In addition to the self-archiving service  , we envisage two other ways to collect metadata for the repository: 1 by extracting them from existing Web sites  , for instance  , by using tools such as the Web- DL environment 1 After excluding splogs from the BlogPulse data  , we Thus  , in addition to the two tables required to represent the entity types work and set  , there is a separate table for each multivalued attribute. Since MESUR follows an approach of usage data analysis inspired by clickstream concepts 12  , 11 grouping events is an essential processing sub-task that needs to be performed before ingesting the usage data into the reference data set. We randomly split SRAA and WebKB datasets such that 80% is used as training data and remaining 20% is used as test data. 7 The MESUR website offers detailed information on metric definitions and abbreviations: http://www.mesur.org/ With the addition of the Thomson Scientific journal Impact Factor a set of 47 metrics of scholarly impact result. The Billion Triple Challenge 1 is a collection of crawled Linked Data that is publicly available and that is often used in Big Data research. The user's interests are almost stable and mainly focus on the design of apps. While the triple store is still a maturing technology  , it provides many advantages over the relational database model. in the following way: the first two recommendations are irrelevant  , and the first relevant recommendation is at the third rank of the result list. The context construct is intuitive and allows for future extensions to the ontology. Instead  , there exists a publishing context that serves as an N-ary operator uniting a journal  , the article  , its publication date  , its authors  , and auxiliary information such as the source of the bibliographic data. We made several approaches to ensure that we visited a large and representative section of the open Semantic Web. Terabytes of raw data are ubiquitously being recorded in commerce  , science and government. The detail of our data preparation can be found in Section 6. Although the vlHMM and Baseline2 have comparable precision and recall in Test0  , the vlHMM outperforms the baseline substantially in Test1  , where the context information is available. However  , their scalability and retrieval efficiency are generally not on a par with the most competitive relational database products . The Web Data Commons project extracts all Microformat  , Microdata and RDFa data from the Common Crawl Web corpus and provides the extracted data for download in the form of RDF-quads or CSV-tables for common entity types e.g. Considering all the blogs in the BlogPulse data  , both in-degree and out-degree distributions have an unusually high number of blogs with degrees ranging from 10 to 500. Across the four data sources  , the best results are obtained from dbSNP  , where the highest recall is 90%. , HEB  , Walmart  , " mall "   , " college "   , and " university " . Most agreements thus contain explicit statements with this regard. The OpenStreetMap project has successfully applied the Wiki approach to geo data. In the experiments we use one graph instance for each targeted application area  , i.e. A marketing analyst is examining sales data from a store like WalMart. We determine the effectiveness of our algorithm in relation to semi-supervised text classification algorithm proposed in 5 NB-EM. More important  , when we provided the same training data to the second step of SAND  , it outperforms all other supervised methods by 6% against SVM and 13% against NB  , showing that it is able to better explore the manually provided training data along with its other self-training  , transductive characteristics. Besides  , we also plot the minimum bounding rectangles MBRs of tourist attractions for reference  , where the tourist attractions are collected from the metadata of OpenStreetMap. To achieve its goal as the main source of information about the scientific production of the Brazilian CS community  , BDBComp strongly relies on its self-archiving service. 1 The analysis consisted of gathering classifications from different human annotators and from different IR / text mining methods and semantic resources  , and of quantitative and qualitative analyses of their outputs. Since the data is from many different semantic data sources  , it contains many different ontologies. Our data is aggregated every 60 minutes  , comes from both TIM customers and roaming customers in the six cities  , and covers the time ranging from February to October 2014. This paper has described preliminary results derived from an analysis of a subset of the MESUR reference data set that consists of over 200 million article-level usage events. , Walmart. The datasets are available from the Stanford Large Network Dataset Collection SNAP  , http: //snap.stanford.edu. The MESUR project will develop metrics using various algorithms drawn from graph theory  , semantic network theory  , and statistics  , along with theoretical techniques developed internal to the project and cross-validated with existing metrics such as the ISI IF  , the Usage Impact Factor 3  , and the Y-Factor 1. This paper addresses these questions by an empirical analysis that uses a part of a standard blog corpus: the corpus offered by Blogpulse for the Weblogging Ecosystem workshop 2006. Similarly to such tasks  , our dataset is composed of a large set of triples coming from LOD datasets  , while our queries consist of entities extracted from news articles and the gold standard is manually created by experts. Furthermore  , according to global OpenStreetMap statistics 1   , Italy and UK are ranked 7th and 10th for number of created spatial objects  , and 4th and 5th for density of created spatial objects per square kilometer. We used the corpus offered by Blogpulse for the Weblogging Ecosystem workshop 2006 2 to refer to a standardized set of texts. A large value of F1 measure indicates a better clustering. OpenStreetMap OSM. Performance results for retrieving points-of-interest in different areas are summarized in Table 3. We validate TermPicker's recommendation quality by performing one evaluation on the DyLDO 21 9 dataset and a second evaluation on the Billion Triple Challenge BTC 2014 dataset 22 10 crawl no. More details and further experimental results are available at http://swa.cefriel.it/geo/eswc2016.html. article metadata  , and a triple database 4 to store and query semantic relationships among items. Therefore  , there exists a strong need for mechanisms for archiving  , preserving  , indexing  , and disseminating the wealth of scientific knowledge produced by the Brazilian CS community. RFID technology has gained significant momentum in the past few years  , with several high-profile adoptions e.g. Point annotations  , for example  , are originally stored as comma separated property-values assignments in a BLOB column within the database. 4. which is a global quantity but measured locally.