In LETOR  , there are a total of 16 ,140 query-document pairs with relevance judgments  , and 25 extracted features. Feature examples include TF  , IDF  , LMIR and BM25 considering  , result title  , abstract  , body  , url and pagerank values. In contrast  , during the second quarter in 2014  , the second user is interested in " center  , partner  , WalMart  , game  , player  , Oklahoma " that are about business   , politics and some sports. The Ohsumed data set is available from the LETOR website 1 . We preprocessed the OAIster collection to produce the bag-of-words representation as follows: Starting with the 668 repositories in the 9/2/2006 harvest  , we excluded 163 primarily non-English repositories  , and 117 small repositories containing fewer than 500 records  , leaving 388 repositories. Actually  , the results of Ranking SVM are already provided in LETOR. Terabytes of raw data are ubiquitously being recorded in commerce  , science and government. Note that individual query strings can generate multiple matches in the database which in turn match multiple of the cases defined in Tables 1 and 2. After queries have been represented by time series  , our goal is to analyze the underlying structure of query logs. In Letor  , the data is represented as feature vectors and their corresponding relevance labels . This process was conducted recursively  , until no further profiles were discovered. The relevancy judgments provided in OHSUMED are scored 0  , 1 or 2 and there are 45 features for each querydocument pair. Finally  , we evaluate the proposed method on LETOR 3.0 benchmark collections1. The Indian middle class represents a huge burgeoning market. As a result  , we create a wider author profile enriched with additional information. In LETOR  , data is partitioned in five subsets. The metadata OAIster collects is in Simple Dublin Core format. Simple K-nearest neighbour KNN with K set to 20 and Regression Tree was used to perform point-wise LETOR. A search with " ICT industry growth in EU " presents 272 results from EconStor; the STW terms used in this search are " ICT industry " and " economic growth " . Table 7shows an example of URL recommendation when the user inputs query " Walmart " . In a medium sized business or in a company big as Walmart  , it's very easy to collect a few gigabytes of data. We use the 5-fold cross validation partitioning from LETOR 10. We conduced 5-fold cross validation experiments  , using the partitions in LETOR. Their similarity   , if needed  , is derived based on the similarity information stored in the tree path. OAIster has built a unique collection of over ten million records. This list of ten further illustrates the variety of content found in metadata repositories. Although distinct in the nature of the information objects they handle  , such systems have common functional and architectural patterns regarding the collection  , storage  , manipulation  , and provision of information objects. In total  , there are 44 features. For our accuracy studies we primarily use the well-known LETOR benchmark 14  , version 3. Standard economic literature users Euclidean distance and location games to model this phenomena; one of our contributions is suggesting that Jacquard distance is a more accurate model to capture the nuances of user tastes. The first evaluation  , based on the LETOR datasets 17  , uses manual relevance assessments as ground-truth labels and synthetic clicks as feedback to BARACO. OAIster's collection has quadrupled in size in three years ---thus scalability and sustainability are a major focus in our evaluations. Letor OHSUMED dataset consists of articles from medical journals . She can further filter out blog posts by date  , leaving only the most recent ones in the result set. and provide similar products and services e.g. Then they talk more about college football and feminism and equality with words like " TXST  , star  , game  , campus  , feminism  , equality and etc. " TD2004 have more relevant documents per topic than other LETOR collections  , relevant documents remain relatively sparse. The process for data cross-linking is based and initiated from the metadata that are used to describe the authors and publications in EconStor. , OCLC-OAIster  , 1 BASE  , 2 DAREnet-NARCIS 3   , and lately experimental data  , collected from OAI-PMH data sources; or in projects such as SAPIR 4   , where an advanced system was built to automatically extract indexing features from images and videos collected from web sources. According to this methodology  , documents in the complete collection are first ranked by their BM25 scores for each query and the top-k documents are then selected for feature extraction. Nevertheless  , in a setup similar to LETOR setup  , as in our experiments  , we show that substantially less documents than the ones used in LETOR can lead to similar performance of the trained ranking functions.