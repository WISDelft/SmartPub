In addition to applications in retail and distribution  , RFID technology holds the promise to simplify aircraft maintenance  , baggage handling  , laboratory procedures  , and other tasks. Hence  , we only compare the proposal algorithm with Ranking-SVM  , but not Rank-Boost. P -perfect user model setting  , I -informational  , N -navigational LETOR eval- uation. , Walmart due to their low cost. For instance  , users prefer to go to a furniture store to buy furniture rather than to a general purpose store such as Walmart. The selected EconStor article and its related blog posts show a meaningful relationship. How to optimize towards diversity under the context LETOR is yet another problem to be studied in future. in two different ways. This is because the LETOR data set offers results of linear RankSVM. People with different mobility patterns significantly differ in the topics they talk about and terms they use  , indicating a fruitful area of further study. In the first experiment  , we used the Letor benchmark datasets 18: OHSUMED  , TD2003  , and TD2004. , HEB  , Walmart  , " mall "   , " college "   , and " university " . , a list of {word-id  , record-id  , count} triples. RFID technology has gained significant momentum in the past few years  , with several high-profile adoptions e.g. In our subject metadata enrichment experiments  , we used three of the fifteen Dublin Core elements: Title  , Subject and Description. The user narrows down the search to " software industry " 5 which reduces the results to 246. 2. It is likely that monitoring all items for sale at Walmart  , say  , is not of interest. Harvested metadata that has no corresponding digital resource is not indexed in OAIster. When the LETOR collection was built  , the fact that documents with low BM25 score were selected only if they were relevant resulted in BM25 being negatively correlated with relevance in the LETOR collection. The empirical results indicate that even with sparse models  , the ranking performance is still comparable to that of the standard gradient descent ranking algorithm. The tool that transforms OAIster metadata from Simple Dublin Core to our native DLXS Bibliographic Class was modified so that it could ingest the file from the first step  , and output a transformed metadata record. The first evaluation is based on the LETOR datasets 17  , which include manual relevance assessments. Consumers making plane and hotel reservations directly ? Figure 4shows the results on Letor OHSUMED dataset in terms of MAP and NDCG  , averaged over five trials. , Walmart. More surprisingly  , however  , our technique can discover interesting relationships even among non-event driven queries whose frequencies do not change greatly over the long term. One example here is that of walmart  , whose frequency function and highest correlated queries are shown in Figure 2. For each EconStor author  , we harvest several other repositories for correlations with other authors  , publications or other relevant information about the initial author. Following LETOR convention  , each dataset is divided into 5 folds with a 3:1:1 ratio for training  , validation  , and test set. In Section 5 we describe experiments with the wellknown public ranking data set LETOR  , from Microsoft. As a result a list of all publications  , co-authors and co-author's publications from our repository will be created and returned to the user of our prototype. For all the SVM models in the experiment  , we employed Linear SVM. Both hedge and LETOR-like document selection methodology   , by design  , select as many relevant documents as possible . Many modem manufacturers and retailers - Walmart is a particularly well known example have found extending the companies boundaries in just this way are central to the 'whole concept of Just in Time and process reengineering. Example 2. definitely  , possibly  , or not relevant. The results of the state-ofthe-art algorithms are provided in the LETOR 3.0. For example  , consider the hierarchical categories of merchandise in Walmart. Our research is based on the EconStor 2 repository  , the leading German Open Access repository for economics which is maintained by ZBW. OAIster's reach often goes beyond that of major web search engines. We would like to improve the search and discovery experience on OAIster by allowing users to restrict search results by subject. It is meaningful to compute the similarity between every two cameras  , but not so meaningful to compute that for each camera and each TV  , as an overall similarity between cameras and TVs should be sufficient. Other applications demand tags with enhanced capabilities. We tried treating 'partially relevant' as 'irrelevant'  , it did not work well for SVM map . In Ranking SVM plus relation  , we make use of both content information and relation information. This is because the LETOR data set offers results of Linear Ranking SVM. Therefore  , in the case where hundreds of raw features are employed  , ranking functions may need more than 1% of the complete collection to achieve optimal performance. We also find this to be true for queries in many other areas; for example  , newspapers  , airlines  , and banks among others also tend to have high correlation among themselves. Because of this  , we have records in our system from original repositories and from aggregator providers collecting original repositories. to the available blog post elements  , we conducted automatic indexing of posts based on the STW thesaurus 3 . The error bars are standard errors of the means. Given an aggregate ranking π  , and relevance levels L  , NDCG is defined as: Since we combine the text from the three elements  , this type of misuse does not affect our subject metadata enrichment. For meta search aggregation problem we use the LETOR 14  benchmark datasets. The training features are the ones used in LETOR benchmark 2 and are described in 2. Thus  , the results reported here refer to non-normalized data. Table 1shows the statistics of the datasets included in the LETOR 3.0 benchmark. Some of the top-ranked posts discuss the relationship of human capital and ICT-related developments. The unique feature of OAIster is that it provides access to metadata pointing to actual digital resources. We have shown very competitive results relative to the LETOR-provided baseline models. LETOR: For comparison purposes  , a LETOR-like document selection methodology is also employed. Further comparisons of these three methods are discussed in 14. Such tags typically operate on the UHF band and are popular in retail and distribution environments e.g. The SVMRank 5 algorithm was used in this task and five-folds cross validation was done. The user selects an article from the result set and its thesaurus-related metadata are retrieved to further support her refine the results Fig. Combining each time different subsets to make the training  , the validation and the test set  , the LETOR authors create 5 different arrangements for five-fold cross validation. Features in Letor OHSUMED dataset consists of 'low-level' features and 'high-level' features. The CORE system provides this functionality and is optimized for regular metadata harvesting and full-text downloading of large amounts of content. Figure 1 The least common denominator approach to metadata is insufficient to serve these multiple contexts  , and can be an inhibitor to meaningful partnerships. For all the SVM models in the experiment  , we employ the linear SVM. For these datasets  , there are 64 features extracted for each query-document pair and a binary relevance judgment for each pair is provided. The simplest RFID tag stores only a 96-bit identifier called the EPC. To avoid the aforementioned implication  , these extra documents with low BM25 scores were dropped in the latest LETOR release 13. As with our first batch of results presented for Ro- bust04  , we again assume the user provides correct feedback. The input to the topic model is the so-called " bag-of-words representation " of a collection  , in which every metadata record is represented by a sparse vector of word counts  , i.e. There are 16 ,140 query-document pairs with relevance labels. The user's interests are almost stable and mainly focus on the design of apps. All presented NDCG  , Precision and MAP results are averaged across the test queries and were obtained using the evaluation script available on the LETOR website. We have described an experimental method in which learnt uncertainty information can be used to guide design choices to avoid overfitting  , and have run a series of experiments on the benchmark LETOR OHSUMED data set for both types of model. 1  , " EconStor Results " . In LETOR 3.0 package  , each dataset is partitioned into five for five-fold cross validation and each fold includes training   , testing and validation sets. If yes  , which one of these methods is better for this purpose ? " We compare our new proposals against several competitive systems  , including structured max-margin learners and RANKBOOST 6. Since a lot of features of LETOR we cannot get  , we droped those columns and then trained the ranking model. State documents from Illinois  , Alaska  , Arizona  , Montana  , etc. We justify why  , for typical ranking problems  , this approximation is adequate. Each of the remaining queries was then searched against the CIC metadata aggregation SQL database to determine whether the query resulted in any matches of the types described in Tables 1 and 2 above. In this paper we use the topic model for subject metadata enrichment of the OAIster collection. We compare the NDCG-Annealing algorithm with linear ranking function described in section 3 with baselines provided in the LETOR 3.0 datasets. The ten largest repositories by size in MB from our 9/2/2006 OAIster harvest are listed in Table 1. We also analyze the results of our approach on a different dataset; OHSUMED 5 which is also available in Letor 16. Renown examples of such systems can be found in the institutional repository area  , where research communities are interested in processing publications e.g. A marketing analyst is examining sales data from a store like WalMart. MAP is then computed by averaging AP over all queries. The OAIster system 16 is another example of a large-scale aggregation system. The fact that CORE caches the actual full-text content in order to process the documents and to discover additional metadata distinguishes this approach from a number of other Open Access federated search systems  , such as BASE or OAISTER  , that rely only on the metadata accessible through OAI-PMH. IW3C2 reserves the right to provide a hyperlink to the author's site if the Material is used in electronic media. Given this  , the set of publications where a is author is represented as Kubler  , Felix "   , in EconStor. However  , the default crawler may end up spidering many pages of the catalog at the cost of possibly missing pages in categories of interest to subscribers  , such as investor relations or press release pages. The idea is similar to that of sitemap based relevance propagation 24. 16  , here we investigate whether a simple unweighted average is sufficient to give improve- ments. We also tried different strategies to normalize our feature vectors  , including L2-norm  , z-score and the LETOR normalization procedure 17  , with no improvements. The datasets provided in the LETOR There are 106 queries in the OSHUMED dataset. At lower levels of mobility  , we see significant words like " railway station " and " bus "   , as well as discussion of " home "   , " work "   , " church "   , grocery stores e.g. Data sets. This effectively brings blog posts at the same vocabulary level as publications from EconStor. We also see from Figure 4 that our NDCG-Annealing algorithm outperforms all the other baseline algorithms on this dataset. OAIster can be found online at http://www.oaister.org/  , with over a million records available from over 140 institutions. There are 106 queries in the collection split into five folds. This realization has led various retail giants such as WalMart 4 to enter Indian market. Multiple LETOR methods have been tried  , which are different in many ways and we expect them to be complimentary during the final fusion. As a result  , the NDCG-Annealing algorithm is more stable and pronounced compared to the baselines in LETOR 3.0 dataset. The results of RankSVM  , RankBoost  , AdaRank and FRank are reported in the Letor data set. We present a high-level * This work was partly supported by the National Science Foundation with grants IIS-9984296 and IIS-0081860. At the time of writing  , the CORE harvesting system has been tested on 142 Open Access repositories from the UK. There are 106 queries in the collection. Table 1summarizes the properties of these data sets. , Walmart  , McDonald's . This enriched metadata could then be distributed to meet the needs of access services  , preservation repositories  , and external aggregation services such as OAIster. We employ five different document selection methodologies that are well studied in the context of evaluation  , along with the method used in LETOR for comparison purposes. These data sets were chosen because they are publicly available  , include several baseline results  , and provide evaluation tools to ensure accurate comparison between methods. 17 reports findings on a number of metadata harvesting experiments. While the frequency function of walmart may not appear unusual  , showing only that it is more popular during the day than at night  , it is in fact distinctive enough such that it correlates very well with other large retailers. EconStor content has also been published in the LOD. We conducted 5-fold cross validation experiments  , following the guideline of Letor. 1. Since we are only training on a single topic  , resulting accuracy is far lower than what typically published LETOR results. a vector  , to represent the query " Walmart " which is showed in Figure 1as follows: Note that it is commonly believed that Rank-Boost performs equally well as Ranking SVM. For the implementation we use EconStor and an RDF dump file of Econstor. Results for the analysis of the 2 ,404 OAIster query strings are given in Tables 4 and 5 below. As the histogram shows  , relevant documents per topic are quite sparse  , restricting the number of feedback iterations possible with stable evaluation. Without considering the context  , Baseline2 recommends the homepage of Sears as the first choice. The experimental results provided in the LETOR collection also confirm this. Our goal is set to design a system as simple as possible  , without using any external processing engine or resources  , other than the standard Indri toolkit and a third party LETOR toolkit. We started the extraction process with one highly connected FriendFeed user and crawled the profiles of all his subscribers and subscriptions . This ensures that users can access the resource itself. For example  , <o1  , Walmart  , c1>  , <c1  , Redmond  , s1>  , <s1  , WA  , t1>  , <t1  , USA> describes an organization entity where o1  , c1  , etc.  LETOR: For comparison purposes  , a LETOR-like document selection methodology is also employed. are identifiers typically generated for maintaining referential links. The decision of whether or not to harvest from aggregator repositories is made more complex because these aggregators contain records that are not currently available through OAI channels  , and they do not always contain all the records of a particular original repository. This is a highly counterintuitive outcome. Although the vlHMM and Baseline2 have comparable precision and recall in Test0  , the vlHMM outperforms the baseline substantially in Test1  , where the context information is available. We compare our proposed NDCG-Annealing algorithm with those baselines provided in LETOR 3.0. As shown in Figure 2  , the documents selected by the two methods also exhibit very high similarity to each other. For our experiments we work with three public data sets: TD2004 and MQ2007 from LETOR data sets 24 and the recently published MSLR-WEB10K data set from Microsoft Research 1. As an example  , a search performed in OAIster for " double-well Duffing oscillator " retrieves two records  , exactly the same  , but one was harvested from the arXiv.org Eprint Archive repository an original repository and one harvested from the CiteBase repository an aggregator. As a result  , an author's profile is enriched with additional information found in the cluster. Figure 5and Figure 6show the results on the Letor TD2003 and TD2004 datasets. This functionality is only possible if we have reliable  , consistent and appropriate subject metadata for each of the ten million records in OAIster. The optimal parameters for the final GBRT model are picked using cross validation for each data set. The KC4 dataset has been taken from the NASA data metrics program http://mdp.ivv.nasa.gov/. With the advent of the Web and mobile devices  , we are observing a boom in local search: that is  , searching local businesses under geographical constraints. Creating individual preprocessing rules for each repository in the collection is not a scalable solution for OAIster  , or any other large metadata collection. The rankers are compared using the metric rrMetric 3. The results on seven datasets in LETOR 3.0 show that the NDCG-Annealing algorithm can outperform the baselines and it is more stable. Hedge finds many relevant documents " common " to various retrieval systems   , thus documents likely to contain many of the query words. Each data set is partitioned on queries to perform 5 fold cross-validation. While our topic modeling approach is statistical  , and can handle some degree of noise  , we found that improved preprocessing of metadata records produced better results.  LETOR: Using only statistical features associated with matched terms features L1−10 and H1−3 in Tab.