This text was converted to upper-case and cleaned using a series of regular expressions. Since Quora does not show when a question is posted  , we estimate the posting time by the timestamp of its earliest answer. RQ1: 14% of repositories are using pull requests on Github. Our dataset consists of a sample of Stack Overflow  , a Q&A Forum for programmers. Three topics get more than 200% improvement  , such as topic 946 +900%  , and only 6 topics get a little drop on performance. We represented interest models as a distribution across categories in the Open Directory Project ODP  , dmoz.org topical hierarchy as in 45. Despite the large number of repositories hosted at GitHub  , developers work only on a consistently smaller fraction of them. The ultimate answer to this question depends on the exact data and queries used  , though based on our experimental analysis above  , we believe that an adaptive materialization strategy provides the best trade-off for running provenanceenabled queries over Web Data in general. We used 4-fold crossvalidation by department. In this paper  , all the experiments use only the 800 queries  , except in the ensemble classifiers  , where we use the 111 sample queries to tune the weight of each single classifier. Because of this convenience and extensibility  , we have also recently launched Coagmento 2.0 on GitHub as an open source tool 4 . This is because for most classes T in the API framework  , GitHub contains many more usage samples than can be extracted from web pages. The entry provided by UMLS for the phrase " mad cow disease " is " bovine spongiform encephalopathy  , bse  , bovine spongiform encephalitis "   , excluding the variants generated by varying the form or order of the words. While pull-based development e.g. The stream-based approach is also applicable to the full data crawls of D Datahub , This is due to poor feature selection  , which selects biased page attributes over the pairwise autocorrelation features. In order to publish the OpenStreetMap data  , we performed some preprocessing of the data structures. We iterated through the open-ended responses using grounded theory methods 12  , to categorize them and identify themes. Next  , we experiment with the extent that the algorithms can produce quality recommendations for groups  , using the MoviePilot data. The experiment8 foreseen require care in the design and population of the test databases. 1 Crawled during February/March 2009  , it comprises about 1.14 billion RDF statements. To the best of our knowledge  , there exists no previous benchmark which can automatically emulate the process of user Web surfing in a way fair to Web browsers. Our combination method is also highly effective for improving an n-way classifier. This searching was by no means complete and no relevance judgements from this phase were retained. A portion of a sample LocusLink entry is shown in The relevance judgements were obtained from the LocusLink database 11. Actually  , we chose the term keyquery in dependence on these two concepts. Recently  , researchers from the same team proposed a new dataset within the context of the SEMEVAL task 11 28  , in which the goal is to provide an evaluation framework for the objective comparison of word sense disambiguation and induction algorithms in SRC for ambiguous queries. In this paper we evaluate the retrieval performance of four methods to discover missing web pages. Stack Overflow http://stackoverflow.com is a website that allows users to post questions and answers concerning problems in computer programming. '16  , May 14 -22  , 2016  , Austin  , TXFigure 1: Monthly growth of pull request usage on GitHub. An overview of the pull request process can be seen in Figure 1. Table 2shows k-means clustering results on the WebKB 4 Universities data set. Two of the top-most topics in the September 2010 DSN include words related to AlgoViz bibliography entries i.e. Most participants were from North America or Europe. We have considered in the same class also other wikis  , such as WackoWiki  , TikiWiki  , and OddMuse  , which support functional templating without parameter passing i.e. MEDoc models judge and label such sequence. This hierarchy is pre-generated using the open directory project dmoz http://dmoz.org to classify various web pages. ACSys made that data available in two ways. Stack Overflow delineates an elaborate procedure to delete a question. These values are rather low. Awareness. The largest qid from our crawled questions is 761030  , leading us to estimate that Quora had roughly 760K questions at the time of our crawl  , and our crawl covered roughly 58% of all questions. To ensure critical mass  , several programmers were explicitly asked to contribute in the early stages of Stack Overflow. We describe the behavioral  , topical  , temporal  , and other features in more detail later in the paper. Due to the lack of In addition to topics 401-450  , we have executed a number of manual queries on the software. Answers on Stack Overflow often become a substitute for official product documentation when the official documentation is sparse or not yet existent 5 . These  , for instance  , are an indicator for available source code. Construct: Are we asking the right questions ? Topic: We utilize the Open Directory Project ODP  , dmoz.org  , a human-generated hierarchical taxonomy of Websites  , as our topical ontology. The WebKB dataset consists of 8275 web-pages crawled from university web sites. The data collection we use is the Billion Triple Challenge 2009 dataset. We have evaluated the proposed method on the BLOG06 collection. The Blog06 dataset also contained a lot of non-english blogs. Knowing the groups  , their interests  , and size gives us leverage on better serving the target audience. WebKB 3 : This dataset contains 4199 university webpages . However  , it was more convenient for us to download the most up-todate original OpenStreetMap data about Bremen  , available as Shapefiles 10 . Note that existing crawlers have no dedicated means of locating websites on which their targets are published. Section 3.2.1  , we considered all the Stack Overflow users and their questions and answers. We discuss hierarchical agglomerative clustering HAC results in section 4.6. Finally we did filtering of offensive content. For example  , the gene olfactory receptor  , family 5  , subfamily V  , member 1 is a member of subfamily V of the olfactory receptor family. The process used by Github to select projects is not public  , but we believe it is orthogonal to our concerns  , and likely based on popularity and recency. Previous work 8  , 9  , 24 studied effectively finding previously answered questions that are relevant to a new question asked by a user. Opinion modules require opinion lexicons  , which are extracted from training data. , biblio. the various categories. The approaches from this line of research that are closest to CREAM is the SHOE Knowledge Annotator 10 and the WebKB annotation tool. Selecting word pairs to evaluate: To create a balanced dataset of both related words and unrelated words  , we applied the following procedure: Let W be a set of all words in the New York Times news articles. We constructed 20 training topics from BlogPulse http://www.blogpulse.com/ and Technorati search http://www.technorati.com/ archives and manually evaluated the search results of the training topics to generate the training data set of 700 blogs. syntactic mistakes  , improper references  , and all the problems sketched in the scenario section. This may be true for a certain point-feature representation of the cities but is not correct for all points inside the city boundaries. For each query  , the returned top 1 ,000 documents are re-ranked according to the score consisting of the topic relevance and the opinion sentiment strength. Transparency. Through Github facilities. The second best contributor is the AcroMed acronym database  , which causes an improvement of 4.8% over the Heuristics only run. This result is higher than the overall we calculated for Github; we attribute this to the fact that the dataset generation process employs heuristics to detect merges in addition to those happening with Github facilities. The results using the WS-353 and Mturk dataset can be seen in Table 3. The full list of public events that have happened on GitHub is available on the GitHub Archive website 8 . To assess word relatedness  , we use the WS-353 benchmark dataset  , available online 14  , which contains 353 word pairs. Stack Overflow is centered around nine design decisions 7 : Voting is used as a mechanism to distinguish good answers from bad ones. Since our system only dealt with english language opinions it made no sense to keep the non english ones. The Billion Triple Challenge 1 is a collection of crawled Linked Data that is publicly available and that is often used in Big Data research. Dmoz: A cut was taken across the Dmoz http://dmoz.org/ topic tree yielding 482 topics covering most areas of Web content. So instead of IDs  , we rely on other methods to identify users whether registered or unregistered. 3.3. 2  is that sentences extracted by our linking approach always reflect the latest content available on Stack Overflow. To define user interests in a manageable way for all models  , we classified the Web pages sourced from each context into the topical hierarchy from a popular Web directory  , the Open Directory Project ODP dmoz.org. It is surprising that adding gene information from euGenes and LocusLink deteriorates the mean average precision comparing rows Heuristics&AcroMed and All of the above in Table  3   , although the additional data increases the recall from 5 ,284 to 5 ,315 relevant documents. To represent two different dimensions of the social connections in GitHub  , we used a measure for social distance and another for prior interaction. Quora is unique because it integrates an effective social network shown above into a tradition Q&A site. For SEMEVAL  , the best performances are provided by STC in terms of ARI and LINGO in terms of F N 1 . We believe that a benchmark like WPBench is useful to evaluate the performance of Web browsers for modern Web 2.0 applications. The rankings are based on the rank of the similarity of the pair of words out of the 353 pairs in the WS-353 dataset. We then use this model to derive a framework for group recommendation Section 3.2 that  , unlike previous work—which focuses on merging recommendations computed for individual users—uses the principles of information matching in order to compute the probabilities of items' relevance to a group  , while taking the entirety of the group into consideration. Let us notice that this is the only dataset for which experiments with query logs can be performed and easily reproduced. To do this  , we compare the classification performance obtained by a simple classifier that uses attributes calculated from the seed lexicon  , with the performance obtained by a classifier with attributes derived from both the seed lexicon and the generated words. The New York Times annotated corpus was a relatively new development and had not been extensively adopted for clustering experi- ments. Web directories such as the Open Directory Project ODP  , dmoz.org provide user-compiled taxonomies of Web sites. Even beyond the cluster/cloud threshold  , however  , we are able to continue to get improved turnaround times for several algorithms using the Hybrid approach. It is not uncommon to find prolific developers contributing code to 5-10 GitHub projects in the same week. As shown in 16  , 32  , 37  , finding a small sample set of URIs that represent the Internet is not trivial. After excluding splogs from the BlogPulse data  , we 14 for the BlogPulse dataset  , we replicate the result that the cumulative in-degree and out-degree distributions show smoother curves  , as shown in Figure 3. They may be static for example  , always show the first 50 words of the document   , or the content of its description metadata  , or a description taken from a directory site such as dmoz.org or query-biased 20. TS task's queries are one or two sentences long  , which show research demanding of companies or experts. Second  , users in Stack Overflow are fully independent and no social connections exist between users. Stack Overflow provides a procedure to undelete a deleted question. In conjunction with the widespread use of smartphones and GPS enabled devices  , this has resulted in a large number of RDF datasets containing geospatial information  , which is of high importance in several application scenarios  , such as navigation  , tourism  , and location-based social media. We followed the advice from a Quora data scientist 3 and start our question crawls using 120 randomly selected questions roughly evenly distributed over 19 of the most popular question topics. Using large language model with and word co-occurrences  , we achieve a performance comparable to the systems in SemEval 2013  , task 13 23. For the ease of presentation   , we highlight the clusters by different colors such that the size and shape of the clusters are clearly illustrated in the figures. This shows that author-deleted questions are inferior in quality than moderator-deleted questions and require more work to improve their content. The pull-based development model  , in conjunction with the social media functions offered by GitHub  , makes contributions and their authors more prominent than in other contribution models. For each context trail extracted from the logs  , we created a user interest model for   , the interaction context   , and the other contextual variants collection  , historic  , task  , and social. The properties link were interpreted as rdf:type of the topics they belong to. 60% of Stack Overflow users did not post any questions or answers  , while less than 1% of active users post more than 1000 questions or answers. dmoz.org. We use the centroid-based approach 23  since it is a popular scheme for compact clusters which are similar to the clusters we see in the AlgoViz DSN. Two users were connected only if they viewed at least 10 similar pages within a month. A 10% sample was taken which maintained the same distribution of intrusions and normal connections as the original data this sample is available as kddcup .data. They concluded that linkage in WT2g was inadequate for web experiments. Figure 1shows DSNs based on AlgoViz log data for the months of September and October 2010 with a connection threshold of 10. We define insight sentences as those sentences on Stack Overflow that are related to a particular API type and that provide insight not contained in the API documentation of the type. analyze questions on Stack Overflow to understand the quality of a code example 20. In addition  , 99% of questions end up with less than 10 answers  , and 20% of all Quora questions managed to collect ≥4 answers. Singhal and Kaszkiel 4 looked at average in-and out-links  , within and across hosts  , between the smaller WT2g corpus and their own large crawl. We begin by examining the follower and followee statistics of Quora users. As mentioned in Section 2  , for the purposes of the opinion finding task  , the document retrieval unit in the collection is a single blog post plus all of its associated comments as identified by a permalink . Section 2 provides a short description of the newly created Blog06 test collection. So  , when we merge the group profiles the items considered in training were the items rated by at-least one member who has a group identifier. Though our method of link-content matrix factorization perform slightly better than other methods  , our method of linkcontent supervised matrix factorization outperform significantly. For technology survey  , we proposed a chemical terminology expansion algorithm with the professional chemical domain information from two chemical websites  , ChemID plus and PubChem. We are aware of the implicit bias of this selection but for simplicity it shall be sufficient. The last step in the data pre-processing of CodeTube consists in indexing both the extracted video fragments and the Stack Overflow discussions  , using Lucene 9   , where each video fragment is considered as a document. The popularity of GitHub among developers living in the USA is really prominent  , as 3 users out of 10 are based there. We tried to follow crawler-etiquette defined in Quora's robots.txt. First  , do user votes have a large impact on the ranking of answers in Quora ? Second  , do super users get more votes  , and do these votes mainly come from their followers ? Previous qualitative research on GitHub by Dabbish et al. We have chosen the AS3AP benchmark for our performance tests due to its completeness in comparing relational systems with vastly different architectures and capabilities over a variety of workloads. Both problems above could be solved by our proposed thematic lexicon. We make the new dataset publicly available for further research in the field. Stack Overflow is driven by the goal to be an exhaustive knowledge base on programming related topics and hence  , the community would like to ensure minimal possible noise on the website. This has been used extensively in previous work on personalization to model search interests at a level beyond queries and documents 524 . We find that both algorithms are powerful for improving retrieval performance in biomedical domain. Community question and answer sites provide a unique and invaluable service to its users. To address this challenge  , we classified the Web pages sourced from each context into the topical hierarchy from a popular Web directory  , the Open Directory Project ODP dmoz.org. In both cases  , for any given time span  , if an entry E in AlgoViz received a certain number of views within a cluster whose topics were highly related to that of E  , then E would be weighted more compared to other entries of similar type. Stack Overflow is a free  , open no registration required website to all users on the Internet and hence  , it is a necessity to maintain quality of content on the website 4. First  , the large majority 95% of users have followed at least 1 topic. It provides detailed information about the function and position of genes. editors  , actors and CEOs. Upweighting of positive examples: no w = 1. Code of the API functions and data from our experiments can be found on github. The tiny relation is a one column  , one tuple relation used to measure overhead. Previous work has revealed that most GitHub repositories are inactive and have a single user 25  , 31 . Understanding the interactions on Q&A websites  , such as Stack Overflow  , will shed light on the information needs of programmers outside closed project contexts and will enable recommendations on how individuals  , companies and tools can leverage knowledge on Q&A websites. Therefore WPBench produces a fairer benchmark for different Web browsers. For each topic  , we download 10 ,000 pages using the best-first algorithm. Once the best feature set is established  , we are going to evaluate our contextualization on the SemEval 2010 20 and SemEval 2013 23 datasets. From the PSLNL documents  , the system extracted 6500 data items on which our evaluation is carried out. It is helpful to the work of conducting the GeneRIF in LocusLink database. which is a global quantity but measured locally. Besides  , we also plot the minimum bounding rectangles MBRs of tourist attractions for reference  , where the tourist attractions are collected from the metadata of OpenStreetMap. In contrast with the previous standard benchmark  , WS-353  , our new dataset has been constructed by a computer algorithm also presented below  , which eliminates subjective selection of words. The WebKB hypertext dataset available at http://www.cs.cmu.edu/afs/cs/project/theo-11/www/-wwkb/ is employed in the experiment of text categorization. We hope that the 10GB dataset next year will contain a higher percentage of Functional links. We describe details below. In contrast  , our work examines a fundamentally different setting where communities are actively competing with each other for users and the unique content they bring. These browsers cover the most wellknown layout engines  , such as Trident and Gecko  , as well as several widely used JavaScript engines. The paper is structured as follows: We motivate the need for a simple RDB-to-RDF mapping solution in Section 2 by comparing indicators for the growth of the Semantic Web with those for the Web. Hence  , it is important to perform a longitudinal study about deleted questions on Stack Overflow. A first fact is the different support between creational and functional templates: about a half of the clones adopt a creational approach  , while less than a fifth adopt a functional one. For Stack Overflow we separately index each question and answer for each discussion. Our proposed pairwise similarity features are list in Table 2  , and categorized into three types: query-based  , URLbased and session-based similarities. In contrast  , Stack Overflow anonymizes all voters and only displays the accumulated number of votes  , which can be negative Sorted Topic Bucket By # of Followers Thus in our analysis of Quora  , we only refer to upvotes and disregard downvotes . A key observation is that given the broad and growing number of topics in Quora  , identifying the most interesting and useful content  , i.e. However  , there are 9% questions with degree less than 5. Prototypical examples of PSLNL document collection include sets of conference information and seminar announcements. Formal releases of these two broswers are expected to fix these problems. Each abstract sentence was classified to gauge its likelihood as a source of a GeneRIF. The Wookieepedia collection provides two distinct quality taxonomies. The GHTorrent dataset covers a broad range of development activities on Github  , including pull requests and issues. A total of 45 ,995 blogs were identified by their homepage URL. Regardless of the topic in question these sites would be ranked highest due to the number of inLinks associated with them. dimacsAp5w5: Representation: Paragraphs  , selected using Locuslink information. Our empirical results show that this strategy performs best when taking into account the costs of materialization  , both on Web Data Commons and on Billion Triple Challenge data. We picked all projects that we could retrieve given the Github API  , and selected from these only based on constraints of building and testing. Second  , does the presence of popular users correlate with high quality questions or answers ? A large value of F1 measure indicates a better clustering. We do present results of LOADED on the full training and testing data set. These interactions are emulated during benchmarking browsers by instrumented JavaScript which is independent of Web browsers. Furthermore  , the extended ontology includes the mappings resulted by the schema matching.  industry sector 2 The task is to classify webpages according to a hierarchy of industrial sectors 4 ,582 instances. They find that programming languages are a mixture of concepts and questions on Stack Overflow are concerned with the code example rather than the application domain. This resulted in a list of 312 endpoints. Exact inference also reduces error as the STACKED- GIBBS approach performs significantly worse p < 0.05 than the STACKED model in every dataset except WebKB. Considering all the blogs in the BlogPulse data  , both in-degree and out-degree distributions have an unusually high number of blogs with degrees ranging from 10 to 500. Approaches such as point-based measures or cluster centroids are often used to assign newly arriving points to an existing cluster. by better interlinking the data with other Linked Data datasets and providing a proper ontology for querying. Hilliness. Answers and Quora. We generate a dataset of URIs by randomly sampling URIs from dmoz.org and assume these pages to be missing. As a result  , we obtained 192 million pointsof-interest   , which are annotated with roughly 800 million property-value combinations. They represent two very different kinds of RDF data. In particular  , and as will be discussed in detail in Section 3  , we use keyword extraction in a subroutine to efficiently find a small subset of diverse keyqueries. In this paper  , we use the data sets from the KDDCUP 2005 competition which is available on the Web 1 . Babelfy has been evaluated using six datasets: three from earlier SemEval tasks 33  , 29  , 28  , one from a Senseval task 38 and two already used for evaluating AIDA 17  , 16. For Reuter-21578  , we used a subset consisting of 10 ,346 documents and 92 categories. The results show our advanced Skipgram model is promising and superior. Many Quora users seem to frequently post replies prompted by others rather than by their personal situation ; hence the lower impact of the temporal component. Table 12presents additional examples of pairs belonging to these relations and the ranking of human judgments  , ESA and TSA algorithms for the WS-353 dataset. Despite a small number of registered users  , AlgoViz project leaders are interested in understanding the trends of its overall user base. These results indicate that taking into account Stack Overflow meta data as well as part-of-speech tags can significantly improve existing unsupervised approaches when applied to Stack Overflow data. The BTC data set has been crawled from the web in a typical web spider fashion and contains about 1.44 billion triples. By positioning good answers at the top of the questions page  , Quora allows users to focus on valuable content. Events include participating in issues  , pull requests  , and commenting on various GitHub artifacts. Although it is a continuous timeline  , we split it into two segments to follow the traffic trends seen in Fall and Spring semesters. The four main categories are used for clustering  , while examples in the remaining categories are used as Urest. We make the following research contributions  We analyze deleted questions on Stack Overflow posted over ≈5 years and conduct a characterization study. Projects were taken from Github 15  , one of the largest public repositories of Java projects. Information about trees and parks is extracted from OpenStreetMap. For our analysis  , we extracted questions asked and answers posted between July 2008 and September 2013. In contrast  , our work performs a similar computational analysis   , but also identifies the platform and motivational factors involved. Following the right topics can introduce users to valuable questions and answers  , but is not the only way to access questions. To analyze the semantic relationships between queries  , we assign each URL to a topic distribution over 385 categories from the second level of " Open Directory Project " ODP  , dmoz.org with a contentbased classifier 18. In contrast  , the RDN models are not able to exploit the attribute information as fully. We noticed that some developers are interested in borrowing emerging technologies e.g. 07 and the participant's papers for details. However  , we observed that in some cases  , software projects are organized into multiple separate repositories on GitHub. The Data Collection Mechanism component is responsible for gathering Q&A data from Stack Overflow. OpenStreetMap OSM. As in the prior studies  , we label the results visited by users across their long-term search histories using category labels from the Open Directory Project ODP  , dmoz.org. Instead  , we used the Open Directory Project ODP  , also referred to as dmoz.org. For WebKB dataset we learnt 10 topics. However  , the mean is a poor statistic to describe the power-law distributions of links on the web; average linkage is dominated by the many pages with few links and gives little insight into the topology. To understand how Quora's social network functions  , a basic question of interest is how users choose their followees. 50 test topics  , each consisting of title phrase  , description sentence  , and narrative paragraph fields  , were constructed using queries from commercial blog search engines e.g. The winner of the KDDCUP 2005 competition found that the best result was achieved by combining the exact matching method and SVM. However  , the database dumps provided by Stack Overflow do not directly contain information about deleted questions. Raw text was extracted from the XML format of the AQU- AINT-2 and Blog06 collections. 2 Stack Overflow has detailed  , explicit guidelines on posting questions and it maintains a firm emphasis on following a question-answer format. Our experiment showed that SugarCube is successful in providing a method for quantifying the propagation of topics  , and also in identifying heavily percolated ones within the test collection. User-Topic Graph: Quora users follow different topics  , and receive updates about questions under topics they follow. Figure 1: Stack Overflow Example meaningful on their own without their surrounding code snippets or the question that prompted a given answer. The correlation of such words  , such as " Mars " and " water " in 1900 should be weighted differently from the correlation they exhibit in 2008  , when NASA images suggested the presence of water on Mars. The data were then processed into connection records using MADAM ID 9 . Allamanis and Sutton perform a topic modeling analysis on Stack Overflow questions to combine topics  , types and code 5. Among the blog document set 100 ,649 feeds 38GB  , 2.8 million permalinks 75GB  , and 325 ,000 homepages 20GB  , only the permalinks were used in our experiment. Figure 1: Number of events detected in the GitHub stream. Quora applies a voting system that leverages crowdsourced efforts to promote good answers. Table 2summarizes the performance of our model on five test sets using three parameter initialization schemas. , which are usually considered as high-quality text data with little noise. In our evaluation experiments  , we used two standard corpora: Reuter-21578 3 and WebKB 4. 'Closed' questions are questions which are deemed unfit for the Stack Overflow format. 2. They might  , however  , rely on subtle social signals that environments like GitHub provide  , without realizing it. BRFS performance matched or exceeded in some cases SS1 and BL. PageRank utilizes the link structure of the Web and measures the quality of a page from the page creator's point of view  , while fRank utilizes content-layout and user click-though information and captures the preference of both page authors and search engine users. In every dataset  , the RDN weights relational features more highly than intrinsic features. Code- Tube also automatically complements the video fragments with relevant Stack Overflow discussions. Our model outperforms all these models  , again without resorting to any feature engineering. If no results were returned by the engine  , no label was assigned. For our static analyses we consider these networks as they appear on the final day of the time window we take into con- sideration. We present a principled method to create additional datasets  , as opposed to the WS-353 benchmark where the word pairs were extracted manually. Our preliminary findings indicate that Stack Overflow is particularly effective at code reviews  , for conceptual questions and for novices. We recruited eight participants from GitHub  , randomly selecting from the 68 ,949 GitHub users who had made at least one contribution in the previous twelve months  , used Java in at least one of their projects  , and had published their email address. Therefore  , social relationships clearly affect Q&A activities  , and serve as a mechanism to lead users to valuable information. This indicates that the bridging classifier works in a different way as the exact matching method and SVM  , and they are complimentary to each other. We recall that a question on Stack Overflow can either be deleted by the author of the question or by a moderator . We chose this collection because it is freely available for download 10 and is the largest forum hosted by Stack Exchange. Two datasets are used in our experiments to measure performance: a sample of 12 ,000 web pages from ODP and a sample of 2 ,000 web pages from the Stanford WebBase collection 9. OpenStreetMap. Since all insight sentences used in this paper were obtained from sets of ten Stack Overflow threads associated with an API type  , we would expect comparable results for any API type with at least ten threads on Stack Overflow. Additionally  , we employed Triplify to publish the 160GB of geo data collected by the OpenStreetMap project. This gap indicates the increased inference variance inherent in approximate inference approaches. More details and further experimental results are available at http://swa.cefriel.it/geo/eswc2016.html. The KDDCUP 2005 winning solution included two kinds of base classifiers and two ensemble classifiers of them. 5 evaluated CORI  , vGlOSS  , and CVV in a testbed based on the 2GB  , 956 server WT2g crawl of the Web. If pattern discovery is effective  , we would expect that most data items would be extracted. We plot two lines for Quora  , a black dashed line for the total number of questions estimated by qid  , and the blue dashed line is the number of questions we crawled from each month. The pages were spidered from four computer science departments and were released as part of the WebKB data 1 . on whether the street is in or near a park. In Quora  , the top 10 includes topics in various areas including technology  , food  , entertainment  , health  , etc. " In addi-tion  , in contrast to the XCRAWL method  , the baseline BN crawler has no built-in capability to identify such target websites effectively. We also used the MoviePilot data  , by disregarding the group memberships. When we failed to identify the location of a user  , we categorize their location as " other " . Researchers can install PHP  , Laravel  , Node.js  , and a SQL framework and download the GitHub repository to get started with their instance of Coagmento. For these reasons  , we used GitHub in our recruiting efforts. The methodology that we adopted sought to align itself to the structure of the CAMRa challenge. For the term " TGFB " in topic 14  , for instance  , the expansion techniques in stage 1 produce 185 candidates including lexical variants. The main assumption of such crawlers is that pages of one relevant website will include links to other websites from the same domain or that directories such as dmoz.org exist that contain links to other target websites. This results in irregular shapes for the cumulative degree distributions  , which represent the proportion of blogs having at least k in-links or out-links. If an acronym included in the expanded query can locate in LocusLink its aliases  , the aliases are included and their weights are equal to the weight of the acronym. Topic labels were taken from the 219 topics from the top two levels of the Open Directory Project ODP  , http://dmoz.org  , and included topics such as " Health/Medicine " and " Recreation/Sports " . Large Linked Datasets. Section 2 provides a short description of the used Blog06 collection. Quora is a question and answer site where users can ask and answer questions and comment on or vote for existing answers. LocusLink is most prominent source of publicly available information on genes. Measures of semantic similarity based on taxonomies are well studied 14 . We believe that we are the first to investigate augmenting natural language software documentation from one source with that from another source. Segments in curly brackets denote whole URLs that match predefined URL patterns   , such as GitHub URLs as denoted by {github}. We plot the evolution on the percentage of intrusions using " averaged shifted histogram ASH " in Figure  1. We begin by constructing DSNs based on AlgoViz log data from Fall 2009 August 1 to December 31 and Spring 2010 January 1 to May 31. Density 20 for a network with edges E and vertices V is defined as: In 2012  , we consolidated the set Bio2RDF open source 5 scripts into a single GitHub repository bio2rdf-scripts 6 . It embeds conceptual graph statements into HTML pages. These low values confirm that sensitivity is rather subjective . LinkedGeoData uses the information collected by the OpenStreetMap project with the aim of providing a rich integrated and interlinked geographic dataset for the Semantic Web. This is because SimFusion+ uses UAM to encode the intra-and inter-relations in a comprehensive way  , thus making the results unbiased. Github can automatically verify whether a pull request can be merged without conflicts to the base repository. The second and third requirements ruled out a uniform 2 % sample. Having targeted only users of GitHub  , this was a surprising result. The AS3AP DB is composed of five relations. The clustering results along with the topics highlighted in the previous section indicate that AlgoViz users have clusters of interests when it comes to using online resources related to algorithm visualizations. This can be attributed to the structure of the WebKB corpus and the quality of the seed documents. 2007URLs. For instance  , http://www.w3.org/People/Berners-Lee/ is then an instance of http://dmoz.org/Computers/ Internet/History/People/Berners-Lee ,_Tim/. In this section  , inspired by KDDCUP 2005  , we give a stringent definition of the QC problem. Merging such a pull request will result in conflicts. The number of positive and negative tweets of these datasets is given in Table 5Table 5: Message-level polarity classification datasets. Table 4shows an example of one generated cluster. Rather than requiring the manual provision of a set of start sites  , XCRAWL re-uses existing information which can for instance be retrieved from public search engines or from manually engineered directories like dmoz.org. Query category is decided based on classification of each possible keyword query into a two-level query type hierarchy. This fact indicates that the text categorization of WWW documents can be more difficult than the categorization of normal documents. The comparison results of TSA on the WS-353 dataset are reported in Table 1. Figure 3depicts the distribution of number of friends per user. We used a version of the LocusLink database containing 128 ,580 entries. Recommendations to Groups. The input to our method is a set of queries; each query is associated with Trels Term RELevance Sets  , which consist of two sets of terms: 1 http://dmoz.org  onTopic: contains terms related to the query that are likely to appear in relevant documents. For the domain of software development   , the website Stack Overflow 4 facilitates the exchange of knowledge between programmers connected via the Internet . There are various reasons why developers are more prolific on GitHub compared to other platforms. First  , we observe that the degree distributions are greatly affected by the existence of splogs. Having calculated PageRank for all the pages in the graph we choose centroid pages as pages with largest PageRank excluding pages which have more than 30% of neighbours with other centroids. On the WebKB dataset  , we obtained a precision of 0.8137  , recall of 0.3081 and an accuracy value of 0.5413. We bring together two existing experimental techniques to launch a thorough study of topic-based properties of the Web: the ability to classify a Web page into predefined topics using a high-speed automatic classifier  , and the ability to draw near-uniform samples from the Web graph using random walks. i word embeddings are initialized using a neural language model 4  , 7  , which is trained on a large unsupervised collection of tweets; ii we use a convolutional neural network to further refine the embeddings on a large distant supervised corpus 1; iii the word embeddings and other parameters of the network obtained at the previous stage are used to initialize the network with the same architecture  , which is then trained on a supervised corpus from Semeval-2015. A survey of current research in the field is given in the overview paper of the 2010 SemEval competition on keyphrase extraction 9. We are surprised to find that the curves from Stack Overflow and Quora are nearly identical. As ODP- 239 is an evolution of AMBIENT and SEMEVAL is the next generation of MORESQUE  , we will only give an overview of the most recent datasets. The classifier has a micro-averaged F1 value of 0.60 and is described more fully in reference 5 . The data extraction experiment proceeded as follows: From the PSLNL documents  , the system extracted 6500 data items on which our evaluation is carried out. 1 The analysis consisted of gathering classifications from different human annotators and from different IR / text mining methods and semantic resources  , and of quantitative and qualitative analyses of their outputs. We perform the first large scale study on poor quality or deleted questions on Stack Overflow. In our solution  , an intermediate taxonomy is used to train classifiers bridging the queries and target categories so that there is no need to collect the training data. We validate TermPicker's recommendation quality by performing one evaluation on the DyLDO 21 9 dataset and a second evaluation on the Billion Triple Challenge BTC 2014 dataset 22 10 crawl no. 52 % of these links reference another document within WT2g but only 0.12 % reference a different server within WT2g. For decision trees in particular   , the small workloads result in very minimal classifier training times. Figure 11 left shows the performance of the recommendation for the AlgoViz Fall 2009 dataset.  Number of reported bugs. 14 The code used to create the LOTUS index is also publicly available. Table 3gives detailed descriptions of two topics in blog06 and blog07. The BLOG06 corpus contains feeds ranking in size from just 1 or 2 posts to feeds with several hun- dred. As Figure 1 shows  , its popularity is constantly growing; in January 2016  , 135 ,000 repositories on the GitHub social coding site received more than 600 ,000 pull requests. This can be attributed to larger categorical attribute dependencies being used in the detection process for the KDDCup data set. Auto- Comment extracts code-descriptions mappings  , which are code segments together with their descriptions  , from Stack Overflow  , and leverages this information to automatically generate descriptive comments for similar code segments in open-source projects. Evaluating word relatedness is a natural ability humans have and is  , therefore  , considered a common baseline. Questions on Stack Overflow are marked 'closed' if they are deemed unfit for the question-answer format on Stack Overflow and indicate low quality. GitHub facilitates collaborative development through project forking  , pull requests  , code commenting  , and merging. For each query or document  , we keep the top three topics returned by the classifier. Thus our hypothesis is that  , outside of the small portion of celebrities who get followers just by their mere presence  , the majority of Quora users attract followers by contributing a large number of high-quality answers. In particular  , in the WebKB task  , the attributes significantly impair RDN performance. The goal of this work is to obtain a deep understanding of the pull-based software development model  , as used for many important open source projects hosted on Github. We randomly selected email addresses in batches of ten. Since Quora has no predefined topic structures for its questions questions can have one or more arbitrary topic " labels "   , getting the full set of all questions is difficult. This again suggests that the distribution of relevant documents played an important role in the determination of topic temporality. In this section we present descriptions of the GitHub setting  , our data collection procedures  , measure calculation  , and analysis technique. Training corpus changes. We use this as a minimum threshold for our later analyses on social factors on system performance. We filter out those points which are either outside of the city boundary or in the ocean. Duplicate sentences selected by more than one approach were only shown to participants once. The OpenStreetMap project has successfully applied the Wiki approach to geo data. The classes and segments are shown in Table 1. We hypothesized that certain topical categories of tasks are more likely to be resumed than others see also 10 . The assumptions we make on the considered dataset are as follows. All works propose interesting issues for SRC. In particular  , if we ranked all systems including ours according to their accuracy on each of the six test sets and compute their average ranks  , our model would be ranked first in both subtasks  , A and B. As a second future work  , we plan use our motif framework as a way to analyze other evolving collaborative systems  , such as non- Wikimedia Wikis  , such as Wikia and Conservapedia  , which have very different editing policies and user bases. TSA results shown in the table are computed using cross correlation with a quadratic weighted function as the distance metric between single time series. For example   , BLOG06-feed-000017 is associated with no permalinks in 20051206/feeds-000.gz according to <PERMALINKS> tags  , but the feed actually contains several permalinks  , such as Http://www. MacHall. Com ?strip id=357. In this paper we describe generation of datasets based on the Open Directory Project ODP  , http://dmoz.org  , although the techniques we propose are readily applicable to other Web directories  , as well as to non-Web hierarchies of documents see Section 2. Prolific Developers. UMLS is used to find the synonyms of the technical terms or phrases not recognized by AcroMed or LocusLink. OpenStreetMap datasets are available in RDF format from the LinkedGeoData project 9 . This estimate might provide an upper bound of actual number of questions  , and our coverage of 58% would be a lower bound. Using recently acquired hardware we have reduced this time to below 2 seconds per query. Datasets. The 1051 pages were manually classified into the categories of course 230 pages and non-course 821 pages. The WT2G collection is a general Web crawl of Web documents  , which has 2 Gigabytes of uncompressed data. With the advent of social coding tools like GitHub  , this has intensified. We illustrate the basic ideas through a cost-sensitive example even though the concept is applicable to both cost-sensitive and traditional accuracy-based problems. KDDCUP 2005 provides a test bed for the Web query classification problem. We represent a document by a vector of categories  , in which each dimension corresponds to the confidence that the document belongs to a category. The configuration can determine the replay policies  , such as whether to emulate the networking latencies. Table 7: Optimal hyper-parameter on all retrieval methods over both types of verbose queries tuned for MAP on WT2g. We used Github data as provided through our GHTorrent project 16  , an off-line mirror of the data offered through the Github API. The taxonomy we used in the paper is from Open Directory Project ODP  , http://dmoz.org/. Nasehi et al. Selecting Applications. The match between geolocation and language improves when we compare location breakdown with the language breakdown for blogs collected by BlogPulse in October 2006. Our study is based on data from the Github collaborative development forge  , as made available through our GHTorrent project 16. For instance  , the most popular of these services  , Wikia 2   , has more than three thousand collections  , some of them with more than fifty thousand documents. Once a week for 14 weeks we crawled each website and reconstructed it with Warrick. Overall  , the project had produced a 160GB database of geo data until July 2008  , in some regions surpassing commercial geo data providers in terms of precision and detail. Various estimates of user growth include numbers such as 150% growth in one month  , and nearly 900% growth in one year 23. Our preliminary findings  , obtained through the analysis of archival data from Stack Overflow and qualitative coding  , indicate that Q&A websites are particularly effective at code reviews  , explaining conceptual issues and answering newcomer questions. The SHOE Knowledge Annotator is a Java program that allows users to mark-up webpages with the SHOE ontology. Stack Overflow is a collaborative question answering Stack Exchange website. We use a 482-class topic taxonomy from DMoz http://dmoz.org/ and a sampling and classifying technique that we will describe in §2. The list of the Web sites were collected from the Open Directory http://dmoz.org. WebKB: The WebKB dataset 5 contains contains 8145 web pages gathered from university computer science departments . To do this automatically we use the content-based classifier described and evaluated in 1. We crawled 1 ,546 ,441 Web pages from ODP which spanned over 172 ,565 categories. Both lines increase smoothly without gaps  , suggesting that Quora did not reset qid in the past and the questions we crawled are not biased to a certain time period. , BlogPulse and Technorati. We analysed the Blog06 collection using SugarCube. According to a recent survey of Quora users 31  , they tend to follow users who they consider interesting and knowledgeable . The tags were mainly used to learn about the topics covered by Stack Overflow  , while the question coding gave insight into the nature of the questions. Figure 2: Performance trend MAP as the single smoothing hyper-parameter λ  , µ  , and ω changes for each language model on the WT2g tuning collection for description only queries top and for description and narrative queries bottom. However  , few of the previous works focus on detecting semantic relationships. If  , for instance  , an important website is not listed in a directory such as dmoz.org  , it will not be considered by the BN-based crawler. The results presented in the experimental section were obtained using the Quora topic model as the background knowledge model. Our benchmark meets all the aforementioned requirements. We gathered our Quora dataset through web-based crawls between August and early September 2012. We also used the API to gather information on all issues and comments for each repository. Table 3shows the performance of our model compared to the top four models in the SemEval 2015 competition note that only the F1-score is reported by SemEval for this task and ParagraphVec. Community Value. The Billion Triple Challenge dataset was crawled based on datasets provided by Falcon-S  , Sindice  , Swoogle  , SWSE  , and Watson using the MultiCrawler/SWSE framework. We repeat this process five times to compute 5-fold cross validated results. Figure 15 plots the complementary cumulative distribution function CCDF for both the incoming degree follower and outgoing degree followee. Also  , they have to be located in the Semantic Web. Our selection of projects and contributors to GitHub projects using the pull-based model may not be indicative of the average project. In the hundred relation most of the attributes have exactly 100 unique AS3AP benchmark: the storage organization of the relation and the selectivity factor of the query. XCRAWL also implements the automatic identification of an initial set of websites that are likely to contain pages with target data  , providing an effective start point. For the arithmetic component  , other codes include overflow and zero divide. As stated above  , this task is ranking blog feeds in response to a query  , not blog posts. Therefore  , despite the presence of comprehensible and explicit question posting guidelines – Stack Overflow receives a high number of extremely poor quality questions which are not fit to exist on its website. First  , we prepare the training data and testing data  , including those GeneRIFs existed in LocusLink and the corresponding Medline abstracts. Generalizability – Transferability. We formed the feature set by selecting the 200 most informative features word counts as measured by information gain. Stack Overflow is a programming based CQA and the most popular Stack Exchange website consisting of 5.1M questions  , 9.4M answers and 2.05 registered users on its website. The proposed model was shown to be effective across five standard relevance retrieval baselines. The dynamic of the OpenStreetMap project will ensure a steady growth of the dataset. To account for potential measurement errors when matching social media data with streets  , we add a buffer of 22.5 meters around each street's polyline. Stack Overflow 4 : This dataset comes from a popular question answering service found among the datasets of the Stack Exchange XML dump. Our analysis relies on two key datasets. 39  , since it also harnesses the natural language text available on Stack Overflow. This situation raises questions about whether social features are useful to contributors. Most notably  , we have only reported MAP scores for the MoviePilot data. The relevance judgements were obtained from the LocusLink database 11. Hence  , we envision some extensions to Triplify such as a more external annotation of the SQL views in order to allow optionally SPARQL processing on Triplify endpoints. Actually  , full-fledged functional templating is supported only by MediaWiki and Wikia which is MediaWikibased . The topics were assigned to pages based on their content using a text-based classifier described and evaluated in 6. For example  , each insight sentence could be accompanied by an expandable widget which shows the entire thread on Stack Overflow from which the insight sentence originated. The participants where selected from the community of Semantic Web SW developers on Github who have had at least one active SW-related repository. In the 2 years since its foundation in 2008  , more than 1 million questions have been asked on Stack Overflow  , and more than 2.5 million answers have been provided. By this method  , an input query is first mapped to an intermediate category  , and then a second mapping is applied to map the query from the intermediate category to the target category. The classic Rocchio's model  , fails to obtain improvement on the WT2G collection. To the best of our knowledge  , this is the first work which studies poor quality questions on a large-scale CQA website like Stack Overflow. Linked- GeoData is derived from OpenStreetMap and OpenStreetMap is an open  , collaborative bottom-up effort for collecting this large-scale spatial knowledge base. each query request is associated with one or more clicked Web pages  , forming a " query session "   , which can be defined as follows: The statistics showed that the vast majority of URIs contained a title and in only 1.1% of all cases no title could be discovered. Two of the four evaluation metrics used in our study—coverage  , and diversity—required information about page topicality and query interest. Figure 14shows this underlying question quality pyramid structure on Stack Overflow. Our results show that normalization can be important  , and that the best normalization strategy is dependent on the underling relevance retrieval baseline. Quora. A connection threshold of size k for an edge indicates that two users have viewed at least k common pages. On average  , each document within the collection includes 9.13 outgoing links. To evaluate the performance of our algorithm  , experiments were performed using a set of classified Web pages extracted from the Open Directory Project ODP http://dmoz.org/. were detailed earlier in this document. The statistics show that Stack Overflow is a very popular programming CQA with 5.1M questions   , 9.4M answers and 2.05M registered users. First 100 elements obtained from three different ranking methods  , tf -idf   , BM 25  , and Rejection are pair-wise compared in Figure 5. Moreover  , the classification accuracies are not uniform across all subject areas. Note that not all questions remain on the site  , as Quora actively deletes spam and redundant questions 5. As Quora continues to grow  , it is clear that helping users easily identify and find the most meaningful and valuable questions and answers is a growing challenge. These are the two Wikia encyclopedias with the largest number of articles evaluated by users regarding their quality. GeneRIF snippets sometimes contain direct quotations from article abstracts but they might also include or paraphrase certain texts extracted from article titles or abstracts. In addition to using Triplify for publishing RDF from the long tail of million of Web applications deployed  , we evaluated the software with the very large datasets produced by the OpenStreetMap project 14 . Note that it is also not the full set of Maven projects  , since Github only returns 99 pages of search results. Table 8provides details on the number of presumed splog posts which infiltrated each element of the relevance scale. Basic biology includes isolation  , structure  , genetics and function of genes/proteins in normal and disease states 9. According to the Stack Overflow guide 2   , a good answer  , besides being correct   , should be clear  , provide examples  , quote relevant material  , be updated  , and link to more information and further reading. However  , our sample of programs could be biased by skew in the projects returned by Github. In our comparative experiments  , we choose the best-first algorithm and the accelerated focused crawler 1 as two other alternatives. The most common use of Stack Overflow is for how-to questions  , and its dominant programming languages are C#  , Java  , PHP and JavaScript. For each query  , the lexicons are applied in the order of AcroMed  , LocusLink  , and UMLS for query expansion. The relatedness of these pairs of words is then evaluated using human annotators   , as done in the WS-353 dataset. However  , the social interaction among Quora users could impact voting in various ways. Assuming we are correct about the use of qid  , we can plot an estimate of the growth of Quora and Stack Overflow   , by plotting qid against time. We do suggest caution being taken when reviewing the Small Web Task to take the results in the context of the WT2g dataset  , lest one conclude that Connectivity Analysis does not improve precision in any case.  WebKB 4 Universities Data WebKB: This data set contains 8  , 282 web pages collected in 1997 from computer science departments of various universities  , which were manually categorized into seven categories such as student  , faculty  , and department. There are 59 ,602 transactions in the dataset. In particular  , our projections suggest that Chinese and Russian should appear prominently in the language based segmentation. Instead of artificially constructing Web content based on a model of typical Web 2.0 applications  , WPBench uses the real data from users' actually browsing and interacting with Web 2.0 sites. The evaluation was structured as follows: Only URLs identified by the " r:resourcE' tag were considered. Topics 1  , 2  , 4  , and 5 are mostly related to AlgoViz catalog entries  , These topics are prominent in clusters 2  , 4 and 5. What role do the " related questions " feature play ? The topic structure defined in our poster is extracted from the top 16 categories in the ODP taxonomy http://dmoz.org. One of the data sets contains 111 sample queries together with the category information. Point annotations  , for example  , are originally stored as comma separated property-values assignments in a BLOB column within the database. The effectiveness of pseudo relevance feedback is reconfirmed in this set of experiments. There are a total of 36 ,643 tags on all questions in Stack Overflow. We chose subject programs by looking at bug reports for popular JavaScript projects on GitHub. In both datasets TSA significantly outperformed the baselines. Some users are mainly interested in bibliography entries. Also  , the infrastructure we used for the analysis is available open source as a GitHub repository 5. , via GitHub is gaining popularity among distributed software development community  , the need to continue studying and supporting the evolution of large long-lived OSS projects remains as important as ever. To enable a richer analysis and of different feature sets we employed classifiers to assign topical labels to the clicks using the hierarchy from the Open Directory Project ODP  , dmoz.org 5 and the complexity of the queries/results  , based on estimates of their U. S. school grade level on a 1-12 scale 12. First  , for a meaningful search result  , we need to consider data obtained by integrating multiple data sources  , which may be provided by autonomous vendors in heterogeneous formats e.g. The purpose was withheld so to not affect the outcome. The BTC dataset contains 10 million quadruples  , but we used smaller excerpts containing 100  , 250 and 500 thousand unique quadruples. 3 The Billion Triple Challenge dataset was created based on datasets provided by Falcon-S  , Sindice  , Swoogle  , SWSE  , and Watson using the MultiCrawler/SWSE framework. Applied to API documentation and content from Stack Overflow  , the idea is to create a summary of the discussions on Stack Overflow as they relate to a given API type  , assuming that the reader is already familiar with the type's API documentation. This provides a consistent topical representation of page visits from which to build models. Projections. In addition  , we propose a category-selection method to select the categories in the intermediate taxonomy so that the effectiveness and efficiency of the online classification can be improved. This paper addresses these questions by an empirical analysis that uses a part of a standard blog corpus: the corpus offered by Blogpulse for the Weblogging Ecosystem workshop 2006. The question dataset stack overflow  , question  consists of 6 ,397 ,301 questions from 1 ,191 ,748 distinct users  , while the answer dataset stack overflow  , answer consists of 11 ,463 ,991 answers from 790 ,713 distinct users. To analyze the different kinds of questions asked on Stack Overflow  , we did qualitative coding of questions and tags. Knowledge-free systems employ co-occurrence and distributional similarities together with language models. The average classification accuracies for the WebKB data set are shown in Table 3. For this year's task is based on Billion Triple Challenge 2009 dataset. Synonyms from genetic databases were sought to complement the set from LocusLink. For merged pull requests  , an important property is the time required to process and merge them. In this section  , we analyze the Quora social graph to understand the interplay between user social ties and Q&A activities. We conduct the first large scale study of deleted questions on Stack Overflow. Given the large number of pages involved  , we used automatic classification. In Subtask E of the SemEval 2016 Task 4 shared task a subtask which deals with ordinal tweet quantification by sentiment – see 8   , the system described in this paper obtained an EM D score of 0.243  , ranking 1st in a set of 10 participating systems  , with a high margin over the other ones systems from rank 2 to rank 8 obtained EM D scores between 0.316 and 0.366. The method of choosing the WT2g subset collection was entirely heuristic. The assessor then searched the Blog06 test collection to see if blog posts with relevant opinions appear in the collection. The final processing step computes a number of performance metrics for the generated dataset. However  , the words in the WS-353 dataset are relatively common  , and primarily related to static concepts  , such as " car " and " love " . We tested SugarCube on the Blog06 collection 5 . webkb 4 The task is to classify university webpages as student  , course  , faculty  , or project 4 ,199 instances. , OpenStreetMap or Open Government Data data  , a restaurant guide  , etc. For simplicity we randomly sampled 300 websites from dmoz.org as our initial set of URLs. 19 found that when GitHub developers engage in information-seeking behaviors  , they use signals in the environment to form impressions of users and projects. Opinion identification is accomplished by combining the four opinion modules that leverage various evidences of opinion e.g  , Opinion Lexicon  , Opinion Collocation  , Opinion Morphology. Usage instructions and further information can be also found at http://LinkedGeoData.org. WebKB. We use two AlgoViz DSNs created from log data captured in Fall 2009 and Spring 2010. In addition  , from Table 4 we observe that PRoc3 outperforms the other two on the WT2G collection. The results provide evidence for the need to weigh the recent changes in time series distance measurement higher than the ancient changes. An example for the LocusLink lexicon is that the acronym " psen1 " corresponds to a list of aliases " ps-1  , pre1  , psen  , zfps1  , zf-ps1 " . I always got these favorites and these retweets  , and then I got followers on GitHub on the project. " Stack Overflow questions contain user supplied tags which indicate the topic of the question. Algorithm 2 needs to use AcroMed and LocusLink databases for query expansion. For our empirical analysis  , we use the different segments of the data set provided for the Billion Triple Challenge BTC 2012. With both the ESA index and the proposed selectioncentric context language model pw|s  , c  , we can compute a selection-centric context semantic vector Vs  , c based on the centroid of the semantic vector of each term. Figure 8top left shows the accuracy of the classifier for the AlgoViz Fall 2009 dataset. The Web Data Commons project extracts all Microformat  , Microdata and RDFa data from the Common Crawl Web corpus and provides the extracted data for download in the form of RDF-quads or CSV-tables for common entity types e.g. For each input URL the server would respond with a list of incoming links from other WT2g documents and outgoing links. WebKB consists of 1051 web pages collected from web sites of computer science departments of four famous universities in U. S. We used a set of 9 ,403 recent MEDLINE documents associated with LocusLink GeneRIF records. In the first experiment set we used a Giant Strongly- Connected Component of the WebKB hyper-link graph 8. This is because Quora recommends topics during the sign-up process. In the next sections  , we describe our investigation of the means to automatically identify sentences on Stack Overflow that are meaningful and add useful information not contained in the API documentation. Nevertheless  , we have adapted the AS3AP benchmark to fit into our purposes. This may explain the relatively small absolute improvement of tLSA over LSA. In fact  , by taking the OpenStreetMap polygons for Santa Barbara and Ventura and defining a regular point grid of 1 × 1 km  , we can compute the probability of grid points contained in Ventura to locate in the southeast of Santa Barbara grid points. To pre-train the weights of our network  , we use a large unsupervised corpus containing 50M tweets for training the word embeddings and a 10M tweet corpus for distant supervision. ODP has also provided a search service which returns topics for issued queries. Furthermore  , according to global OpenStreetMap statistics 1   , Italy and UK are ranked 7th and 10th for number of created spatial objects  , and 4th and 5th for density of created spatial objects per square kilometer. In the replaying stage  , the data in WPBench Store are fed to browsers by a proxy according to the local configuration so that browsers could obtain the Web content as if they were actually from the Internet. We denote such documents as partially-structured  , largely-naturallanguage PSLNL documents. The performance is measured as the average F1-score of the positive and the negative class. The work described in 10   , for instance  , is based on the first assumption and is implemented as a combination of two focused crawlers: one to discover relevant websites and the other to crawl them. Therefore   , Stack Overflow has attracted increasing attention from different research communities like software engineering  , human computer interaction  , social computing and data min- ing 6  , 9  , 10  , 21  , 22. For the phrase-level subtask the size of the word type embeddings  , which encode tokens that span the target phrase or not  , is set to 10. Its responsiveness performance is closer to users' perception than any of other benchmarks. The key issue is how to get function words and introducers and how to measure such scores. In the absence of adequate explicit user feedback  , AlgoViz usage data has helped us to generate networks and find common usage patterns. The Web Data Commons project extracts all Microformat  , Microdata and RDFa data from the Common Crawl Web corpus  , the largest and most up-to-data Web corpus that is currently available to the public  , and provides the extracted data for download in the form of RDF-quads and also in the form of CSV-tables for common entity types e.g. In order to test whether the associated hypothesis is true  , we developed a software application which would produce results based on conventional Content Analysis the baseline result and then re-rank those results based on a number of related Connectivity Analysis approaches. Structured call sequences are extracted from open-source projects on GitHub. The discovery strategy is based on observations of typical documents. This dataset was used in KDDCUP 2000 18. The underlying theme of Stack Overflow is programming-related topics and the target audience are software developers  , maintenance professionals and programmers . We assigned URLs in our dataset to categories in the Open Directory Project ODP  , dmoz.org in an automated manner using a content-based classifier  , described and evaluated in 4 . Pull requests and shared repositories are equally used among projects. To complete this annotating procedure  , we have to deal with the first stage automatically since the coverage of GeneRIF records in LocusLink depends on human experts and it cannot come up with the speedy growth of the literatures. Different gold standards have been used for the evaluation of SRC algorithms among which the most cited are: AMBIENT 6  , ODP-239 10  , MORESQUE 27 and SEMEVAL 28 . To create the user graph cf. To identify topical category  , we use automatic query classification into the top two levels of the Open Directory Project ODP  , dmoz.org hierarchy . In GitHub a user can create code repositories and push code to them. This work is situated in the context of an information extraction framework developed in 6  , 7. We also evaluated the performance of SimFusion+ on D- BLP and WEBKB datasets. The Disk1&2  , Disk4&5 collection contains newswire articles from various sources  , such as Association Press AP  , Wall Street Journal WSJ  , Financial Times FT  , etc. In Section 4  , we briefly introduce the previous methods and put forward a new method. The basic statistics of both datasets are shown in Table 1Quora. We used the corpus offered by Blogpulse for the Weblogging Ecosystem workshop 2006 2 to refer to a standardized set of texts. For getting the informative words  , i.e. We have implemented a contextualization system that we are now extending with new features for a publication in the near future.  dimacsAw20w5: Representation: Windows with halfwindow size 20  , selected using LocusLink information. Table 3shows the overall statistics of user-generated content on Stack Overflow between August 2008 inception to June 2013 current. In addition  , the training data must be found online because   , in general  , labeled training data for query classification are very difficult to obtain. The first data set was collected by the WebKB Project 3. On the other three collections  , the performance of all the three PRoc models is very close. 4 and is not applicable here. The database dump contains publicly available information of questions  , answers  , comments  , votes and badges from the genesis of Stack Overflow August 2008 to the release time of the dump. We can see that the performance on Blog-2008 is worse compared to Blog06 and Blog 07. Over the last couple of years GitHub 4   , which is the most popular repository hosting service for Git projects  , has taken the open source community by storm 19. , surrounding code snippets  , the complete answer   , or the corresponding question is available on Stack Overflow  , it would be possible to display it along with an insight sentence. and WT2g. We use the pages chosen by the Open Database Project ODP -see http://dmoz.org. The data consist of a set of 3 ,877 web pages from four computer science departments  , manually labeled with the categories: course  , faculty  , staff  , student  , research project  , or other. In previous work 13  , we were able to recruit such participants from GitHub 3 . Question Topics. The out-links file consisted of  , for each document d  , the document numbers of the documents d links to. The collection included a selection of " top blogs " provided by Nielsen BuzzMetrics and supplemented by the University of Amsterdam. Our snapshots were complete mirrors of the 154 Web Sites. So In order to facilitate better classification  , we increased the dataset by manually annotating some splog in the Blog06 dataset itself. After that  , we design the experiments on the SemEval 2013 and 2014 data sets. The SHOE Knowledge Annotator is rather a little helper like our earlier OntoPad 12  , 5 than a full fledged annotation environment. Despite their different topics of interest  , Quora and Stack Overflow share many similarities in distribution of content and activity. Before creating an index of the blog06 corpus  , we extract textual information from the permalink files. Given such a dataset  , a naNe application of classification such as decision tree would result in no useful information. Selection Criteria. This cluster contains 43 questions  , and all questions are related to " Quora. " The existing intermediate taxonomy used in the paper is from Open Directory Project ODP  , http://dmoz.org/. This open-source alternative mapping service also publishes regular database dumps. Using it  , we first explore the use of almost 2 million pull requests across all projects in Github. In general  , such a set of features is based on datasets and vocabularies used in some LOD collection  , e.g. The winning solution in the KDDCUP 2005 competition  , which won on all three evaluation metrics precision  , F1 and creativity  , relied on an innovative method to map queries to target categories. Q5 Last but not least  , which computational and empirical methods are suited to analyzing these questions ? To evaluate the performance of the contextualization system  , we are going to use the TWSI dataset 4 here as well. We crawled 1 ,546 ,441 Webpages from ODP which spanned over 172 ,565 categories. 8 GitHub user profiles  , confirm this consideration. Since we decided to focus on Milano and London  , however  , we can discard this potential issue: our direct knowledge of the city of Milano let us affirm that the spatial objects mapping is quite good and homogeneous throughout the city; OpenStreetMap coverage in the London area was evaluated in 18 and shown to be quite accurate in comparison to official sources. For this dataset  , we also gathered information about each unique GitHub user associated with the set of pull requests. Quora is a question and answer site with a fully integrated social network connecting its users. In the AcroMed lexicon  , entries are indexed by technical terms or phrases  , and each entry is a list of acronyms associated with the corresponding technical term/phrase  , accompanied by the frequencies of such associations. IDF was calculated on the corpus of all 429 ,183 blog posts from the 4th July that were contained in the original Blogpulse corpus. We located the words from the GeneRIF within the title and abstract. Pyramid. Such hierarchical sentiment analysis model is applied to the whole Blog06 corpus to generate an opinion polarity judgment list for all the documents  , combined with the corresponding sentiment strength within interval 0  , 1. All these browsers can browse all the Web sites in WPBench normally except that IE 8 beta and Firefox 3.1 beta cannot browse one of them due to unsupported features used by the Web site. Nick Craswell developed software for extracting hyper-link connectivity information from WT2g. More precisely  , the goal was to reproduce the GeneRIF Gene Reference into Function used in the LocusLink 1 database  , either from a Medline record or from the entire article. We also examined the top ranked features by expected entropy loss from the full-text of the WebKB dataset categories of courses and faculty. The spatial data is collected by the OpenStreetMap 5 project and it is available in RDF format. In the UMLS lexicon  , entries are indexed by technical terms or phrases  , and each entry is a list of synonyms associated with the corresponding technical term/phrase. Therefore  , we integrated the professional chemical information from the suggested website ChemID plus 5 and PubChem 6 in our Algorithm 1. a5 derives from the observation that because of the rich context of blogs  , captured for example in hyperlinked sources  , important terms may not actually be frequent in the post itself  , such that their being unusual high IDF creates a better indicator of importance 10. Next  , we plot the distribution of views and answers per question in Figure 5and Figure 6. The Github API data come in two forms; a streaming data flow lists events  , such as forking or creating pull requests  , happening on repositories in real time  , while a static view contains the current state of entities. For example  , for the category " staff " of the WebKB dataset  , the F 1 measurement is only about 12% for all methods. Synonyms are the first type of words for which the TSA method seems to outperform the ESA method. Most of the proposed systems for this task see for example 6 exploit IR indexing and ranking techniques over the RDF dataset used at the Billion Triple Challenge 2009. To validate this statement  , we performed several small experiments where we added small bursts of new meaningful questions to Quora. By using the annotated hierarchical taxonomy of Web pages such as the one provided by ODP website http://dmoz.org/  , we can build a thematic lexicon. For comparison  , we applied our method for both classification and naming to full-texts for the categories of courses and faculty from the WebKB dataset. As an example  , let us consider the KDDCUP'99 " intrusion detection " dataset that is widely used in the stream mining literature. GitHub is based on the Git revision control system 6 . We initially wanted to choose a random set of websites that were representative of the Web at large. To describe those segments  , we rely on data gathered and distributed for free by OpenStreetMap OSM a global group of volunteer cartographers who maintain free crowdsourced online maps and by Ordnance Survey the national mapping agency for Great Britain. To assess the quality of our ESA index   , we apply it to compute word relatedness on the widelyaccepted WS-353 benchmark dataset 12  , which contains 353 word pairs  , and our experiments show a Spearman's rank correlation of 0.735  , which is consistent to the previously reported numbers 16  , 17. In Table 13  , we show the MAP scores of our best runs on opinion finding and polarity tasks based on different datasets for comparison Blog06  , 07  , and 08. , a huge collection of RDF graphs that was crawled by a Linked Data crawler like the Billion Triple Challenge dataset. In total  , 1 ,000 ,000 collaborative GitHub projects i.e. To investigate the problem  , we closely looked at the blog06 corpus and found that many permalink URLs were not properly extracted from the corresponding feed files. LocusLink is used to find the aliases of the acronyms identified by AcroMed. Members of the GitHub community regard certain members as being at a higher standing. Given the datasets above  , we now describe how we tested and measured the efficacy of the recommendation algorithms described in Sections 2 and 3. Update summarization is often applied to summarizing overlapping news stories. Quora makes visible the list of upvoters  , but hides downvoters. As it is commonly used in many topic classification studies   , we used the Open Directory Project ODP  , dmoz.org ontology of the web to study the empirical effectiveness of our proposed approach. We tested and evaluated Triplify by integrating it into a number of popular Web applications. The study was performed through a webpage mimicking the look-and-feel of the moviepilot website  , on this page users were presented with a random selection of movies they had previously rated  , with the ratings withheld. We analyzed development activity and perceptions of prolific GitHub developers. A sentence classifier was built using GeneRIF entries in LocusLink excluding those that were in the secondary .txt file and their abstracts. We used the Github Archive database 4 to make a list of the most-watched Rails-associated repositories. Our data is aggregated every 60 minutes  , comes from both TIM customers and roaming customers in the six cities  , and covers the time ranging from February to October 2014. In Fig. , products  , organizations   , locations  , etc. The associated subset is typically called WebKB4. To confirm this intuition we randomly sampled another set of URIs from dmoz.org a total of 10  , 000 URIs and parsed their content for the title. It contains contextualized substitutions for about 150 ,000 sentences  , a larger collection than used for SemEval WSD tasks. We analyze the question-answering Q&A site Stack Overflow  , which makes extensive use of badges and was one of the first sites to use them on a large scale. To determine the probability that a GeneRIF would be found in a particular position  , we annotated a set of 200 MedLine entries from LocusLink associated with GeneRIFs. Related to our solution for linking Stack Overflow threads to API types is the work by Rigby and Robillard 30. in that we focus on single sentences from Stack Overflow that are relevant to an API type instead of a code snippet. WebKB The WebKB dataset contains webpages gathered from university computer science departments. Profile based features are based on the user-generated content on the Stack Overflow website. We utilized a GitHub dataset collected during prior work that contains information on prolific developers with a long and active contribution history 10. The WT2g connectivity data see http://pastime.anu.edu.au/WAR/WT2g_Links/ilink_WTonly.gz and the Small Web qrels file were used to find the set of documents which link directly to relevant documents. We evaluate our approach using the evaluation framework used in the Semantic Search Challenge 2010 3 . For the comparison between ORCA and LOADED  , we used the 10% subset of the KDDCup 1999 training data as well as the testing data set  , as ORCA did not complete in a reasonable amount of time on the full training data set. A publicly available dataset periodically released by Stack Overflow  , and a dataset crawled  from Quora that contains multiple groups of data on users  , questions   , topics and votes. Overall  , these results are encouraging and preliminary at the same time. Since OpenStreetMap is a prominent example of volunteered geographic information VGI 7  , LinkedGeoData knowledge reflects the way in which the environment is experienced 8 . SISE will only work if a topic is discussed on Stack Overflow. The survey participants reported development experience was 17.2 years on average median 15; range 7 to 40  , while their GitHub experience was 5.9 years on average median 6; range less than 1 to since GitHub was founded. A search for " internet service provider " returned only Earthlink in the top 10. We find a total of 9 ,350 undeleted questions on Stack Overflow. This figure shows the feasibility of maintaining the knowledge bases and ontology using natural language processing technology. In fact  , contributing to as many GitHub projects as possible is an accomplishment  , valued by peers and employers alike 32. The project has been collecting data since February 2012. Sampling projects and candidate respondents. In the LocusLink lexicon  , entries are indexed by acronyms  , and each entry is a list of aliases that are only associated with the corresponding acronym but no other acronyms. BM25 slightly outperforms LM with Dirichlet prior on the WT2G collection. Sampling uniformly from the Web is currently not possible 35  , so we sampled from the Open Directory Project ODP at dmoz.org. ODP is an open Web directory maintained by a community of volunteer editors. Firstly  , we classified trail pages present in into the topical hierarchy from a popular Web directory  , the Open Directory Project ODP dmoz.org. The proposed methods LIB  , LIB+LIF  , and LIB*LIF all outperformed TF*IDF in terms of purity  , rand index  , and precision. 4. Since this context e.g. Each page was described by 8 ,000 dimensional feature vector. 1. Table 5: Results of the Dual C-Means algorithm for ODP-239 and SEMEVAL. Using various data sources of substantial size gives the opportunity to find intended POIs  , which may fall into multiple concepts ranging from rather generic to more detailed ones such as " restaurant " vs. " pizzeria. " In forums such as Stack Overflow  , the answers are expected to be correct and should be ranked according to their quality. We note that the MoviePilot data does not contain the group information for all the users in the training data. For example  , impressions of general coding ability could be gleamed from the contents of a GitHub user's profile. , GitHub and bringing them to their own working environments. Github automatically detects conflicting pull requests and marks them as such. AS3AP is the ANSI SQL Standard Scaleable and Portable Benchmark for comparing relational DBMSs. We test our model on two subtasks from Semeval-2015 Task 10: phrase-level subtask A and message-level subtask B 1 . In order to handle the sheer size of the DMOZ hierarchy  , we included only the first three levels of the hierarchy in our experiments . This set of user information includes 95 ,270 unique GitHub user accounts. In this paper  , we report the benchmark called WPBench Web Performance Benchmark that we have recently designed and developed to measure the performance of browsers for Web 2.0 applications. In particular  , it tends to give high results when the other metrics decrease. Table 2shows the most prominent words for each of the chosen topics from the Quora topic model. While there exist many bibliographic utilities comprehensive list e.g. Not surprisingly  , questions under well-followed topics generally draw more answers and views. Pull Requests in Github. NIST assessors referred to the WT2g collection during the process of ad hoc topic generation. in the following way: the first two recommendations are irrelevant  , and the first relevant recommendation is at the third rank of the result list. Still  , the mapping can be inhomogeneous some zones can be more detailed annotated than others. for functional languages — would be less justified. The community counts its users in hundreds of thousands  , ratings in dozens of millions and movies in tens of thousands. A procedure 5 All data sets except the largest one are breadth-first crawls of sunysb.edu domain starting from http://www.sunysb.edu. There are a number of ways in which graphs can be analyzed  , graph partitioning being one. We can see our re-ranking procedure successfully rescores almost all the target documents into the top 100 results. Despite its short history Quora exited beta status in January 2010  , Quora seems to have achieved where its competitors have failed  , i.e. Our approach achieves a significant improvement by 8% over IG for both classifiers when the whole WebKB collection is applied. Thus  , the problem to be solved in this paper is to develop flexible techniques for discovering patterns in PSLNL documents. This is not surprising  , as the BlogPulse blog data was used as a source set of blog urls for harvesting blog author profiles. The evaluation is done on three collections of tweets that were manually annotated to positive and negative classes: 6Hu- manCoded 5   , Sanders 6   , and SemEval 7 . Deep analysis shows that ARI embodies an interesting property for the SRC task as it is well-known that the sizes of the clusters are not distributed equally on the Web. We use the Billion Triple Challenge BTC collection 3   , a publicly available Semantic Web crawl; we consider this collection as a reasonable sample of Linked Open Data LOD. As an effort to provide additional evaluation data in this problem domain  , we created a new dataset 1 to further evaluate our results upon. We observed 56K topics in our dataset  , which is twice more than that of Stack Overflow  , even though Quora is smaller by 0   20   40   60   80   100   10 0 10 1 10 2 10 3 10 4 10 5 10Table 2lists the top 10 topics with most number of questions in each site. Performance results for retrieving points-of-interest in different areas are summarized in Table 3. Community based features are derived via the crowdsourced information generated by the Stack Overflow community. After excluding splogs from the BlogPulse data  , we The ODP metadata being used was downloaded from dmoz.org in September 2004  , and contains 0.6 million categories and 4.4 million leaf nodes. We collected the MEDLINE references as described before  , LocusLink has a set of references to MED- LINE documents relevant to the gene for documents corresponding to each organism in LocusLink. In this article  , we refer to this sample as WPEDIA. We took SPARQL Endpoints from the SPARQLES survey 3  , vocabularies from Linked Open Vocabularies LOV 2 and prefix.cc  , and we augmented these data with spidered data from the Billion Triple Challenge BTC 2014 13 dataset. In this paper  , 3 http://dmoz.org/ SocialPageRank is proposed to explore static ranking from social annotations and capture the preference of web annotators. Answers and Stack Overflow  , there is no formalized friendship connection. First  , what triggers Quora users to form social ties ? BLOG06 is a collection of blog home pages  , blog entry pages permalinks and XML feed documents. By explicitly identifying the sense of a word  , the system does not have to determine the sense of the user's category annotation or query. In an effort to bring documentation from different sources together  , we presented an evaluation of different techniques for extracting insight sentences from Stack Overflow. In our experiments we used real data that were taken from the Billion Triple Challenge BTC dataset small crawl 6 . Taking the coffee sense of the word Java  , taking a path through the DMOZ tree would give us: http://dmoz.org/../Coffee and Tea/Coffee. The user-topic interaction has considerable impact on question answering activities in Quora. This effectively creates a related question graph  , where nodes represent questions  , and links represent a measure of similarity as determined by Quora. One of the prominent collections of AlgoViz is the bibliography of publications related to algorithm visualizations . One example of a project that combines an educational portal with online community is the AlgoViz Portal http: //algoviz.org. Although none of these sites are represented in the WT2g dataset  , we had to take this possibility into account. This strategy is also more in line with intuition. As our testbed we use the AlgoViz Portal 1 which collects metadata on Algorithm Visualizations and provides community support. The features used for the personalization include long-term click behavior and topical classifications of the clicked results  , both similar to those shown to be effective in previous work on personaliza- tion 278. moviepilot provides its users with personalized movie recommendations based on their previous ratings. Out of the 264K extracted users  , we found that roughly 5000 1.9% profiles were no longer available  , likely deleted either by Quora or the user. WebKB 3 extracts instances of classes and relations based on web page contents and their linkage path. The ODP indexes a wide variety of websites in over 40 languages  , and all search engines have an equal chance of indexing it. From Fig- ure3  , one can see that number of lattice levels has a greater affect on the detection rate in the case of the KDDCup data set than in the other data sets. For SRAA dataset we learnt 10 topics on the complete dataset and labeled these 10 topics for all the three classification tasks. The dataset is the Billion Triple Challenge 2009 collection. Table 7 shows some examples of undeleted questions on Stack Overflow. We observe an increasing trend in the number of deleted questions on Stack Overflow over the last 2 years. The two most recent contextualization shared tasks are the Word Sense Disambiguation WSD tasks of SemEval 2010 20 and SemEval 2013 23. Warrick was also used to recover the WWW'06 conference website when a fire destroyed the building housing the web server 25. Therefore  , we denote it by F1 instead of " performance " for simplicity. " While approaches to recommend Stack Overflow discussions exist 32  , our aim is to determine whether the textual content of the video tutorial fragment can be used to retrieve relevant discussions . The dataset contained 476 abstracts  , which were divided into four research areas: Natural Language Processing NLP  , Robotics/Vision  , Systems  , and Theory. Table 9gives the numbers of directly and indirectly relevant documents. Finding a representative sample of websites is not trivial 14. Finally  , the userto-user social network attracts views  , and leverages social ties to encourage votes and additional high quality answers. This is because some of their related questions were not crawled questions deleted by Quora and thus are not included as nodes. The reason for this is that the performance of the neighbourhood and latent factor models was close to 0 7 . We now perform a temporal trend analysis of deleted questions on Stack Overflow. Analysis of the training queries and their corresponding qrel documents showed other discrepencies within gene symbols. Github is currently the most popular repository for open source code and its transparent environment implies a suitable basis for evaluating reuse and collaboration among developers 21. Each burst contains 10 new questions sent seconds apart  , and consistently produced 10 sequential qid's. Some of these queries have produced quite impressive results using the WT2g dataset and associated connectivity data. Again  , TSA performs substantially better than ESA  , confirming that temporal information is useful on other datasets. Such information can only be retrieved via simple keyword-based search  , unless the data is extracted and stored in a more structured form  , such as XML or relational tuples. The datasets used in Semeval-2015 are summarized in Table 1. We import Stack Overflow documents from the public data dump provided as a set of XML file 5 . Since the data is from many different semantic data sources  , it contains many different ontologies. Their work found that higher levels of joint memberships between Wikia communities was correlated with success. We extract a set of tourist attractions in the metadata of OpenStreetMap. The test queries include output tests  , selections  , joins  , projections  , aggregates  , and updates. We then give details on the key Quora graph structures that connect different components together. We believe that this is mainly because the number of alias symbols provided by the LocusLink database is overwhelming. However  , participants were free to use any of the other Blog06 collection components for retrieval such as the XML feeds and/or the HTML homepages. The results of our evaluation suggest that the context of sentences will play an important role when complementing API documentation with sentences from Stack Overflow. For our experiments we used preprocessed WebKB dataset 1 . Similarity ranking measures the relevance between a query and a document. However  , despite of the presence of question posting guidelines and an ebullient moderation community  , a significant percentage of questions on Stack Overflow are extremely poor in nature. Figure 8 and Figure 9show the experimental results for the two DSNs. The positive contribution of answers from blog documents to the various component scores was likely depressed due to the nature of the questions asked. The dataset as well as custom-built Ruby and R analysis tools are available on the Github repository gousiosg/pullreqs  , along with instructions on how to use them. From now on  , we refer to this encyclopedia as WPEDIA. The first 75% are selected as training documents and the rest are test documents. The naming regularities in LocusLink allowed us to design a simple set of rules and to extract 13 ,456 different genes grouped into 3 ,575 families/subfamilies/superfamilies. To our knowledge  , this is the first application of Percolation Theory in the quantification of propagation in Information Retrieval. Table 1 Zhu  , Kraut  , and Kittur 2014 examine community survival as a function of multiple memberships within Wikia communities. Thus  , we find English  , Chinese and Russian languages to be strongly represented as the location segmentation implies. the entire WT2g Dataset  , both for inLinks and outLinks. Some previous work has identified a certain fraction of splogs in these two datasets. We further refined the selection using the GitHub API to retrieve more detailed information about each repository with the following criteria: This selection included 185 ,342 repositories. For example  , see BLOG06-feed-000065  , BLOG06-feed-001152  , etc. The first evaluation was conducted in early 2007 and the results were reported at the SemEval-2007 workshop. One of Quora's core features is the ability to locate questions " related " to a given question. We evaluate LOADED 1 using the following real data sets 2 : a The KDDCup 1999 network intrusion detection data set with labels indicating attack type 32 continuous and 9 categorical 1 For all experiments unless otherwise noted  , we run LOADED with the following parameter settings: Frequen cyThreshold=10  , CorrelationThreshold=0.3  , AE Score=10  , ScoreWindowSize=40. Typically  , classification accuracies averaged over all the six classes are published with WebKB and are usually in the 70 − 90% range depending on the choice of features. Rather than attempt to get an unbiased sample  , we randomly sampled 500 URIs from the Open Directory Project dmoz.org. On the testing data set our approach is able to detect most of the unknown attacks a problem for almost all of the KDDCup 1999 participants . The second collection is the largest provided by the Wikia service  , Wookieepedia  , about the Starwars universe.