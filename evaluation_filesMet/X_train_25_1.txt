£ View matching must be integrated with cost-based plan enumeration. However  , there are a number of requirements that differ from the traditional materialized view context. To exploit statistics on views we can leverage existing system infrastructure built to support materialized views. When tuples are deleted from a view or a relation  , the effect must be propagated to all " higher-level " views defined on the view/relation undergoing the deletion. The traditional way of removing data from materialized views is deletion. First we illustrate the problem and its solution in the presence of hash indices or in the absence of indices on the materialized view. In this section  , we illustrate the split group duplicate problem that arises if we ignore this subtle difference between materialized view maintenance and the " traditional " associative/commutative update problems studied by Korth Kor83 and others. Thus  , for materialized views  , it may be adequate to limit support to a subclass of common operations where view substitution has a large query execution payoff. In the context of traditional materialized views  , maximum benefit is obtained when the view stores a " small " result obtained by an " expensive " computation  , as it is the case with aggregates . However  , our method utilizes a set of special properties of empty result sets and is different from the traditional method of using materialized views to answer queries. Our fast detection method for empty-result queries uses some data structure similar to materialized views − each atomic query part stored in the collection C aqp can be regarded as a " mini " materialized view. In the sequel all derived relations are assumed to be materialized  , unless stated otherwise. A derived relation may be virtual  , which corresponds to the traditional concept of a view  , or materialized  , meaning that the relation resulting from evaluating the expression over the current database instance is actually stored. Thus  , an important question originally considered in TB88  , Hu96   , which was never raised in traditional view-maintenance work  , is to determine whether a view is maintainable  , that is  , guaranteed to have a unique new state  , given an update to the base relations   , an instance of the views  , and an instance of a subset of the base relations. Such situations never arise in traditional work on materialized view maintenance GM95  , Kuc91  , GMS93  , SJ96 where all the base data is usually assumed to be available . A derived relation is defined by a relational expression query over the base relations. For example  , consider the following two queries: In general  , the design philosophy of our method is to achieve a reasonable balance between efficiency and detection capability. Hence  , in certain cases  , the coverage detection capability of our method is more powerful than that of the traditional materialized view method. After enough information about previously-executed  , empty-result queries has been accumulated in C aqp   , our method can often successfully detect empty-result queries and avoid the expensive query execution. 5 Due to the utilization of a set of special properties of empty result sets  , its coverage detection capability is often more powerful than that of a traditional materialized view method. While view materialization is well understood for traditional relational databases  , it remains an active research for XML and RDF stores. This brings forth a need for a simple way of describing and extracting a relevant subset of information materialized views over large RDF stores. In deciding whether a query will return an empty result set  , our method ignores those operators e.g. , projection  , duplicate elimination that have no influence on the emptiness of the query output. As a result of not using all the base relations  , there may be situations where there is not enough information to maintain a view unambiguously  , even if we are given the specific contents of the views  , a subset of the base relations  , and the base update. DBMSs are being used more and more for interactive exploration 7  , 14  , 37  , where users keep refining queries based on previous query results. Fourth  , our method utilizes a set of special properties of empty result sets so that its coverage detection capability is often more powerful than that of the traditional materialized view method e.g. , if πR=∅  , we know immediately that R=∅  , σR=∅  , and R ⋈ Recall that the problem is that for the V lock to work correctly  , updates must be classified a priori into those that update a field in an existing tuple and those that create a new tuple or delete an existing tuple  , which cannot be done in the view update scenario. In order to use support vector machine  , kernel function should be defined. Mathematical details of support vector machine can be found in 16J. During testing phase  , the texture fea­ ture extracted from the image will be classified by the support vector machine. During learning phase  , the support vector machine will be trained to learn the edge and non­ edge pattern. In the faceted distillation task  , we use the support vector machine to evaluate the extent to which a blog post is opinionated. Support vector machine has been proven to be an efficient classifier in text mining 1 . special effects. As expected  , the Support Vector Machine was the most robust method  , also with respect to outliers  , i.e. 36 train a support vector machine to extract mathematical expressions and their natural language phrase. Yokoi et al. Section 3 addresses the concept and importance of transductive inference  , together with the review of a well-known transductive support vector machine provided by T. Joachims. Section 2 offers a brief introduction to the theory of support vector classification. SV M struct generalizes multi-class Support Vector Machine learning to complex data with features extracted from both inputs and outputs. More recently  , a maximum margin method known as Struct Support Vector Machine SV M struct  19 was proposed to solve this problem. A more general definition of a pattern can involve mixed node types within one pattern  , but is beyond the scope of this paper. For example  , a pattern of a 'term' type is a set of unigrams that make up a phrase  , such as {support  , vector  , machine} or 'support vector machine' for simpler notation. Probabilistic graphical models can further be grouped into generative models and discriminative models. SV M struct is one of the support vector machine implementations for sequence labeling 16. Consider a two class classification problem. Support Vector Machine is well known for its generalization performance and ability in handling high dimension data. As in 7  , quarterly data were the most stable ones. The final generalization of the Support Vector Machine is to the nonseparable case. For more information on this approach see 7  , 6  , and 22. For support vector machine  , the polynomial kernel with degree 3 was used. The estimated values were: 60 Allele  , 40 Expression  , 25 Gene Ontology and 25 Tumor. It is based on structural risk minimization principle from computational learning theory. Support vector machine is a model of binary classifier 6. In the second set of experiments  , we use transductive support vector machine for model training. The other sets of experiments are designed similar to the first set. We show that the proposed general framework has a close relationship with the Pairwise Support Vector Machine. With L = W   , we can have: used six electrodes mounted on target muscles and a support vector machine was employed as a classifier 2. Wang et al. Maximizing the margin enhances the generalization capability of a support vector machine 16. The quadratic term in 1 maximizes the distance or " margin " between the bounding planes. Note  , that this phrase also includes function words  , etc. While classifiers differ  , we believe our results enable qualitative conclusions about the machine predictability of tags for state of the art text classifiers. Predictability " is approximated by the predictive power of a support vector machine. It was able to orient our test images with modest accuracy  , but its performance was insufficient to break the captcha. We tested the viability of machine learning attacks by implementing a support vector machine. Furthermore  , a method for utilising the HSS as the basis for Support-Vector Machine person recognition was detailed. A method for constructing the HSS  , a scale and viewing angle robust feature vector that encapsulates these interperson variations  , was presented. A large majority of them are either provably or potentially unstable. The support state of a walking machine is a binary row vector  , whose com onents are the support states of its individual legs 4f There are in all 26 or 64 possible support states for a six-legged machine. It assumes a value of 1 if the leg is on the ground and 0 otherwise. It is organized as follows: Section 2 presents the question classification problem; Section 3 compares several machine learning approaches to question classification with conventional surface text features; Section 4 describes a special kernel function called tree kernel to enable the Support Vector Machines to take advantage of the syntactic structures of questions; Section 5 is the related work; and Section 6 concludes the paper. This paper presents our research work on automatic question classification through machine learning approaches  , especially the Support Vector Machines. One binary support vector machine is trained for each unordered pair of classes on the training document set resulting in m*m-1/2 support vector machines. This time  , however  , only the first primary descriptor assigned to the document was used  , assuming that this is the most important descriptor for the respective document. We detect the name entities using a support vector machine-based classifier 13  , and use the tagged Brown corpus 1 as training examples to train the classifier. : which include names of people  , organizations   , locations  , etc. By adding virtual relevant documents generated by transformation of original documents to training set  , we could improve performance significantly. Support vector machine was used to learn from the artificially enlarged training documents. We also show results that demonstrate the advantages of our approach over support vector machine based models. This causes a significant improvement in the classification performance  , especially when path and non-path have similar color features. Machine learning methods such as support vector machines were usually employed in the classification. In 12  , 14  , 22  , 26  , queries were classified according to users' search needs  , for instance  , topic distillation  , named page finding  , and homepage finding. A support vector machine was trained on the first three quarters of the data and tested on the unused data. The window around a boredom event was classified as 30 frames prior to the boredom rating and 90 frames after. We tried training a support vector machine to predict the category labels of the snippets. Many snippets neither indicate similarity nor difference  , but merely mention a pair of products  , for example asking how they compare. According to this strategy  , fields in records are encoded using feature vectors that are used to train a binary support vector machine classifier. In 3   , a learning strategy is used for determining similarity between records. Experiment results show that our new idea on the feature is successful at least in this field. We still use Support Vector Machine  , a common  , simple yet powerful tool  , as the classifier. The approach taken was to train a support vector machine based upon textual features using active learning. The Melbourne team was a collaboration of the University of Melbourne  , RMIT University   , and the Victorian Society for Computers and the Law. Teo and Vishwanathan proposed fast and space efficient string kernels based on SAs and used the kernel with the support vector machine 33. Some studies that use suffix arrays SAs for document classification have been proposed. However  , query classification was not extensively applied to query dependent ranking  , probably due to the difficulty of the query classification problem. However  , they assume that the features depend only on the input sequence and are independent of the output tag sequence. 15  proposes a multi-Criteria-based active learning for the problem of named entity recognition using Support Vector Machine. We report results as averages across all EC classes in We performed " one-class vs. rest " Support Vector Machine classification and repeated this for all six EC top level classes. This section presents the core of CSurf's Context Analyzer module  , that drives contextual browsing. Then  , a support vector machine 32 is used to compute the relevance score of these sections 2 Note  , this is different from HTML frames. They formalized the problem as that of classification and employed Support Vector Machines as the classifier. 10 proposed a machine learning based method to conduct extraction from research papers. Three runs were conducted  , one based on nouns  , one based on stylometric properties  , and one based on punctuation statistics. The resulting blogs were classified using a Support Vector Machine trained on a manually labelled subset of the TREC Blogs08 dataset. Georeferencing has not only been applied to images or videos. A failure here results in the exploitation of visual features which are used as input to a support-vector machine based classifier. 8 provides some initial answers to these questions  , but does not address predictability directly  , nor does it look specifically at anchor text. " Many classifiers can be used with kernels  , we use Support Vector Machine. We define and combine two different kernel functions that calculate the pairwise similarity between sentences bag-of-words and verb. We compare the results obtained using the kernel functions defined in Sect. The trade-off parameter c of the Support Vector Machine learning was set to 1 in all experiments. Because the task is a binary classification personal or organizational   , a support vector machine was used Chang and Lin 2011. Datasets for both evaluations were constructed to be the same size in order to make the results comparable. The method was tested in the domain of robot localization. The outputs are then used as input to a Support Vector Machine  , that combines optimally the different cue contributions. The whole system consists of three major compo­ nents  , namely texture feature extractor  , texture clas­ sifier and boundary detector. The feature will be put into the support vector machine and the associated da.% will be reported. During testj'lg phase  , the texture feature of testing im­ age will be extmcted. 9  also describes a classification of outliers using a ball  , as a special case of One-class classification . One-class classification 9  transfers the problem of detecting outliers to a quadratic program solved by Support Vector Machine. We will use support vector machine classification and term-based representations of comments to automatically categorize comments as likely to obtain a high overall rating or not. Can we predict community acceptance ? Then  , titles from the same PDFs were extracted with a Support Vector Machine from Cite- Seer 1 to compare results. In an experiment  , titles of 1000 PDF files were extracted with SciPlore Xtract. Our dataset PDFs  , software  , results is available upon request so that other researchers can evaluate our heuristics and do further research. Surprisingly  , this simple rule based heuristic performs better than a Support Vector Machine based approach. If no location is found  , PLSA 10 is performed on the tag data of the corpus. The support vector machine then learns the hyperplane that separates the positive and negative training instances with the highest margin. These training instances are represented in terms of their transformed feature vectors in the kernel space. This run used a support vector machine built from the normal features in Table 5to retrieve documents using a hybrid representation. Overlap in passages were removed and the lists were trimmed to the top 1000 re- sults. Our official submission  , however  , was based on the reduced document model in which text between certain tags was indexed. We also studied query independent features on an Support Vector Machine classifier. Support Vector Machine based text categorization 8  is adopted to automatically classify a textual document into a set of predefined hierarchy that consists of more than 1k categories. We further introduce probabilistic model to describe latent semantics. 18  propose three margin based methods in Support Vector Machine to select examples for querying which reduce the version space as much as possible. use entropy based methods 7 to select unlabeled examples for the application of image retrieval. The emotional state annotations are derived through a framework based on a Multi-layer Support Vector Machine ap- proach 18. – automatic audio annotations coming from emotional states recognition for example fear  , neutral  , anger. Once we have computed the distance for each field of the record pair  , we use a support vector machine to determine the overall goodness of the match. Creating this distance metric is the focus of this paper. We then train a two-class support vector machine with the labelled feature vectors. We form such feature vectors for all synonymous word-pairs positive training examples as well as for non-synonymous word-pairs negative training examples. The shallow semantic parser we use is the ASSERT parser  , which is trained on the PropBank Kingsbury et al. , 2002 corpus and uses support vector machine classifiers. This goal is achieved by performing shallow semantic parsing. PropBank was manually annotated with verbargument structures. The confidence of the learned classifier is then used as a similarity metric for the records. Surprisingly  , our simple rule based heuristic performed better than a support vector machine. Our tests showed 1 that style information such as font size is suitable in many cases to extract titles from PDF files in our experiment in 77.9%. As already mentioned  , a VAD system tries to determine when a verbalization starts and when it ends. Then  , the signal is classified as voice or unvoice using a Support Vector Machine classifier. Chen Chen et al. , 2010  , by means of the Wavelet Transform  , obtains the audio signal in the time-frequency domain. In general our contiguous support vector machine is more  sitive and more specific. Based on the experiments described in this article we conclude that our automatic approach to the classification of images performs at least as well as human observers. One would need more data  , especially of control subjects to be able to state that automatic methods always significantly outperform human observers in clinical practice. For a normally distributed variable  , outliers are objects with Mahalanobis distance above a given threshold. However  , there are only a few papers describing machine learning approaches to question classification  , and some of them such as 17 are pessimistic. In this year's task  , we made a thorough modification to our classification system: a new type of feature  , which can contain more semantic information  , is proposed  , and to generate this feature  , a new recursive incremental machine learning method is employed. For example  , an article on Support Vector Machines might not mention the words machine learning explicitly  , since it is a specialized topic in the field of machine learning. Furthermore  , documents with high path lengths are more specialized and thus tend to use a more specialized vocabulary. The table that follows summarises generalization performance percentage of correct predictions on test sets of the Balancing Board Machine BBM on 6 standard benchmarking data sets from the UCI Repository  , comparing results for illustrative purposes with equivalent hard margin support vector machines. We remove repeated occurrences of the same input vector and assign the most common label for this input vector to the occurrence that we leave in the training set. Results of a systematic and large-scale evaluation on our YouTube dataset show promising results  , and demonstrate the viability of our approach. Two sources of relevance annotations were used for different runs: the official annotations   , provided by the topic authorities; and annotations provided by a member of the Melbourne team with e-discovery experience though not legal training. Summarized  , despite the issue that many PDFs could not be converted  , the rule based heuristic we introduced in this paper  , delivers good results in extracting titles from scientific PDFs 77.9% accuracy. Since the appearance of microarray technology in to­ day's biological experiment  , gene expression data gen­ erated by various microarray experiments have in­ creased enormously  , and lots of works based on these data have been published. Guyon et at 10 used Support Vector Machine methods with Recursive Fea­ ture Elimination RFE for gene selection to achieve better classification performance. The underlying distribution of the unlabeled data is also investigated to choose the most representative examples 10. In the framework of Support Vector Machine18  , three methods have been proposed to measure the uncertainty of simple data  , which are referred as simple margin  , MaxMin margin and ratio margin. From the previous work on active learning 7 18  , measurement of uncertainty has played an important role in selecting the most valuable examples from a pool of unlabeled data. Most research are focused on analyzing microarray gene expression either to determine significant pathways that contribute to a phenotype of interest or deal with features genes selection problem. Their method was compared with five feature selection methods using two classifiers: K-nearest neighbour and support vector machine and it preformed the best for three microarray datasets. Pang and Lee found that using the Support Vector Machine classifier with unigrams and feature presence resulted in a threefold classification accuracy of 83%; therefore we also follow this strategy and use unigrams and only take into account feature presence. This corpus is mined from the Internet Movie Database archive of the rec.arts.moviews.reviews newsgroup. We used an opinionated lexicon consisting of 389 words  , which is a subset complied from the MPQA subjective lexicon 11. The well-known kernel trick is difficult to be applied to 9  , while kernel trick is considered as one of the main benefits of the traditional support vector machine. Note that by exploring the low rank property  , the optimization problem is not convex. 2005   , who show that explicit feature mapping is preferable to implicit feature mapping using   , for example  , suffix trees for support vector machine training and classification of strings  , when using small k-mers. This approach is similar to that recommended by Sonnenburg et al. The knowcenter group classified the topic-relevant blogs using a Support Vector Machine trained on a manually labelled subset of the TREC Blogs08 dataset. In the second stage  , for the identification of the facet inclination of a given feed  , the IowaS group used sentiment classifiers and various heuristics for ranking posts according to each facet. 11 selected strongly correlated genes for accurate disease classification by using pathways as prior knowledge. Previous methods summarized above can only be used to select one element in the sequence which can not be labeled without context information. One of the most well-known approaches within this group is support vector machine active learning developed by Tong and Koller 31. Another group of approaches measure the classification uncertainty of a test example by how far the example is away from the classification boundary i.e. , classification margin 4  , 24  , 31. After doing so  , we can produce a probabilistic spatiotemporal model of an event. We prepare the training data and devise a classifier using a support vector machine based on features such as keywords in a tweet  , the number of words  , and the context of target-event words. This work was extended to assign features to each of the regions such as spatial features  , number of images  , sizes  , links  , form info  , etc that were then fed into a Support Vector Machine to assign an importance measurement to them. Each region is assigned a degree of coherence that is based on visual properties of the region including fonts  , colors and size. Support Vector Machine is trained to produce initial group suggestion as the baseline. Four popular visual descriptors  , tiny image  , color histogram  , GIST 6  , and CEDD 7  , and topic representation of user annotations 8 are extracted to represent the images in compact feature space. Three experiments were conducted  , one based on nouns  , one based on stylometric properties  , and one based on punctuation statistics. From the top 2500 result blog entries  , the top 100 blogs were identified according to the accumulated relevance score of the particular blog entries. A central goal of the music information retrieval community is to create systems that efficiently store and retrieve songs from large databases of musical content 7. Second  , they take a one-vs-all approach and learn a discriminative classifier a support vector machine or a regularized least-squares classifier for each term in the Basically  , Support Vector Machine aim at searching for a hyperplane that separates the positive data points and the negative data points with maximum margin. We conducted experiments with the following additional multi-class classification approaches see 21  for more information about the methods: 32 have shown superb performance in binary classification tasks. A support vector machine classifier is able to achieve an identification accuracy of over 88% using either the full force profile over the insertion or through the section of perceive work and stiffness metrics. Insertions into a plastic cochlea model have produced similar insertion forces and allowed us to identify cases of tip folding during PEA insertion. In the proposed system  , the bi will be the texturc feature set {3 i  ,i'} after texture extraction on the in­ put image and {+ 1  , -I} refers to edge and non-edge classes. When the sequence length t is large  , the huge number of classes makes the multi-class Support Vector Machine infeasible. with t elements and |D| possible tags for each element y i   , i = 1  , · · ·   , t  , the possible number of classes is |D| t . Simple margin measures the uncertainty of an simple example x by its distance to the hyperplane w calculated as: In the framework of Support Vector Machine18  , three methods have been proposed to measure the uncertainty of simple data  , which are referred as simple margin  , MaxMin margin and ratio margin. Similar to regular Support Vector Machine  , a straightforward way to which is based on the negative value of the prediction score given by formula 10. Given a pool of unlabeled sequences  , U = {s 1   , s 2   , ..  , s m }  , the goal of active learning in sequence labeling is to select the most valuable sequences from the pool. Additionally  , we could show that it is possible to precisely predict the action  , by using a Support Vector Machine. Furthermore we could show that it is possible to predict the expected action based on our spatial features whereby we found that the distance measures are the most influential values. In reducing total prediction error MNSE and AME polynomial kernel produced the best result while in predicting trend DS  , CU and CD radial basis and polynomial kernel produced equally good results. This paper investigates the performance of support vector machine for Australian forex forecasting in terms of kernel type and sensitivity of free parameters selection. Using a support vector machine with normalized quadratic kernel and an all-pairs method  , this yields an accuracy of 67.9%. To obtain an upper bound  , we classify the documents directly using bag-of-words features from the text  , which should perform better than transforming the text into a visualization. The importance measurement was used to order the display of regions for single column display. In addition  , we present a new tensor model that not only incorporates the domain knowledge but also well estimates the missing data and avoids noises to properly handle multi-source data. Our framework is built upon support vector machine  , which has been widely used to analyze OSNs in many areas 11  , 12  , such as business  , transportation  , and anomaly intrusion detection . For the second step  , we employ a support vector machine as our classifier model. If the copy sent to the crawler contains more than a threshold of links that don't exist in the copy sent to the browser  , we mark it as a candidate and send it to the second step. The selection of which method to use may depend on the implementation hardware as each provides similar statistical performance. Second  , they take a one-vs-all approach and learn a discriminative classifier a support vector machine or a regularized least-squares classifier for each term in the First  , they use a set of web-documents associated with an artist whereas we use multiple song-specific annotations for each song in our corpus. It is clear that popularity of topics vary over time  , new topics emerge and some topics cease to exist. Another interesting fact to note is that Support Vector Machine is virtually non-existent in the collection until 1997  , according to ACM repository. The classifier was trained on the Blog06 text collection first  , and then applied to the posts in the Blog08 text collection to estimate the probability of each post being relevant to the query. Note that the features in sequence labeling not only depend on the input sequence s  , but also depends on the output y. In the following section  , we describe how the distance metric F i is learned. Due to its popularity and success in the previous studies  , it is used as the baseline approach in our study. We used synonymous word pairs extracted from Word- Net synsets as positive training examples and automatically generated non-synonymous word pairs as negative training examples to train a two-class support vector machine in section 3.4. Therefore  , we can conclude that 2500 examples are sufficient to leverage the proposed semantic similarity measure. We present an approach where potential target mentions of an SE are ranked using supervised machine learning Support Vector Machines where the main features are the syntactic configurations typed dependency paths connecting the SE and the mention. The focus of our paper is on the problem of linking sentiment expressions to the mentions they target. Borrowing from past studies on demographic inference   , three types of features were used for distinguishing between account types: 1 post content features  , 2 stylistic features  , how the information is presented  , and 3 structural and behavioral features based on how the account interacts with others. In the third set of experiments   , we apply our framework in the same manner as the first set  , except that the unformatted text block detection component is not used. Once the name entities are detected  , we compute their occurrence frequencies within the document corpus  , and discard those name entities which have very low occurrence values. LIF and LIB*TF  , which have an emphasis on term frequency  , achieved significantly better recall scores. The proposed methods LIB  , LIB+LIF  , and LIB*LIF all outperformed TF*IDF in terms of purity  , rand index  , and precision. Overall  , LIB*LIF had a strong performance across the data collections. While LIB and LIB+LIF did well in terms of rand index  , LIF and LIB*TF were competitive in recall. When an application initializes Comm- Lib  , it automatically initiates an instance of ServiceX. Lib instances. Methods with the LIB quantity  , especially LIB  , LIB+LIF  , and LIB*LIF  , were effective when the evaluation emphasis was on within-cluster internal accuracy  , e.g. , in terms of purity and precision. Similar to IDF  , LIB was designed to weight terms according to their discriminative powers or specificity in terms of Sparck Jones 15. This is very consistent with WebKB and RCV1 results . Compared to TF*IDF  , LIB*LIF  , LIB+LIF  , and LIB performed significantly better in purity  , rand index  , and precision whereas LIF and LIB*TF achieved significantly better scores in recall. As shown in Table 4  , the proposed methods outperformed TF*IDF in terms of multiple metrics. Lib exposes a public API  , createSocket  , which constructs Socket objects on behalf of its clients. Sample Code Figure 1shows the Java code of two library classes  , Lib and Priv  , and two client classes  , Enterprise and School. LIB+LIF: To weight a term  , we simply add LIB and LIF together by treating them as two separate pieces of information. Hence we propose three fusion methods to combine the two quantities by addition and multiplication: 1. By modeling binary term occurrences in a document vs. in any random document from the collection  , LIB integrates the document frequency DF component in the quantity. The LIB*LIF scheme is similar in spirit to TF*IDF. Working versions are contained in libraries whose names consist of Xlib   , and the corresponding systems versions are found in <lib . The working version belongs therefore to the programmer private  , who is capable of modifying it unprotected . SPL-programs for example are found in the libraries XSPL and SPL. The application runs from the command line. All D-Lib articles are written in HTML. Daikon 4.6.4 is an invariant generator http://pag.csail.mit.edu/daikon/. com/p/plume-lib/  , downloaded on Feb. 3  , 2010. To prevent its clients now on the stack from requiring the relevant FilePermission—which a maliciously crafted client could misuse to erase the contents Classes Permissions Enterprise School Lib Priv java.net. SocketPermission "ibm.com"  , "resolve" java.net. SocketPermission "ibm.com:80"  , "connect" java.net. SocketPermission "vt.edu"  , "resolve" java.net. SocketPermission "vt.edu:80"  , "connect" java.io. FilePermission "C:/log.txt"  , "write" Upon constructing a Socket  , Lib logs the operation to a file. LIB is similar in spirit to IDF and its value represents the discriminative power of the term when it appears in a document. The larger the LIB  , the more information the term contributes to the document and should be weighted more heavily in the document representation . By emphasizing the discriminative power specificity of a term  , LIB reduces weights of terms commonly shared by unrelated documents  , leading to fewer of these documents being grouped together smaller false positive and higher precision. Experiments showed that methods with the LIB quantity were more effective in terms of within-cluster accuracy e.g. , precision and purity. The first Col/Lib and second Loc columns give information about the name of the collection and their location. Subsequently  , each block is sorted according to geographical location second column  , value: Loc  , and finally  , the collections or the libraries first column  , value: Col/Lib are ordered alphabetically for each geographical location. The evaluation results are presented in Table 3. The approach is evaluated on four open source applica- tions: Neuroph  , WURFL  , Joda-Time  , and Json-lib. For evaluation purposes  , we selected a random set of 70 D-Lib papers. The average reference accuracy is the average over all the references. The above equation gives the amount of information a term conveys in a document regardless of its semantic direction . Hence  , LI Binary LIB can be computed by: We used the reference linking API to analyze D-Lib articles. The second example gathers and stores reference linking information for future use. Plume is a library of utility programs and data structures http://code.google. Avatar assistant robot  , which can be controlled remotely by a native teacher  , animates the 3D face model with facial expression and lib-sync for remote user's voice. 1. With the NY Times corpus  , LIB*LIF continued to dominate best scores and performed significantly better than TF*IDF in terms of purity  , rand index  , and precision Table 5. The other methods such as LIF and LIB*TF emphasize term frequency in each document and  , with the ability to associate one document to another by assigning term weights in a less discriminative manner  , were able to achieve better recalls. A possible reason is that many of the interclass invocations are associated with APIs whose parameters are often named more carefully. Querying Google with the LS returns 11 documents  , none of which is the DLI2 homepage. The third LS is taken from Wilensky's and Phelps article in D-Lib Magazine from July 2000 11. have been generated based on keyword and document semantic proximities 7. As a demonstration of the viability of the proposed methodology  , SKSs for a number of communities the Los Alamos National Laboratory's LANL Research Library http://lib-www.lanl.gov/. Annotations made in the reader are automatically stored in the same Up- Lib repository that stores the image and text projections. This reader provides thumbnail overviews  , freehand pen annotations  , highlighting  , text sticky notes  , bookmarks  , and full text keyword search. Additionally  , we use the keyboard to allow for the entrance of data. The display may be used in text mode or graphics mode by direct access to video memory by using SVGA-lib. The first column contains the collection names from ten university libraries. The default resolution of symbols is to routines in the library itself. For instance  , calling routine f of library lib is done by explicitly opening the library and looking up the appropriate routine: For thrift-lib-w2-5t  , although HaPSet checked 14 runs  , it actually spent more time than what DPOR spent on checking 23 runs. Second  , the monitoring and control of memoryaccessing events often have large overhead. These environments are dominated by issues of software construction. Policies take the form of conventions for organizing structures as for example in UNIX  , the bin  , include  , lib and src directories and for ordering the sequence of l While we have demonstrated superior effectiveness of the proposed methods  , the main contribution is not about improvement over TF*IDF. In most experiments  , the proposed methods  , especially LIB*LIF fusion   , significantly outperformed TF*IDF in terms of several evaluation metrics. The DMG-Lib concept and workflow takes into account that technical knowledge exists in different forms e.g. These functional models are digitized and available as videos and interactive animations. In addition  , whereas KL is infinite given extreme probabilities e.g. , for rare terms  , the amount of least information is bounded by the number of inferences. Experiments on several benchmark collections showed very strong per-formances of LIT-based term weighting schemes. These animations are augmenting original figures and can be displayed in the e-book pages with an integrated Java Applet. To better understand the motion of figured mechanisms and machines DMG-Lib can animate selected figures within e-books. One for the flight vehicle information such as predicted pose and velocity provided by the INS  , RPM data and air speed data  , while the other bus handles the DDF information. At run time  , the two clients will require SocketPermissions to resolve the names and connect to ports 80 of hosts ibm.com and vt.edu  , respectively. where ni is the document frequency of term ti and N is the total number of documents. The two are related quantities with different focuses. While LIB uses binary term occurrence to estimate least information a document carries in the term  , LIF measures the amount of least information based on term frequency. Here thrift-lib-w2-5t  , for example  , stands for the test case with 2 worker threads and 5 tasks per worker. The first four columns show the name  , the lines of code  , the number of threads  , and the bug type. This scanner then adds supported document types that it finds to a specified instance of an Up- Lib repository. UC also includes a utility to scan a portion of the file system specified by the user. texts  , pictures and physical models see Figure1 and requires analytical  , graphical and physical forms of representation. lib " represents the library from which the manuscript contained in the image originates and can be one of eight labels: i AC -The Allan and Maria Myers Academic Centre  , University of Melbourne  , Australia. fol " .tif. " A limitation of the case studies is that all the applications and components used were software developed by ABB Inc. involving .lib library files. The second author then revealed the actual changes and the black-box testing results. Our first corpus contained the complete runs of the ACM International Conference on Digital Libraries and the JCDL conference  , and the complete run of D-Lib Magazine see Table  2. We selected two corpora to work from. Hence  , it helped improve precision-oriented effectiveness. {10} {1 ,2 ,7 ,10}{1 ,2 ,3 ,7 ,8 ,10} {1 ,2 ,3 ,4 ,7 ,8 ,92 ,3 ,4 ,5 ,7 ,8 ,9 ,11}{1 ,2 ,3 ,4 ,5 ,6 ,7 ,8 ,9 ,11} Description ,Library {9} {4 ,6 ,9} {1 ,2 ,3 ,4 ,6 ,7 ,8 ,9 ,11}{1 ,2 ,3 ,4 ,5 ,6 ,7 ,8 ,9 ,11} Even if the indexing phase is correct  , certain documents may not have been indexed under all the conditions that could apply to them. Lib. Typically text documents in the field of mechanisms and machine science are containing many figures. For instance  , calling routine f of library lib is done by explicitly opening the library and looking up the appropriate routine: The reference can be obtained using the library pathname. In the CLR  , the privilege-asserting API is Assert. of the file or log false information in it—Lib creates an instance of Priv and passes it to doPrivileged  , the Java privilege-asserting API 6  , which modifies the stack-inspection mechanism as follows: at run time  , doPrivileged invokes the run method of that Priv object  , and when the stack inspection is performed to verify that each caller on the stack has been granted the necessary FilePermission  , the stack walk recognizes the presence of doPrivileged and stops at createSocket  , without demanding the FilePermission of the clients of Lib. The solution presented in this paper addresses these concerns. These 690 requests were targeting 30 of our 541 monitored shells  , showing that not all homephoning shells will eventually be accessed by attackers. As mentioned earlier  , since these URLs  , e.g. , www.banking.com/img/lib/shell3.php  , were never made public   , anyone who knows them  , must know them because a shell  , either through client-side  , or server-side homephoning   , leaked its precise URL to an attacker. The remaining columns show the performance of each method  , including the number of interleavings tested and the run time in seconds. Policies take the form of conventions for organizing structures as for example in UNIX  , the bin  , include  , lib and src directories and for ordering the sequence of l The mechanisms communicate with each other by a simple structure  , the file system. More than 3800 text documents  , 1200 descriptions of mechanisms and machines  , 540 videos and animations and 180 biographies of people in the domain of mechanism and machine science are available in the DMG- Lib in January 2009 and the collection is still growing. The Digital Mechanism and Gear Library is a heterogeneous digital library with regard to the resources and media types. LIF  , on the other hand  , models term frequency/probability distributions and can be seen as a new approach to TF normalization . Library means that the library has created its own digitized or born-digital material. The fourth column A-m shows the acquisition method of the material  , which has five values: library Lib  , third-party T-p  , license Lic  , purchase Pur and voluntary deposit V-d. In order to evaluate the effectiveness of the proposed control method for the exoskeleton  , upper-lib motion assist bower assist experiment has be& carried out with tbree healthy human subjects Subject A and B are 22 years old males  , Subject C is 23 years old male. . Segmentation of the gait cycle based on the lib-terrain interaction isolates portions of the gait bounce signal with high information content. Different limb-terrain interactions generate 222 gait bounce signals with different information content  , thus deliberate limb motions can effect higher information content. The experimental setup is shown in Fig. There are three blocks or categories: digitized value: Dig  , digitized and born-digital value: Dig  , B-d  , and born-digital value: B-d. Library and owners can appear as value Lib  , Own  , if both the library and the owners require written permission. Case-by-case means that the written permission is examined on case-by-case basis and N/A means that it is not applicable. This is because not all these 14 runs are included in the 23 runs; and each run may execute a different set of statements and therefore may take a different amount of time. 12 Although the most recent version of the application profile  , from September 2004 13  , retains the prohibition on role refinement of <dc:creator>  , the efforts the DC- Lib group made to find some mechanism for communicating this information supports the view that role qualification is considered important. They are not included in the application profile  , awaiting approval by DCMI of a mechanism to express these. " In light of TF*IDF  , we reason that combining the two will potentiate each quantity's strength for term weighting. As discussed  , the LIB quantity is similar in spirit to IDF inverse document frequency whereas LIF can be seen as a means to normalize TF term frequency. Whereas LIF well supported recall  , LIB*LIF was overall the best method in the experiments and consistently outperformed TF*IDF by a significant margin  , particularly in terms of purity  , precision  , and rand index. In all experiments on the four benchmark collections  , top mance scores were achieved among the proposed methods. Unfortunately  , this effort has not been continued. Related to this effort  , the D-Lib Working Group on Digital Library Metrics 2 was formed and was involved in the organisation of a workshop 3 in 1998  , which addressed several aspects of DL evaluation. We have implemented the lazy  , schedule recording  , and UW approaches described in Section 3 in our ESBMC tool that supports the SMT logics QF AUFBV and QF AUFLIRA as specified in the SMT-LIB 27. In our experiments  , we chose CHESS v0.1.30626.0 21 and SATABS v2.5 6 as two of the most widely used verification tools. Not surprisingly  , there was very little consistency among data providers on the syntax of role pseudo-qualifiers. where the conflict rate is most significant. This can be considered as 100 lockable objects in the LIB-system  , or alternatively  , these 100 objects can be regarded as the highly active part of the CB-system catalog data  , access path data  , . This allows the user to fluidly read and annotate documents without having to manage annotated files or explicitly save changes. Library means that the copyright of the material is owned by the organization that the library belongs to  , and is administered by the library. The fifth column C-o presents the copyright owner  , which has five values: library Lib  , individual Ind  , organization Org  , vary and public domain P-d. Since NCSTRL+ can access other Dienst collections we can extend searches to all of NCSTRL  , CoRR  , and D-Lib Magazine as well. The tools have been used to create a testbed for NCSTRL+ which  , at this time  , runs on three NCSTRL+ servers with index service for five archives. In this section  , we show how to conclude the construction of M Imp by incorporating the assumption PAs into M Exp . Let g i be the guard obtained from g i by replacing every parameter of lib by the corresponding argument passed to it at c. In each set of experiments presented here  , best scores in each metric are highlighted in bold whereas italic values are those better than TF*IDF baseline scores. The search result for a single query from the ad-hoc task is a list of structured data; each contains a web TREC-ID and the extracted main body of content. At last  , we stem the words on the content using a tool called lib-stemmer library 1 . The NCSTRL+ DL interface is based on our extensions to the Dienst protocol to provide a testbed for experimentation with buckets  , clusters  , and interoperability. Some general rules for the handling of digitized and born-digital material can be derived from Table 1and its discussion  , showing that there is a variety of arrangements depending on ownership of the material and its copyright. If the value library  , owners Lib  , Own appears  , the fee should be paid to both library and owners. The above described methodology relies critically on our ability to generate a population of agents that share a SKS. The multimedia collection consists of e-books  , pictures  , videos and animations. -PAR 1 is set to maxobj = 100. It is useful to think of these segments as motion primitives  , which are typically defined in relation to terrain interaction.  Retrieve and apply updates for synchronization: updates can also be represented using in-memory objects  , files and tables. The default implementation of these methods assumes that there is no immutable data  , and that the public mutable data consists of the entire Web archive WAR file of the replicable service application except those under WEB-INF/classes and WEB- INF/lib  , while the private mutable data consists of the HTTPSession object created for the client. Stack inspection is intended to prevent confused-deputy attacks 9  , which arise when a component C 1 that was not granted access to a resource r obtains access to r indirectly  , by calling into a component C 2 that was granted access to r. Figure 1. We derive two basic quanti-ties  , namely LI Binary LIB and LI Frequency LIF  , which can be used separately or combined to represent documents. By quantifying the amount of information required to explain probability distribution changes  , the proposed least information theory LIT establishes a new basic information quantity and provides insight into how terms can be weighted based on their probability distributions in documents vs. in the collection. Information on the data structure  , functions  , and function calling relationships of the source code is stored in the binary files according to pre-defined formats  , such as Common Object File Format COFF 5 33  , so that an external system is able to find and call the functions in the corresponding code sections. Prior to distribution  , component source code is compiled into binary code formats  , such as .lib  , .dll  , or .class files. These test beds comprise different media; however  , since the focus of most the projects spawning off the test beds was on technological aspects  , users and usage as well as the content play a minor role in most of these test beds. Connecting attackers: During the eight weeks of our honeypot experiment  , we received 690 attempts to access the URLs of hosted shells  , from 71 unique IP addresses  , located in 17 countries with the top three being Turkey  , USA  , and Germany. gc ,template will not have side-effects on the database  , so the entire computation can be rolled back if desired. a All strings occurring in root occur in node In this example  , the rule template gc-template we exhibit shall be a function from deltas t.o deltas  , such t ,hat if A is an arbitrary set of insertions and deletions on a database instance LIB  , then applygc ,templateA ,DB will be the result of garbage collection on applyA  , DB. In Java and the CLR  , access control is based on stack inspection 6 : when a security-sensitive operation is performed   , all the methods currently on the stack are checked to see if their classes have been granted the relevant permission . To prevent its clients now on the stack from requiring the relevant FilePermission—which a maliciously crafted client could misuse to erase the contents Classes Permissions Enterprise School Lib Priv java.net. SocketPermission "ibm.com"  , "resolve" java.net. SocketPermission "ibm.com:80"  , "connect" java.net. SocketPermission "vt.edu"  , "resolve" java.net. SocketPermission "vt.edu:80"  , "connect" java.io. FilePermission "C:/log.txt"  , "write" There are many studies of users of digital libraries and collections 1 and a great deal of work on evaluating digital libraries for examples  , see issues of D-Lib at http://www.dlib.org/ and Chris Neuhaus's bibliography http://www.uni.edu/neuhaus/digitalbibeval.html  , but we did not find studies of null searches to identify collections gaps in order to develop user-centered collections. DLESE resources are contributed or collected from many sources  , and although all the materials need to be within the scope of DLESE as expressed by the Collections Policy  , there was no guarantee of balance in the collection across the many subjects that were of interest to the diverse and generally unknown user groups. We take a multi-phase optimization approach to cope with the complexity of parallel multijoin query optimization. Parallel multi-join query optimization is even harder 9  , 14  , 25.  Query optimization query expansion and normalization.  Query execution. a join order optimization of triple patterns performed before query evaluation. We focus on static query optimization  , i.e. Query optimization is a fundamental and crucial subtask of query execution in database management systems. Specify individual optimization rules. Any truly holistic query optimization approach compromises the extensibility of the system. There has been a lot of work in multi-query optimization for MV advisors and rewrite. First  , is to include multi-query optimization in CQ refresh. We now apply query optimization strategies whenever the schema changes. We now highlight some of the semantic query optimizationSQO strategies used by our run time optimizer. Thus the system has to perform plan migration after the query optimization. In query optimization using views  , to compute probabilities correctly we must determine how tuples are correlated. portant drawbacks with lineage for information exchange and query optimization using views. Semantic query optimization is well motivated in the literature6 ,5 ,7  , as a new dimension to conventional query optimization. is implemented as a rule-based system. Our experiments were carried out with Virtuoso RDBMS  , certain optimization techniques for relational databases can also be applied to obtain better query performance. Meta query optimization. The major problem that multi-query optimization solves is how to find common subexpressions and to produce a global-optimal query plan for a group of queries. Multi-query optimization is a technique working at query compilation phase. Multi-query optimization detects common inter-and intra-query subexpressions and avoids redundant computation 10  , 3  , 18  , 19. This comprises the construction of optimized query execution plans for individual queries as well as multi-query optimization. Logical query optimization uses equalities of query expressions to transform a logical query plan into an equivalent query plan that is likely to be executed faster or with less costs. Optimization. It complements the conventional query optimization phase. This is exactly the concept of Coarse-Grained Optimization CGO. One category of research issues deals with mechanisms to exploit interactions between relational query optimization and E-ADT query optimization. SQL Query Optimization with E-ADT expressions: We have seen that E-ADT expressions can dominate the cost of an SQL query. As in applying II to conventional query optimization  , an interesting question that arises in parametric query optimization is how to determine the running time of a query optimizer for real applications . These results are very promising and indicate that  , by using sipIIsl  , parametric query optimization can be efficiently supported in current systems. Not only are these extra joins expensive  , but because the complexity of query optimization is exponential in the amount of joins  , SPARQL query optimization is much more complex than SQL query optimization. SQL systems tend to be more efficient than triple stores  , because the latter need query plans with many self-joins – one per SPARQL triple pattern. The optimization on this query is performed twice. This query is shown in Figure 7. 33. 6  reports on a rule-based query optimizer generator  , which was designed for their database generator EXODUS 2. Rule-based query optimization is not an entirely new idea: it is borrowed from relational query optimization  , e.g. , 5  , 8  , 13  , 141. We divide the optimization task into the following three phases: 1 generating an optimized query tree  , 2 allocating query operators in the query tree to machines  , and 3 choosing pipelined execution methods. Breaking the Optimization Task. The parallel query plan will be dete&iined by a post optimization phase after the sequential query optimization . DB2 query optimizer has the' cost function in terms of resource consumption such as t.he CPU 'dime and I/O time. Typically  , all sub-expressions need to be optimized before the SQL query can be optimized. Query Optimization: The optimization of an SQL query uses cost-based techniques to search for a cheap evaluation plan from a large space of options. Query optimization in general is still a big problem. ? The architecture should readily lend itself to query optimization. 4. Optimization of the internal query represen- tation. 2. Good query optimization is as important for 00 query languages as it is for relational query languages. 5 21. Mid-query re-optimization  , progressive optimization  , and proactive re-optimization instead initially optimize the entire plan; they monitor the intermediate result sizes during query execution  , and re-optimize only if results diverge from the original estimates. The optimization of each stage can use statistics cardinality   , histograms computed on the outputs of the previous stages. However  , semantic optimization increases the search space of possible plans by an order of magnitude  , and very ellicient searching techniques are needed to keep .the cost'of optimization within reasonable limits. As a result  , large SPARQL queries often execute with a suboptimal plan  , to much performance detriment. Then query optimization takes place in two steps. The Query Evaluator parses the query and builds an operator based query tree. The optimization goal is to find the execution plan which is expected to return the result set fastest without actually executing the query or subparts. This simplifies query optimization Amma85. Second  , the project operations are posponed until the end of the query evaluation. They investigate the applicability of common query optimization techniques to answer tree-pattern queries. Cost-based query optimization techniques for XML 22  , 29 are also related to our work. Substantial research on object-oriented query optimization has focused on the design and use of path indexes  , e.g. , BK89  , CCY94  , KM92. For query optimization  , we show how the DataGuide can be used as a parh index. Note that most commercial database systems allow specifying top-k query and its optimization. In general  , the need for rank-aware query optimization and possible approaches to supporting it is discussed in 25. The notion of using algebraic transformations for query optimization was originally developed for the relational algebra. We would like to develop a formal basis for query optimization for data models which are based on bags. In Section 3  , we view query optimization as a generic search problem and introduce a class hierarchy to model search strategies. In Section 2  , we model the search space  , which describes the query optimization problem and the associated cost model. Finally  , the optimal query correlatioñ Q opt is leveraged for query suggestion. Typically  , the optimization finishes within 30 iterations. The optimization problem of join order selection has been extensively studied in the context of relational databases 12  , 11  , 16. Picking the next query edge to fix is essentially a query optimization problem. This is in some cases not guaranteed in the scope of object-oriented query languages 27. 3 Dynamic Query Optimization Ouery optimization in conventional DBS can usually be done at compile time. While research in the nested algebra optimization is still in its infancy  , several results from relational algebra optimization 13 ,141 can be extended to nested relations. The QUERY LANGUAGE OPTIMIZER will optimize this query into an optimized access plan. IQP: we consider a modified version of the budget constrained optimization method proposed in 13 as a query selection baseline. Therefore  , the optimization function is changed to 6 also gives an overview over current and future development activities. Cost based optimization will be explored as another avenue of future work. Our current implementation is based on rule-based query optimization. Each iteralion contains a well-defined sequence of query optimization followed by data allocation optimization. The iterative approach controls the overall complexity of the combined problem. the optimization time of DPccp is always 1. As the optimization time varies greatly with the query size  , all performance numbers are given relative to DPccp  , e.g. More importantly  , multi-query optimization can provide not only data sharing but also common computation sharing. However  , the multi-query optimization technique can provide maximized capabilities of data sharing across queries once multiple queries are optimized as a batch. The major form of query optimization employed in KCRP results from proof schema structure sharing. . In a set-at-a-time system  , query optimization can take place at at least two levels. -We shall compare the methods for extensible optimization in more detail in BeG89. Another approach to extensible query optimization using the rules of a grammar to construct query plans is described in Lo88. A novel architecture for query optimization based on a blackboard which is organized in successive regions has been devised. For illustration purpose a sample optimization was demonstrated. Our approach allows both safe optimization and approximate optimization. The amount of pruning can be controlled by the user as a function of time allocated for query evaluation. A modular arrangement of optimization methods makes it possible to add  , delete and modify individual methods  , without affecting the rest. Semantic query optimization also provides the flexibility to add new information and optimization methods to an existing optimizer. The optimization problem becomes even more interesting in the light of interactive querying sessions 2  , which should be quite common when working with inductive databases. This is effectively an optimization problem  , not unlike the query optimization problem in relational databases. That is  , we break the optimization task into several phases and then optimize each phase individually. Query Evaluation: If a query language is specified  , the E- ADT must provide the ability to execute the optimized plan. Query Operators and Optimization: If a declarative query language is specified  , the E-ADT must provide optimization abilities that will translate a language expression into a query evaluation plan in some evaluation algebra. This expansion allows the query optimizer to consider all indexes on relations referenced in a query. To give the optimizer more transformation choices  , relational query optimization techniques first expand all views referenced in a query and then apply cost-based optimization strategies on the fully expanded query 16 22 . First we conduct experiments to compare the query performance using V ERT G without optimization  , with Optimization 1 and with Optimization 2. In this section we present experimental results. The current implementation of DARQ uses logical query optimization in two ways. It utilizes containment mapping for identifying redundant navigation patterns in a query and later for collapsing them to minimize the query. 9 exploits XQuery containment for query optimization. 14 into an entity-based query interface and provides enhanced data independence   , accurate query semantics  , and highlevel query optimization 6 13. 17  and object-oriented approaches e.g. We represent the query subject probability as P sb S and introduce it as the forth component to the parsing optimization. Query open doesn't have the query subject. After query planning the query plan consists of multiple sub-queries. To build the plan we use logical and physical query optimization. Secondly  , relational algebra allows one to reason about query execution and optimization. This allows the result of one query to be used in the next query. We abstract two models — query and keyword language models — to study bidding optimization prob- lems. The query optimization steps are described as transformation rules or rewriting rules 7. 0 That is  , any query optimization paradig plugged-in. The signature of the SumScan operator is: open. ASW87 found this degree of precision adequate in the setting of query optimization. Astrahan  , et al. What happens when considering complex queries ? We showed the optimization of a simple query. This problem can also be solved by employing existing optimization techniques. 13 for query q. And does this have impact with our technique ? We introduce a new loss function that emphasizes certain query-document pairs for better optimization. : Multiple-query optimization MQO 20 ,19 identifies common sub-expressions in query execution plans during optimization  , and produces globally-optimal plans. To avoid unnecessary materializations  , a recent study 6 introduces a model that decides at the optimization phase which results can be pipelined and which need to be materialized to ensure continuous progress in the system. For instance   , NN queries over an attribute set A can be considered as model-based optimization queries with F  θ  , A as the distance function e.g. , Euclidean and the optimization objective is minimization. This definition is very general  , and almost any type of query can be considered as a special case of model-based optimization query. This lower optimization cost is probably just an artifact of a smaller search space of plans within the query optimizer  , and not something intrinsic to the query itself. Figure 2shows that the optimization cost of all three queries is comparable  , although Q 2 has a noticeably lower optimization cost. Heuristics-based optimization techniques generally work without any knowledge of the underlying data. Heuristics-based optimization techniques include exploiting syntactic and structural variations of triple patterns in a query 27  , and rewriting a query using algebraic optimization techniques 12 and transformation rules 15 . The optimization cost becomes comparable to query execution cost  , and minimizing execution cost alone would not minimize the total cost of query evaluation  , as illustrated in Fig Ignoring optimization cost is no longer reasonable if the space of all possible execution plans is very large as those encountered in SQOS as well as in optimization of queries with a large number of joins. Both directions of the transformation should be considered in query optimization. Figure 2a and Classical database query optimization techniques are not employed in KCRP currently  , but such optimization techniques as pushing selections within joins  , and taking joins in the most optimal order including the reordering of database literals across rules must be used in a practical system to improve RAP execution. Our experiments show that the SP approach gives a decent performance in terms of number of triples  , query size and query execution time. Since only default indexes were created  , and no optimization was provided   , this leaves a room for query optimization in order to obtain a better query performance. For the query performance  , the SP queries give the best performance  , which is expected and consistent with the query length comparison. RDF native query engines typically use heuristics and statistics about the data for selecting efficient query execution plans 27. In this paper  , we present a value-addition tool for query optimizers that amortizes the cost of query optimization through the reuse of plans generated for earlier queries. The inherent cost of query optimization is compounded by the fact that typically each new query that is submitted to the database system is optimized afresh. The detection of common sub-expressions is done at optimization time  , thus  , all queries need to be optimized as a batch. Finally  , our focus is on static query optimization techniques. In fact  , the query performance of query engines is not just affected by static query optimization techniques but  , for instance  , also by the design of index structures or the accuracy of statistical information. By contrast  , we postpone work on query optimization in our geographic scalability agenda  , preferring to first design and validate the scalability of our query execution infrastructure. To our knowledge  , Mariposa was never deployed or simulated on more than a dozen machines  , and offered no new techniques for query execution  , only for query optimization and storage replication. In general  , any query adjustment has to be undertaken before any threshold setting  , as it aaects both ast1 and the scores of the judged documents  , all of which are used in threshold setting. These include: Reweighting query terms Query expansion based on term selection value Query optimization weights anddor selection of terms Threshold optimization. We begin in Section 2 by motivating our approach to order optimization by working through the optimization of a simple example query based on the TPC-H schema using the grouping and secondary ordering inference techniques presented here. We empirically show the benefits of plan refinement and the low overhead it adds to the cost of SELECT c custkey  , COUNT * FROM Customer  , Supplier WHERE c nationkey = s nationkey GROUPBY c custkey Figure 1: A Simple Example Query query optimization Section 5. On the other  , they are useful for query optimization via query rewriting. On the one hand  , the kinds of identities above attest to the naturality of our deenitions. Optimization of this query should seek to reduce the work required by PARTITION BY and ORDER BYs. The main query uses these results. Our work builds on this paradigm. However  , sound applications of rewrite rules generate alternatives to a query that are semantically equivalent. 'I'he traditional optimization problem is to choose an optimal plan for a query. Relational optimizers thus do global optimization by looking inside all referenced views. The paper is organized as follows. Furthermore  , the rules discovered can be used for querying database knowledge  , cooperative query answering and semantic query optimization. Optimization techniques are discussed in Section 3. In Section 2  , query model is formalized by defining all the algebraic operations required to compute answers to a query. That is  , at each stage a complete query evaluation plan exists. The " wholistic " approaches  , e.g. , 26  , 41  , consider an optimization graph-logical or physical--representing the entire query. They suffer from the same problems mentioned above. SQL-based query engines rely on relational database systems storage and query optimization techniques to efficiently evaluate SPARQL queries. The query engine uses this information for query planning and optimization. Data sources are described by service descriptions see Section 3.1. During the query optimization phase  , each query is broken down into a number of subqueries on the fragments . We discuss extensions in $2.3. JOQR is similar in functionality to a conventional query optimizer . We adopt a two-phase approach HS91 to parallel query optimization: JOQR followed by parallelization. Sections 4 and 5 detail a query evaluation method and its optimization techniques. Section 3 explains query generation without using a large lexicon. , April 21–25  , 2008ACM 978-1-60558-085-2/08/04. Query queries  , we have developed an optimization that precomputes bounds. Unfortunately  , restructuring of a query is not feasible if it uses different types of distance-combining functions. Table  IncludingPivot and Unpivot explicitly in the query language provides excellent opportunities for query optimization. These operations provide the framework to enable useful extensions to data modeling. Still  , strategy 11 is only a local optimization on each query. A simplr I ,RU type strategy like strategy W  , ignoring the query semantics  , performs very badly. The main concerns were directed at the unique operations: inclusive query planning and query optimization. Validity  , reliability  , and efficiency are more complex issues to evaluate. On the other hand  , more sophisticated query optimization and fusion techniques are required. Data is not replicated and is guaranteed to be fresh at query time. Tioga will optimize by coalescing queries when coalescing is advantageous. An optimization available on megaplans is to coalesce multiple query plans into a single composite query plan. In the third stage  , the query optimizer takes the sub-queries and builds an optimized query execution plan see Section 3.3. It highlights that our query optimization has room for improvement. We consider that this is due to a better consideration of this query particular pattern. Weights  , constraints  , functional attributes  , and optimization functions themselves can all change on a per-query basis . The attributes involved in each query will be different. The consideration of RDF as database model puts forward the issue of developing coherently all its database features. query optimization  , query rewriting  , views  , update. Motivated by the above  , we have studied the problem of optimizing queries for all possible values of runtime parameters that are unknown at optimization time a task that we call Parametric Query Optimiration   , so that the need for re-optimization is reduced. When these optimization-time assumptions are violated at execu-tion time  , m-optimization is needed or performance suffers. The multi-query optimization technique has the most restrictive requirement on the arrival times of different queries due to the limitation that multiple queries must be optimized as a batch. Thus  , a main strength of FluXQuery is its extensibility and the ability to benefit from a large body of previous database research on algebraic query optimization. Query optimization is carried out on an algebraic  , query-language level rather than  , say  , on some form of derived automata. On the other hand  , declarative query languages are easier to read since inherently they describe only the goal of the query in a simpler syntax  , and automatic optimization can be done to some degree. Manual optimization is easily possible without having to know much about the query engine's internals. This post optimizer kxamines the sequential query plan to see how to parallelize a gequential plan segment and estimates the overhead as welLas the response time reduction if this plan segment is executed in parallel. The query optimizer can add-derivation operators in a query expression for optimization purpose without explicitly creating new graph view schemes in the database. The query optimizer can naturally exploit this second optimization by dynamically building a temporary graph view: bfaidhd = e QEdge:rmdtypd'main mad " @oad and by applying Paths0 on it. optimization cost so far + execution cost is minimum. The notation is summarized in Integrated Semantic Query Optimization ISQO: This is the problem of searching the space of all possible query execution plans for all the semantically equivalent queries  , hut stopping the search when the total query evaluation time i.e. Query Language: An E-ADT can provide a query language with which expressions over values of/that E-ADT can be specified for example  , the relation E-ADT'may provide SQL as the query language  , and the sequence E-ADT may provide SEQinN. Our query language permits several  , possibly interrelated  , path expressions in a single query  , along with other query constructs. However  , we decided to build a new overall optimization framework for a number of reasons: Previous work has considered the optimization of single path expressions e.g. , GGT96  , SMY90. We differ in that 1 if the currently executing plan is already optimal  , then query re-optimization is never invoked. Techniques for dynamic query re-optimization 1615 attempt to detect sub-optimal plans during query execution and possibly re-use any intermediate results generated to re-compute the new optimal plan. However  , unlike query optimization which must necessarily preserve query equivalence  , our techniques lead to mappings with better semantics  , and so do not preserve equivalence. Our techniques are in the same spirit of work on identifying common expressions within complex queries for use in query optimization 25. To overcome this problem  , parametric query optimization PQO optimizes a query into a number of candidate plans  , each optimal for some region of the parameter space CG94  , INSS97  , INSS92  , GK94  , Gan98. Optimizing a query into a single plan may result in a substantially sub-optimal plan if the actual values are different from those assumed at optimization time GW89. For achieving efficiency and handling a general class of XQuery codes  , we generate executable for a query directly  , instead of decomposing the query at the operator level and interpreting the query plan. Second  , we present a new optimization called the control-aware optimization   , which can improve the efficiency of streaming code. When the SQL engine parses the query  , it passes the image expression to the image E-ADT   , which performs type checking and returns an opaque parse structure ParseStruct. This is an issue that requires further study in the form of a comprehensive performance evaluation on sipI1. Subsequently  , Colde and Graefe 8 proposed a new query optimization model which constructs dynamic plans at compile-time and delays some of the query optimization until run-time. Graefe and Ward 15 focused on determining when re-optimizing a given query that is issued repeatedly is necessary. The challenge in designing such a RISCcomponent successfully is to identify optimization techniques that require us to enumerate only a few of all the SPJ query sub-trees. Yet  , layering enables us to view the optimization problem for SPJ+Aggregation query engine as the problem of moving and replicating the partitioning and aggregation functions on top of SPJ query sub-trees. In FS98 two optimization techniques for generalized path expressions are presented  , query pruning and query rewriting using state extents. In CCM96  an algebraic framework for the optimization of generalized path expressions in an OODBMS is proposed  , including an approach that avoids exponential blow-up in the query optimizer while still offering flexibility in the ordering of operations. In section 6 the performance measurement is presented  , and finally section 7 summarizes our experiences and outlines future work. Kabra and DeWitt 21 proposed an approach collecting statistics during the execution of complex queries in order to dynamically correct suboptimal query execution plans. LEO is aimed primarily at using information gleaned from one or more query executions to discern trends that will benefit the optimization of future queries. Both solutions deal with dynamic reoptimization of parts of a single query  , but they do not save and exploit this knowledge for the next query optimization run. If the format of a query plan is restricted in some manner  , this search space will be reduced and optimization will be less expensive. For example  , during optimization  , the space of alternative query plans is searched in order to find the " optimal " query plan. There are six areas of work that are relevant to the research presented here: prefetching  , page scheduling for join execution  , parallel query scheduling  , multiple query optimization  , dynamic query optimization and batching in OODBs. Another topic for future \irork is providing support for cancelling submitted subqueries to the scheduler when a restrict or a join node yields an empty result. The idea of the interactive query optimization test was to replace the automatic optimization operation by an expert searcher  , and compare the achieved performance levels as well as query structures. The searcher is able to study  , in a convenient and effortless way  , the effects of query changes. Query optimization: DBMSs typically maintain histograms 15 reporting the number of tuples for selected attribute-value ranges. l Once registered in Routines within Kleisli manage optimization  , query evaluation  , and I/O from remote and local data sources. Service Descriptions are represented in RDF. Furthermore  , service descriptions can include statistical information used for query optimization. Even the expressions above and in And as such these approaches offer excellent opportunities for query optimization. Mondial 18 is a geographical database derived from the CIA Factbook. So  , the query offers opportunities for optimization. Open PHACTS 15   , query optimization time dominates and can run into the tens of seconds. In many RDF applications  , e.g. Extensions to the model are considered in Section 5. Section 4 deals with query evaluation and optimization. Search stops when the optimization cost in last step dominates the improvement in query execution cost. mi. We know that these query optimizations can greatly improve performance. Pipelined join execution is a Pipelining optimization. Generate the set of equivalent queries. which fragments slmultl be fetched from tertiary memory . part of the scheduler to do multiple query optimization betwtcn the subqucries. The optimization in Eq. The numbering in the query canvas implies the order in which the faces are specified. In Section 2 we present related work on query optimization and statistical databases. POP places CHECK operators judiciously in query execution plans. If the CHECK condition is violated  , CHECK triggers re-optimization. Graefe surveys various principles and techniques Gra93. A large body of work exists on query optimization in databases. There are several open challenges for our CQ architecture. Histograms were one of the earliest synopses used in the context of database query optimization 29  , 25. 5. In the context of deductive databases. Identifying common sub-expressions is central to the problem of multiple query optimization. In Section 3  , we describe our new optimization technique . In the next section  , we describe query evaluation in INQUERY. The second optimization exploits the concept of strong-token. Suppose we derive h hit-sequences from a query document. The three products differ greatly from each other with respect to query optimization techniques. We start explaining DJ's techniques. A key difference in query optimization is that we usually have access to the view definitions. 5.2. This makes them difficult to work with from an optimization point of view. Query execution times are  , in theory  , unbounded. Here n denotes the number of documents associated with query q i . , which makes the optimization infeasible. Analogous to order optimization we call this grouping optimization and define that the set of interesting groupings for a given query consists of 1. all groupings required by an operator of the physical algebra that may be used in a query execution plan for the given query 2. all groupings produced by an operator of the physical algebra that may be used in a query execution plan for the given query. by avoiding re-hashing if such information was easily available. A database system that can effectively handle the potential variations in optimization queries will benefit data exploration tasks. They are complementary to our study as they target an environment where a cost-based optimization module is available. In the area of Semantic Query Optimization  , starting with King King81  , researchers have proposed various ways to use integrity constraints for optimization. The idea of using integrity constraints to optimize queries is not new. In particular  , we describe three optimization techniques that exploit text-centric actions that IE programs often execute. Unlike current extraction approaches  , we show that this framework is highly amenable to query optimization . The Auto-Fusion Optimization involves iterations of fusion runs i.e. , result merging  , where best performing systems in selected categories e.g. , short query  , top 10 systems  , etc. This year  , we devised another alternative fusion weight determination method called Auto-Fusion Optimization. Our demonstration also includes showing the robustness POP adds to query optimization for these sources of errors. All of these sources of errors can trigger re-optimization because of a violation of the validity ranges. Thus  , optimizing the evaluation of boolean expressions seems worthwhile from the standpoint of declarative query optimization as well as method optimization. The need for optimizing methods in object bases has been motivated by GM88  , LD91. This file contains various classes of optimization/translation rules in a specific syntax and order. The information needed for optimization and query translation itself comes from a text file " OptimizationRules " . The DBS3 optimizer uses efficient non-exhaustive search strategies LV91 to reduce query optimization cost. When compared through this metrics  , many more tentative PTs are kept during the search  , thereby increasing significantly the optimization cost. Indeed  , our investigation can be regarded as the analogue for updates of fundamental invest ,igat.ions on query equivalence and optimization. Most of our results concern transaction equivalence and optimization. In all experiments  , TSA yields the best optimization/execution cost  , ratio. The major contribution of this paper is an extension of SA called Toured Simulated Annealing TSA  , to better deal with parallel query optimization. Contrary to previous works  , our results show clearly that parallel query optimization should not imply restricting the search space to cope with the additional complexity. For this purpose; we extended randomized strategies for parallel optimization  , and demonstrated their effectiveness. Further  , we also improve on their solution. In the next Section we discuss the problem of LPT query optimization where we import the polynomial time solution for tree queries from Ibaraki 841 to this general model of  ,optimization. For example   , if NumRef is set to the number of relations in the query  , it is not clear how and what information should be maintained to facilitate incremental optimization . Second  , the proposed incremental optimization strategy has a limitation. Following Hong and Stonebraker HS91  , we break the optimization problem into two phases: join ordering followed by parallelization. We address the problem of parallel query optimization  , which is to find optimal parallel plans for executing SQL queries. Clearly  , the elimination of function from the path length of high traffic interactions is a possible optimization strategy. Others question the propriety of removing DBMS services such as query optimization and views and suggest utilizing only high level interfaces. We have demonstrated the effects of query optimization by means of performance experiments. The primary contribution of this research is to underscore the importance of algebraic optimization for sequence queries along with a declarative language in which to express them. Our second goal with this demo is to present some of our first experiments with query optimization in Galax. Researchers interested in optimization for XQuery can implement their work in a context where the details of XQuery cannot be overlooked. We also showed how to incorporate our strategies into existing query optimizers for extensible databases. Our optimization strategies are provably good in some scenarios  , and serve as good heuristics for other scenarios where the optimization problem is NP-hard. AQuery builds on previous language and query optimization work to accomplish the following goals: 1. Edge optimization and sort splitting and embedding seem to be particularly promising for order-dependent queries. These optimization rules follow from the properties described earlier for PIVOT and UNPIVOT. The architecture of our query optimizer is based on the Cascades framework 3  , which enables defining new relational operators and optimization rules for them. However  , we can think of static optimization such as determining whether a query or a subquery is type-invalid early by inspecting the type information to avoid useless evaluation over potentially large amounts of irrelevant data. The method normalizes retrieval scores to probabilities of relevance prels  , enabling the the optimization of K by thresholding on prel. The threshold K was calculated dynamically per query using the Score-Distributional Threshold Optimization SDTO 1. This also implies that for a QTree this optimization can be used only once. If the outer query already uses GROUP-BY then the above optimization can not be applied. While ATLAS performs sophisticated local query optimization   , it does not attempt to perform major changes in the overall execution plan  , which therefore remains under programmer's control. and in-memory table optimization  , is carried out during this step. The direct applicability of logical optimization techniques such as rewriting queries using views  , semantic optimization and minimization to XQuery is precluded by XQuery's definition as a functional language 30. Finally query generation tools tend to generate non-minimal queries 31. The query term selection optimization was evaluated by changing /3 and 7. Although the precision decreased by several percent  , especially in the middle ranges in recall  , the combined optimization speeded retrieval by a factor of 10. A powerful 00 data modelling language permits the construction of more complex schemas than for relational databases. In order to query iDM  , we have developed a simple query language termed iMeMex Query Language iQL that we use to evaluate queries on a resource view graph. Therefore  , we follow the same principle as LUBM where query patterns are stated in descending order  , w.r.t. We deem query plan optimization an integral part of an efficient query evaluation. Given a logical query  , the T&O performs traditional query optimization tasks such as plan enumeration  , evaluating join orderings  , index selections and predicate place- ment U1188  , CS96  , HSSS. Our approach incorporates a traditional query optimizer T&O  , as a component. The different formats that exist for query tree construction range from simple to complex. As will be shown  , the different formats offer different tradeoffs  , both during query optimization and query execution. In database query languages late binding is somewhat problematic since good query optimization is very important to achieve good performance. Having late binding in the query language is necessary @ the presence of inheritance and operator overloading. There is currently no optimization performed across query blocks belonging to different E-ADTs . In this example   , the SQL optimizer is called on the outer query block  , and the SEQUIN optimizer operates on the nested query block. The entity types of our sample environment are given in Figs. In our experiments we found that binning by query length is both conceptually simple and empirically effective for retrieval optimization. Any query-dependent feature or combination of thereof can be used for query binning. Dynamic re-optimization techniques augment query plans with special operators that collect statistics about the actual data during the execution of a query 9  , 13. Moreover  , our approach is effective for any join query and predicate combinations. If a query can m-use cached steps  , the rest of the parsing and optimization is bypassed. These include exact match of the query text and equivalent host types from where the query originated. The task of the query optimizer is to build a feasible and cost-effective query execution plan considering limitations on the access patterns. Also  , the underlying query optimizer may produce sub-optimal physical plans due to assumptions of predicate independence. For this modularity  , we pay the penalty of inefficient query optimizers that do not tightly couple alternate query generation with cost-based optimization . DB2 Information Integrator deploys cost-based query optimization to select a low cost global query plan to execute . Statistics about the remote databases are collected and maintained at II for later use by the optimizer for costing query plans. We discuss the various query plans in a bit more detail as the results are presented. Consequently  , all measurements reported here are for compiled query plan execution i.e. , they do not include query optimization overhead. Development of such query languages has prompted research on new query optimization methods  , e.g. The evolution of relational databases into Object-Relational databases has created the need for relationally complete and declarative Object-Oriented 00 query languages. By compiling into an algebraic language  , we facilitate query optimization. Secondly  , many query optimizers work on algebraic representations of queries  , and try to optimize the order of operations to minimize the cost while still computing an algebraically equivalent query. Semantic query optimization can be viewed as the search for the minimum cost query execution plan in the space of all possible execution plans of the various semantically equivalent hut syntactically ditferent versions of the original query. Heurirtic Marching: We note that other researchers have termed such queries 'set queries' Gavish and Segev 19861. Query optimization derives a strategy for transmitting and joining these relations in order to minimize query total time or query response time. FluXQuery is  , to our knowledge  , the first XQuery engine that optimizes query evaluation using schema constraints derived from DTDs 1 . Apart from the obvious advantage of speeding up optimization time  , it also improves query execution efficiency since it makes it possible for optimizers to always run at their highest optimization level as the cost of such optimization is amortized over all future queries that reuse these plans. We have presented and evaluated PLASTIC  , a valueaddition tool for query optimizers that attempts to efficiently and accurately predict  , given previous training instances   , what plans would be chosen by the optimizer for new queries. RuralCafe  , then allows the users to choose appropriate query expansion terms from a list of popular terms. The first optimization is to suggest associated popular query terms to the user corresponding to a search query. The objective of this class of queries is to test whether the selectivity of the text query plays a role in query optimization. A natural example of such a query is searching for catalog items by price and description. The optimal point for this optimization query this query is B.1.a. Since the worklist is now empty  , we have completed the query and return the best point. The next important phase in query compilation is Query Optimization. A prominent example in which this can happen is a query with a Boolean AND expression if one of the subexpressions returns false and the other one returns an error. There are several reasons for wanting to restrict the design of a query tree. Planning a function like S&QWN causes the optimization of the embedded query to be performed. However  , existing work primarily focuses on various aspects of query-local data management  , query execution   , and optimization. In addition to the early work on Web queries  , query execution over Linked Data on the WWW has attracted much attention recently 9 ,10 ,12 ,13 ,14. The trade-off between re-optimization and improved runtime must be weighed in order to be sure that reoptimization will result in improved query performance. It remains future work to investigate whether and when re-optimization of a query should take place. E.g. , Given two topic names  , " query optimization " and " sort-merge join "   , the Prerequisite metalink instance " query optimization Pre sort-merge join  , with importance value 0.8 " states that " prerequisite to viewing  , learning  , etc. Metalinks represent relationships among topics not sources; i.e. , metalinks are " meta " relationships. The blackbox ADT approach for executing expensive methods in SQL is to execute them once for each new combination of arguments. A control strategy is needed to decide on the rewrite rules that should be applied to a given statement sequence. These optimizations are similar to rewrite rules used in conventional single-query optimization 4 as well as in multi-query optimization 1  , 6. With such an approach  , no new execution operators are required  , and little new optimization or costing logic is needed. Transforming PIVOT into GROUP BY early in query compilation for example  , at or near the start of query optimization or heuristic rewrite requires relatively few changes on the part of the database implementer. Optimization during query compilr tion assumes the entire buffer pool is available   , but in or&r to aid optimization at nmtime  , the query tree is divided into fragments. STON89 describes how the XPRS project plans on utilizing parallelism in a shared-memory database machine. The original query is transformed into syntactically different  , but semantically equivalent t queries  , which may possibly yield a more efficient execution planS. Compared to the global re-optimization of query plans  , our inspection approach can be regarded as a complementary   , local optimization technique inside the hash join operator. If the operator detects that the actual statistics deviate considerably from the optimizer's estimates  , the current execution plan is stopped and a new plan is used for the remainder of the query. The Plastic system  , proposed in GPSH02   , amortizes the cost of query optimization by reusing the plans generated by the optimizer. CHS99  proposes least expected cost query optimization which takes distribution of the parameter values as its input and generates a plan that is expected to perform well when each parameter takes a value from its distribution at run-time. Optimization for queries on local repositories has also focused on the use of specialized indices for RDF or efficient storage in relational databases  , e.g. Research on query optimization for SPARQL includes query rewriting 9 or basic reordering of triple patterns based on their selectivity 10. A " high " optimization cost may be acceptable for a repetitive query since it can be amortized over multiple executions. This is made difficult by the necessary trade-off between optimization cost and quality of the generated plans the latter translates into query execution cost. We see that the optimization leads to significantly decreased costs for the uniform model  , compared to the previous tables. In Table 5we show CPU costs with this optimization  , for queries with expected query range sizes of 7 days  , 30 days  , and one year  , under the uniform and biased query model. In Figure 5  , we show results for the fraction pruning method and the max score optimization on the expanded query set. We found that  , counter to general wisdom regarding the max score optimization  , max score and our technique did not work as effectively on our expanded query set as on title queries. For example  , if our beers/drinkers/bars schema had " beers " as a top level node  , instead of being as a child node of Drinkers  , then the same query would had been obtained without the reduction optimization. We observed that this optimization also helps in making the final SQL query less sensitive to input schema. Many researchers have investigated the use of statistics for query optimization  , especially for estimating the selectivity of single-column predicates using histograms PC84  , PIH+96  , HS95 and for estimating join sizes Gel93  , IC91  , SS94 using parametric methods Chr83  , Lyn88 . Cost-based query optimization was introduced in SAC+79. For suitable choices of these it might be feasible to efficiently obtain a solution. It is evident that the result of a general OPAC query involves the solution of an optimization problem involving a potentially complex aggregation constraint on relation   , the nature of the aggregation constraint  , and the optimization objective  , different instances of the OPAC query problem arise. Third-order dependencies may be useful  , however   , and even higher-order dependencies may be of interest in settings outside of query optimization. The results in 16  indicate that  , for purposes of query optimization  , the benefits of identifying kth-order dependencies diminish sharply as k increases beyond 2. Doing much of the query optimization in the query language translator also helps in keeping the LSL interpreter as simple as possible. It is the translator  , not the LSL interpreter  , which can easily view the entire boolean qualification so as to make such an optimization. This research is an important contribution to the understanding of the design tradeoffs between query optimization and data allocation for distributed database design. The numhcr  , placement  , and effective use of data copies is an important design prohlem that is clearly intcrdcpcndent with query optimization and data allocation. Many researchers have worked on optimizer architectures that facilitate flexibility: Bat86  , GD87  , BMG93  , GM931 are proposals for optimizer genera- tors; HFLP89  , BG92 described extensible optimizers in the extended relational context; MDZ93  , KMP93  proposed architectural frameworks for query optimization in object bases. Fre87  , GD87  , Loh88 made rule-based query optimization popular  , which was later adopted in the object-oriented context  , as e.g. , OS90  , KM90  , CD92. Another approach to this problem is to use dynamic query optimization 4 where the original query plan is split into separately optimized chunks e.g. Our approach is to do local optimization of the resolvents of late bound functions and then define DTR in terms of the locally optimized resolvents. The technique in MARS 9 can be viewed as a SQL Optimization technique since the main optimization occurs after the SQL query is generated from the XML query. While this technique has its own advantages  , it does not produce efficient SQL queries for simple XML queries that contain the descendant axis // like the example in Section 2.1. However  , what should be clear is that given such cost-estimates  , one could optimize inductive queries by constructing all possible query plans and then selecting the best one. In the current implementation we e two-level optimization strategy see section 1 the lower level uses the optimization strateg present in this paper  , while the upper level the oy the in which s that we join order egy. Moreover  , as the semantic information about the database and thus the corresponding space of semantically equivalent queries increases  , the optimization cost becomes comparable to the cost of query execution plan  , and cannot be ignored. For query optimization  , a translation from UnQL to UnCAL is defined BDHS96  , which provides a formal basis for deriving optimization rewrite rules such as pushing selections down. query language BDHS96  , FS98 is based on a graph-structured data model similar to OEM. Moreover  , most parallel or distributed query optimization techniques are limited to a heuristic exploration of the search space whereas we provide provably optimal plans for our problem setting. Due to lack of code shipping  , techniques for parallel and distributed query optimization   , e.g. , fragment-replicate joins 26  , are inapplicable in our scenario. In the following we describe the two major components of our demonstration: 1 the validity range computation and CHECK placement  , and 2 the re-optimization of an example query. POP detects this during runtime  , as the validity range for a specific part of a query plan is violated  , and triggers re-optimization. 13; however  , since most users are interested in the top-ranking documents only  , additional work may be necessary in order to modify the query optimization step accordingly. After developing the complete path algebra  , we can apply standard query optimization techniques from the area of database systems see e.g. In this section  , we propose an object-oriented modeling of search systems through a class hierarchy which can be easily extended to support various query optimization search strategies. To establish the framework for modeling search strategies  , we view the query optimization problem as a search problem in the most general sense. We notice that  , using the proposed optimization method  , the query execution time can be significantly improved in our experiments  , it is from 1.6 to 3.9 times faster. The speedup is calculated as the query execution time when the optimization is not applied divided by the optimized time. Whereas query engines for in-memory models are native and  , thus  , require native optimization techniques  , for triple stores with RDBMS back-end  , SPARQL queries are translated into SQL queries which are optimized by the RDBMS. Because of the fundamentally different architectures of in-memory and on-disk models  , the considerations regarding query optimization are very different. When existing access structures give only partial support for an operation  , then dynamic optimization must be done to use the structures wisely. An ADT-method approach cannot identify common sub-expressions without inter-function optimization  , let alone take advantage of them to optimize query execution. Experiment 5 showed that the common subexpression optimization could reduce query execution time by almost a factor of two. For multiple queries  , multi-query optimization has been exploited by 11 to improve system throughput in the Internet and by 15 for improving throughput in TelegraphCQ. The work in 24 proposes rate-based query optimization as a replacement of the traditional cost-based approach. The novel optimization plan-space includes a variety of correlated and decorrelated executions of each subquery  , using VOLCANO's common sub-expression detection to prevent a blow-up in optimization complexity. Further  , ROLEX accepts a navigational profile associated with a view query and uses this profile in a costbased optimizer to choose a best-cost navigational query plan. The original method  , referred to as query prioritization QP   , cannot be used in our experiments because it is defined as a convex optimization that demands a set of initial judgments for all the queries. Note the importance of separating the optimization time from the execution time in interpreting these results. The diversity of search space is proportional to the number of different optimization rules which executed successfully during optimization. The size of the plan space is a function of the query size and complexity but also proportional to the number of exploration rules that created alternatives during optimization. To perform optimization of a computation over a scientific database system  , the optimizer is given an expression consisting of logical operators on bulk data types. In this section we present an overview of transformation based algebraic query optimization  , and show how the optimization of scientific computations fits into this framework. In this way  , the longer the optimization time a query is assigned  , the better the quality of the plan will be.2 Complex canned queries have traditionally been assigned high optimization cost because the high cost can be amortized over multiple runs of the queries. The optimizer should also treat the optimization time as a critical resource. Therefore  , some care is needed when adding groupings to order optimization  , as a slowdown of plan generation would be unacceptable . Experimental results have shown that the costs for order optimization can have a large impact on the total costs of query optimization 3. Apart from the obvious advantage of speeding up optimization time  , PLASTIC also improves query execution efficiency because optimizers can now always run at their highest optimization level – the cost of such optimization is amortized over all future queries that reuse these plans. Further  , even when errors were made  , only marginal additional execution costs were incurred due to the sub-optimal plan choices. These five optimization problems have been solved for each of the 25 selected queries and for each run in the set of 30 selected runs  , giving a total of 5×25×30 = 3  , 750 optimization problems. As seen in Figures 3 and 4  , there are five optimization problems to be solved for each query of each run one for each measure. While search efficiency was one of the central concerns in the design and implementation of the Volcano optimizer generator 8  , these issues are orthogonal to the optimization of scientific computations  , and are not addressed in this paper. To overcome this problem  , parametric query optimization PQO optimizes a query into a number of candidate plans  , each optimal for some region of the parameter space CG94  , INSS92  , GK94  , Gan98. On the other hand  , optimizing a query into a single plan at compilation time may result in a substantially suboptimal plan if the actual parameter values are different from those assumed at optimization time GW89. However  , a plan that is optimal can still be chosen as a victim to be terminated and restarted  , 2 dynamic query re-optimization techniques do not typically constrain the number of intermediate results to save and reuse  , and 3 queries are typically reoptimized by invoking the query optimizer with updated information. To tackle the problem  , we clean the graph before using it to compute query dissimilarity. If the graph is unreliable  , the optimization results will accordingly become unreliable. In addition  , we show that incremental computation is possible for certain operations . : Many of these identities enable optimization via query rewriting. Recent works have exploited such constraints for query optimization and schema matching purposes e.g. , 9. Example constraints include " housearea ≤ lot-area " and " price ≥ 10 ,000 " . Flexible mechanisms for dynamically adjusting the size of query working spaces and cache areas are in place  , but good policies for online optimization are badly missing. Memory management. The contributions in SV98 are complementary to our work in this paper. They also propose techniques for incorporating these alternative choices for cost based query optimization. 27  introduces a rank-join operator that can be deployed in existing query execution interfaces. 20 focuses on the optimization of the top-k queries. Let V denote the grouping attributes mentioned in the group by clause. We empirically show the benefits of plan refinement and the low overhead it adds to the cost of query optimization. Some alternatives are discussed in Has95. A few proposals exist for evaluating transitive closures in distributed database systems 1 ,9 ,22 . Parallelism is however recognized as a very important optimization feature for recursive query evaluation. l The image expression may be evaluated several times during the course of the query. l Deciding between different plans requires cost-based optimization of the image expression. Since vague queries occur most often in interactive systems  , short response times are essential. The models and procedures described here are part of the query optimization. The associated rewrite rules exploit the fact that statements of a sequence are correlated. Section 3 shows that this approach also enables additional query optimization techniques. Repetition is eliminated  , making queries easier to ready  , write  , and maintain. The implementation appeared to be outside the RDBMS  , however  , and there was not significant discussion of query optimization in this context. SchemaSQL 5 implements transposing operations. In Sections 2–4 we describe the steps of the BHUNT scheme in detail  , emphasizing applications to query optimization. The remainder of the paper is organized as follows. Static shared dataflows We first show how NiagaraCQ's static shared plans are imprecise. It can also be used with traditional multiple-query optimization MQO schemes. This monotonicity declaration is used for conventional query optimization and for improving the user interface. The user can specm  , for example  , that WEIGHT =< WEIGHTtPREV. The rest of the paper is organized as follows. The weights for major concepts and the sub concepts are 1.0 and 0.2  , respectively. The method for weight optimization is the same as that for query section weighting. Table 2shows the speedup for each case. have proposed a strategy for evaluating inductive queries and also a first step in the direction of query optimization. De Raedt et al. In addition to the usual query parsing  , query plan generation and query parallelization steps  , query optimization must also determine which DOP to choose and on which node to execute the query. The query coordinator prepares the execution depending on resource availability in the Grid. It also summarizes related work on query optimization particularly focusing on the join ordering problem. Section 5 reviews previous work on index structures for object-oriented data bases. We conclude with a discussion of open problems and future work. An approach to semantic query optimization using a translation into Datalog appears in 13  , 24. Precomputed join indexes are proposed in 46 . We envision three lines of future research. We enforced C&C constraints by integrating C&C checking into query optimization and evaluation. The remaining of this paper is structured as follows. Service call invocations will be tracked and displayed to illustrate query optimization and execution. Section 5 describes the impact of RAM incremental growths on the query execution model. Section 4 addresses optimization issues in this RAM lower bound context. Over all of the queries in our experiments the average optimization time was approximately 1/2 second. Each query was run with an initially empty buffer. 10 modeled conditional probability distributions of various sensor attributes and introduced the notion of conditional plans for query optimization with correlated attributes. Deshpande et al. Moral: AQuery transformations bring substantial performance improvements  , especially when used with cost-based query optimization. The result is consistently faster response times.   , s ,} The problem of parametric query optimization is to find the parametric optimal set of plans and the region of optimality for each parametric optimal plan. Ten years later  , the search landscape has greatly evolved. On the other hand  , in the SQL tradition  , W3QL was a declarative query language that offered opportunities for optimization. First  , our query optimization rules are based on optimizing XPath expressions over SQL/XML and object relational SQL. Our work is unique in the following respects. Furthermore. Sophisticated optimization will be used to separate the original query inlo pieces targeted for individual data sources whose content and order of execution are optimal. Schema knowledge is used to rewrite a query into a more efficient one. In this demo  , we highlight the schema-based optimization SQO on one abstraction level. Next  , we turn our attention to query optimization. We then show how to compile such a program into an execution plan. The module for query optimization and efficient reasoning is under development. The prototype of OntoQuest is implemented with Java 1.4.2 on top of Oracle 9i. For traditional relational databases  , multiplequery optimization 23 seeks to exhaustively find an optimal shared query plan. The problem of sharing the work between multiple queries is not new. We can now formally define the query optimization problem solved in this paper. This assumption is also validated by our experiments Section 7. The second step consists of an optimization and translation phase. Then  , this m%imal Query PCN is build in main memory. Section 2 provides an overview of BP-Mon  , and Section 3 briefly describes the underlying formal model. The size of our indexes is therefore significant  , and query optimization becomes more complex. But within that  , we maintain multiple tables of hundreds of millions of rows each. The existing optimizers  , eg. The approach of simultaneous query optimization will lead to each such plan being generated exactly once for all the queries optimized together. query execution time. For SQO  , we have to consider the trade-off between the cost of optimization and solution quality i.e. No term reweighting or query expansion methods were tried. As last year  , on this occasion we have tried only the threshold optimization. A similar concept is proposed in DeWitt & Gray 92. In addition to syntactic rules  , we may also study the domain-specific rules for inferring new triples using provenance  , temporal or spatial information. Whether or not the query can be unnested depends on the properties of the node-set . This optimization would unnest such a subquery. Several plans are identified and the optimal plan is selected. The basic idea of global planning is the same as query optimization in database management systems. Section 2 formally defines the parametric query optimization problem and provides background material on polytopes. For more sophisticated rules  , cost functions were needed Sma97  to choose among many alternative query plans. Some optimization techniques were designed  , but not all of them were implemented . A related approach is multi-query execution rather than optimization. Such operator sharing is even the cornerstone of the Q-Pipe architecture 14. 4.9  , DJ already maintains the minimal value of all primary keys in its own internal statistics for query optimization. In fact  , as explained in Sect. In Section 2  , we provide some background information on XML query optimization and the XNav operator. Scientific data is commonly represented as a mesh. This model can be exploited for data management and  , in particular  , we will use it for query optimization purposes. Their proposed technique can be independently applied on different parts of the query. 3  , 9  both consider a single optimization technique using one type of schema constraint. sources on sort-merge join "   , and this metalink instance is deemed to have the importance sideway value of 0.8. sources on query optimization is viewing  , learning  , etc. Compiling SQL queries on XML documents presents new challenges for query optimization. And this doesn't even consider the considerable challenges of optimizing XQuery queries! Experiment 3 demonstrates how the valid-range can be used for optimization. These valid ranges can be propagated through the entire query as described in SLR94. This function can be easily integrated in the query optimization algorisms Kobayashi 19811. The second difficulty can be resolved by introducing imaginary tuples. Resolve ties by choosing fragment that has the greater number of queries. Imposing a uniform limit on hot set size over all queries can be suboptimal. One is based on algebraic simplification of a query and compilr tinlc> heuristics. Finally  , consider the two major approaches to qitcry optimization for regular databases. An experienced searcher was recruited to run the interactive query optimization test. In practice  , the test searcher did not face any time constraints. However  , their optimization method is based on Eq. a given query node to Orn time  , thus needing Orn 2  time for all-pairs SimRank. Thirdly  , the relational algebra relies on a simple yet powerful set of mathematical primitives. Figure 4summarizes the query performance for 4 queries of the LUBM. Hence  , it is not surprising that for certain queries no optimization is achieved at all. MIRACLE exploits some techniques used by the OR- ACLE Server for the query optimization a rule-based approach and an statistical approach. 5.3. Section 3 presents our RAM lower bound query execution model. Second  , they provide more optimization opportunities. First  , users can calculate the whole Skycube in one concise and semantic-clear query  , instead of issuing 2 d − 1 skyline queries. To our best knowledge  , the containment of nested XQuery has so far been studied only in 9  , 18  , and 10. We use document-at-a-time scoring  , and explore several query optimization techniques. Second  , we are interested in evaluating the efficiency of the engine. During the first pass the final output data is requested sorted by time. The mathematical problem formulation is given in Section 3. In the literature  , most researches in distributed database systems have been concentrated on query optimization   , concurrency control  , recovery  , and deadlock handling. Finally  , conclusions appear in Section 5. In Section 6 we briefly survey the prior work that our system builds upon. The query evaluation and optimization strategies are then described in Sections 4 and 5. We also plan to explore issues of post query optimization such as dynamic reconfiguration of execution plan at run time. These are topics of future research. In Section 4  , we give an illustrative example to explain different query evaluation strategies that the model offers. Figure 8depicts this optimization based on the XML document and query in Figure 4. Also we can avoid creating any edges to an existence-checking node. The system returned the top 20 document results for each query. The results of our optimization experiments are shown in Tables 2 and 3. Query-performance predictors are used to evaluate the performance of permutations. The approach is based on applying the Cross Entropy optimization method 13 upon permutations of the list. The compiled query plan is optimized using wellknown relational optimization techniques such as costing functions and histograms of data distributions. As a result  , many runtime checks are avoided. If a DataGuide is to be useful for query formulation and especially optimization  , we must keep it consistent when the source database changes. Ct An important optimization technique is to avoid sorting of subcomponents which are removed afterwards due to duplicate elimination. The arrangement of query modification expressions can be optimized. The other set of approaches is classified as loose coupling. However  , such approaches have not exploited the query optimization techniques existing in the DBMSs. Query optimization is a major issue in federated database systems. A CIM application has been prototyped on top of the system RF'F95. Since the early stages of relational database development   , query optimization has received a lot of at- tention. Section 5 concludes the paper. The translation and optimization proceeds in three steps. Our query optimizer translates user queries written in XQuery into optimized FluX queries. Besides  , in our current setting  , the preference between relevance and freshness is assumed to be only query-dependent. For example  , using Logistic functions can naturally avoid the range constrains over query weights in optimization. These specific technical problems are solved in the rest of the paper. Then we give an overview of how a query is executed; this naturally leads to hub selection and query optimization issues. This is a critical requirement in handling domain knowledge  , which has flexible forms. Second  , a declarative query language such as SQL can insulate the users from the details of data representation and manipulation   , while offering much opportunity in query optimization. We examine only points in partitions that could contain points as good as the best solution. DB2 has separate parsers for SQL and XQuery statements   , but uses a single integrated query compiler for both languages. Histograms of element occurrences  , attribute occurrences  , and their corresponding value occurrences aid in query optimization. Many sources rank the objects in query results according to how well these objects match the original query. These characteristics also impact the optimization of queries over these sources. Additionally it can be used to perform other tasks such as query optimization in a distributed environment. A catalog service in a large distributed system can be used to determine which nodes should receive queries based on query content. The Postgres engine takes advantage of several Periscope/SQ Abstract Data Types ADTs and User-Defined Functions UDFs to execute the query plan. The query is then passed on to Postgres for relational optimization and execution . The optimization of Equation 7 is related to set cover  , but not straightforwardly. shows whether query graph q l has feature fi  , and z jl indicates whether database graph gj is pruned for query graph q l . The Epoq approach to extensible query optimization allows extension of the collection of control strategies that can be used when optimizing a query 14. The control we present here is designed to support thii kind of extensibility. Above results are just examples from the case study findings to illustrate the potential uses of the proposed method. One important aspect of query optimization is to detect and to remove redundant operations  , i.e. It is the task of the query optimizer to produce a reasonable evaluation strategy  161. Lots can be explored using me&data such as concept hierarchies  and discovered knowledge. Knowledge discovery in databases initiates a new frontier for querying database knowledge  , cooperative query answering and semantic query optimization. At every region knowledge wurces are act ivatad consecutively completing alternative query evaluation plans. The resultant query tree is then given to the relational optimizer  , which generates the execution plan for the execution engine. The query tree is then further optimized through view merging and subquery to join conversion and operator tree optimization. The goal of such investigations is es- tablishing equivalent query constructs which is important for optimization. Formalization cordtl cotlcern utilization of viewers in languages  , for example  , in query operators or programming primitives. Contributions of R-SOX include: 1. Our R-SOX system  , built with Raindrop 4  , 6  , 5 as its query engine kernel  , now can specify runtime schema refinements and perform a variety of runtime SQO strategies for query optimization. Moreover  , translating a temporal query into a non-temporal one makes it more difficult to apply query optimization and indexing techniques particularly suited for temporal XML documents. Even for simple temporal queries  , this approach results in long XQuery programs. There is no other need for cooperation except of the support of the SPARQL protocol. In particular  , M3 uses the statistics to estimate the cardinality of both The third strategy  , denoted M3 in what follows  , is a variant of M2 that employs full quad-based query optimization to reach a suitable physical query plan. More precisely  , we demonstrate features related to query rewriting  , and to memory management for large documents. At query optimization time  , the set of candidate indexes desirable for the query are recorded by augmenting the execution plan. The broad architecture of the solution is shown in Figure 4. Incorporate order in a declarative fashion to a query language using the ASSUMING clause built on SQL 92. Such models can be utilized to facilitate query optimization  , which is also an important topic to be studied. Another exciting direction for future work is to derive analytical models 12 that can accurately estimate the query costs. Originally  , query containment was studied for optimization of relational queries 9  , 33 . Finally  , we note that query containment has also been used in maintenance of integrity constraints 19  , 15  and knowledge-base ver- ification 26. Suppose we can infer that a query subexpression is guaranteed to be symmetric. Thus we can benefit from the proposed query optimization techniques of Section 3 even if we do not have any stored kernels in the database. query optimization has the goal to find the 'best' query execution plan among all possible plans and uses a cost model to compare different plans. Currently  , we support two join implementations: However  , it is important to optimize these tests further using compile-time query optimization techniques. Evaluating the query tests obviously takes time polynomial in the size of the view instance and base update. For optimization  , MXQuery only implements a dozen of essential query rewrite rules such as the elimination of redundant sorts and duplicate elimination. MXQuery does not have a cost-based query optimizer . Since OOAlgebra resembles the relational algebra   , the familiar relational query optimization techniques can be used. For OODAPLEX  , we had developed an algebra  , OOAlgebra   , as the target language for query compilation DAYA89 . SEMCOG also maintains database statistics for query optimization and query reformulation facilitation. The server functions are supported by five modules to augment the underlying database system multimedia manipulation and search capability. The most expensive lists to look at will be the ones dropped because of optimization. Terms with long inverted lists will therefore be examined last since the query terms are sorted by decreasing query weight. Second  , we describe a novel two-stage optimization technique for parameterized query expansion. As we show  , this framework is a generalization and unification of current state-of-the-art concept weighting 6  , 18  , 31 and query expansion 24  , 15 models. Similarly  , we weight the query terms according to whether they are sub-concepts or not. Similarly   , automatic checking tools face a number of semidecidability or undecidability theoretical results. Extended Datalog is a query language enabling query optimization but it does not have the full power of a programming language. After rewriting  , the code generator translates the query graphs into C++ code. In fact  , V represents the query-intent relationships  , i.e. , there is a D-dimensional intents vector for each query. To solve the optimization problem in 6  , we use a matrix V and let V = XA T . The conventional approach to query optimization is to pick a single efficient plan for a query  , based on statistical properties of the data along with other factors such as system conditions. 1 Suppose the following conditions hold for the example: This type of optimization does not require a strong DataGuide and was in fact suggested by NUWC97. For this query and many others  , such a finding guarantees that the query result is empty. In this case we require the optimizer to construct a table of compiled query plans. When query optimization occurs prior to execution  , resource requests must be deferred until runtime. Section 3.3 describes this optimization. In particular  , we may be able to estimate the cost of a query Q for an atomic configuration C by using the cost of the query for a " simpler " configuration C'. The optimizer's task is the translation of the expression generated by the parser into an equivalent expression that is cheaper to evaluate. For example  , V1 may store some tuples that should not contribute to the query  , namely from item nodes lacking mail descendants. Summary-based optimization The rewritten query can be more efficient if it utilizes the knowledge of the structural summary. Work on frameworks for providing cost information and on developing cost models for data sources is  , of course  , highly relevant. UFA98 describes orthogonal work to incorporate cost-based query optimization into query scrambling. Enhanced query optimizers have to take conditional coalescing rules into consideration as well. However restricting attention to this class of rules means not to exploit the full potential of query optimization. In this method  , subqueries and answers are kept in main memory to reduce costs. However  , a clever optimization of interpreted techniques known as query/sub-query has been developped at ECRC Vieille86 . This query is a variant of the query used earlier to measure the performance of a sequence scan. During execution of the SQL query  , the nested SE &UIN expression is evaluated just as any other function would be. Note  , however  , that the problem studied here is not equivalent to that of query containment. For an overview and references  , see the chapters on query optimization in MA831 or UL82. Well-known query optimization strategies CeP84 push selections down to the leaves of a query tree. The first one is about the consequences of these results for data fragmentation. It is important to understand the basic differences between our scenario and a traditional centralized setting which also has query operators characterized by costs and selectivities. In contrast to MBIS the schema is not fixed and does not need to be specified  , but is determined by the underlying data sources. In this paper  , we make a first step to consider all phases of query optimization in RDF repositories. We defined transformation rules on top of the SQGM to provide means for rewriting and simplifying the query formulation. Then  , we will investigate on optimization by using in-memory storage for the hash tables  , in order to decrease the query runtimes. the input threshold. The join over the subject variable will be less expensive and the optimization eventually lead to better query performance. Therefore  , a static optimizer should reverse the triple patterns. A set of cursor options is selected randomly by the query generator. Typically cursors involve different optimization  , execution and locking strategies depending on a variety of userspecified options. To improve the XML query execution speed  , we extract the data of dblp/inproceedings  , and add two more elements: review and comments. No optimization techniques are used. Copyright 2007 VLDB Endowment  , ACM 978-1-59593-649-3/07/09. Reordering the operations in a conventional relational DBMS to an equivalent but more efficient form is a common technique in query optimization. Reordering Boxes. We call this the irrelevant index set optimization. In this case  , the estimated cost for the query is the same as that over a database with no indexes. 19851. In general  , constraints and other such information should flow across the query optimization interfaces. This is more efficient because X is only accessed once. General query optimization is infeasible. Without this restriction  , transducers can be used for example to implement arbitrary iterative deconstructors or Turing machines. for each distinct value combination of all the possible run-time parameters. In principle  , the optimal plan generated by parametric query optimization may be different. Optimization of this query plan presents further difficulties. The DSMS performs only one instance of an operation on a server node with fewer power  , CPU  , and storage constraints. medium-or coarse-grained locking  , limited support for queries  , views  , constraints  , and triggers  , and weak subsets of SQL with limited query optimization. Many provide limited transaction facilities e.g. First  , expressing the " nesting " predicate .. Kim argued that query 2 was in a better form for optimization  , because it allows the optimizer to consider more strategies. The advantages of STAR-based query optimization are detailed in Loh87. In this example  , TableAccess has only two alternative definitions  , while TableScan has only three. Perhaps surprisingly  , transaction rates are not problematic. We used the same computer for all retrieval experiments. Using conditional compilation allows the compiler freedom to produce the most efficient code for each query optimization technique. 33  proposed an optimization strategy for query expansion methods that are based on term similarities such as those computed based on WordNet. al. In this section we evaluate the performance of the DARQ query engine. In this case DARQ has few possibilities to improve performance by optimization. The optimization of the query of Figure 1illustrated this. Inferred secondary orderings or groupings can be used to infer new primary orderings or groupings. Section 7 presents our conclusions  , a comparison with related work  , and some directions for future research. Section 6 compares query optimization strategies  , transformationfree with SA and II. The top layer consists of the optimizer/query compiler component. The knowledge gamed in performance tests can subsequently be built into optimization rules. The solution to this problem also has applications in " traditional " query optimization MA83 ,UL82. Database snapshots are another example of stored  , derived relations ALgO. But  , to our best knowledge  , no commercial RDBMS covers all major aspects of the AP technology. Some RDBMSs have means to associate optimization hints with a query without any modification of the query text. Fernandez and Dan Suciu 13 propose two query optimization techniques to rewrite a given regular path expression into another query that reduces the scope of navigation. They address the issue of equivalence decidability of regular path queries under such constraints. This query is optimized to improve execution; currently  , TinyDB only considers the order of selection predicates during optimization as the existing version does not support joins. The query is input on the user's PC  , or basestation.  For non-recursive data  , DTD-based optimizations can remove all DupElim and hash-based operators. Optimization of query plans using query information improves the performance of all alternatives  , and the addition of DTD-based optimizations improves them further. But  , the choice of right index structures was crucial for efficient query execution over large databases. Since query execution and optimization techniques were far more advanced  , DBAs could no longer rely on a simplistic model of the engine. For the purposes of this example we assume that there is a need to test code changes in the optimization rules framework. The query optimizer makes use of transformation rules which create the search space of query plan alternatives. It is important to point out their connection since semantic query optimization has largely been ignored in view maintenance literature. This is different from  , but related to  , the use of constraints in the area of semantic query optimiza- tion CGM88. The stratum approach does not depend on a particular XQuery engine. The advantage of this approach is that we can exploit the existing techniques in an XQuery engine such as the query optimization and query evaluation. Database queries are optimized based on cost models that calculate costs for query plans. , s ,} The problem of parametric query optimization is to find the parametric optimal set of plans and the region of optimality for each parametric optimal plan. Lack of Strategies for Applying Possibly Overlapping Optimization Techniques. Or for an XQuery that has nested subqueries  , a failed pattern in the inner query should not affect the computations in the outer query discussed more in Section 3.1. The query is interesting because it produces an intermediate result 1676942 facts that is orders of magnitude larger than the final results 888 facts. Note that the query is not optimized consecutively otherwise it is no different from existing techniques. The effect is equivalent to that of optimizing the query using a long optimization time. Learning can also be performed with databases containing noisy data and excep tional cases using database statistics. TTnfortllllat.ely  , query optimization of spatial data is different from that of heterogeneous databases because of the cost function. database systems e.g. , Dayal  , 19841 appears t ,o be ap plicahle to spatial query opt ,imizat.ion. That means the in memory operation account for significant part in the evaluation cost and requires further work for optimization. That effect is more considerable for the first query since that query will use larger memory. This is an open question and may require further research. Although this will eliminate the need for a probe query  , the dynamic nature of the switch operator provides only dynamic statistics which makes further query optimization very difficult. The Periscope/SQ optimizer rewrites this query using the algebraic properties of PiQA and cost estimates for different plans. Optimization is done by evaluating query fimess after each round of mutations and selecting the " most fit " to continue to the next generation. It then modifies queries by randomly adding or deleting query terms. The resulting megaplan is stored for subsequent execution by an extended execution engine. The rule/goal graph approach does not take advantage of existing DBMS optimization. Our aim is to eliminate this limitation by " normalixing " the query to keep only semantic information that is tmessay to evaluate the query. To select query terms  , the document frequencies of terms must be established to compute idf s before signature file access. When one uses the query term selection optimization  , the character-based signature file generates another problem. In the current version of IRO-DB  , the query optimizer applies simple heuristics to detach subqueries that are sent to the participating systems. Routines within Kleisli manage optimization  , query evaluation  , and I/O from remote and local data sources. CPL is implemented on top of an extensible query system called Kleisli2  , which is written entirely in ML 19.  the query optimization problem under the assumption that each call to a conjunctive solver has unit cost and that the only set operation allowed is union. In this case  , one could actually employ the following query plan: Most important is the development of effective and realistic cost functions for inductive query evaluation and their use in query optimization. Nevertheless  , there are many remaining opportunities for further research. We use a popular LDC shingle dataset to perform two optimizations. However  , we believe that the optimization of native SPARQL query engines is  , nevertheless   , an important issue for an efficient query evaluation on the Semantic Web. Clearly  , main memory graph implementations do not scale. Thus  , optimization may reduce the space requirements to Se114 of the nonoptimized case  , where Se1 is the selectivity factor of the query. In addition  , entries need only be made for tuples within the selectivity range of the query. the resulting query plan can be cached and re-used exactly the way conventional query plans are cached. Note that during optimization only the support structures are set up  , i.e. Those benefits are limited  , as in any other software technology  , by theoretical results.  Order-Preserving Degree OPD: This metric is tailored to query optimization and measures how well Comet preserves the ordering of query costs. Over-costing good plans is less of a concern in practice. We continue with another iteration of query optimization and data allocation to see if a better solution can be found. Apers and is optimal  , given the existing query strategies. While we do have some existing solutions  , these are topics that we are currently exploring further. The X-axis shows the number of levels of nesting in each query  , while the Y-axis shows the query execution time. The results with and without the pipelining optimization are shown in Figure 17. As these methods do not pre-compile the queries  , they generate call loops to the DBMS which are rather inefficient. 4  , 5 proposed using statistics on query expressions to facilitate query optimization. 15 only considers numeric attributes and selection on a single relation  , while our method needs to handle arbitrary attributes and multiple relations. This capability is crucial for many different data management tasks such as data modeling   , data integration  , query formulation  , query optimization  , and indexing. We have described GORDIAN  , a novel technique for efficiently identifying all composite keys in a dataset. Likewise query rewrite and optimization is more complex for XML queries than for relational queries. However  , deciding whether a given index is eligible to evaluate a specific query predicate is much harder for XML indexes than for relational indexes. We also briefly discuss how the expand operator can be used in query optimization when there are relations with many duplicates. We pick the Starburst query optimizer PHH92 and mention how and where our transformations can be used. Therefore  , many queries execute selection operations on the base relations before executing other  , more complex operations. In 13   , the query containment problem under functional dependencies and inclusion dependencies is studied. There has also been a lot of work on the use of constraints in query optimization of relational queries 7  , 13  , 25. In 22   , a scheme for utilizing semantic integrity constraints in query optimization  , using a graph theoretic approach  , is presented. Users do not have to possess knowledge about the database semantics  , and the query optimieer takes this knowledge into account to generate Semantic query optimization is another form of automated programming. Thus  , cost functions used by II heavily influence what remote servers i.e. However  , database systems provide many query optimization features  , thereby contributing positively to query response time. Compared with database based stores  , native stores greatly reduce the load and update time. The proposed method yielded two major innovations: inclusive query planning  , and query optimization. ACKNOWLEDGMENTS I am grateful to my supervisor Kalervo J~velin  , and to the FIRE group: Heikki Keskustalo  , Jaana Kekiilainen  , and others. We can see that the transformation times for optimized queries increase with query complexity from around 300 ms to 2800ms. shows the time needed for query planning and optimization transformation time. To reduce execution costs we introduced basic query optimization for SPARQL queries. Using service descriptions provides a powerful way to dynamically add and remove endpoints to the query engine in a manner that is completely transparent to the user. In query optimization mode  , BHUNT automatically partitions the data into " normal " data and " exception " data. We focus here on the direct use of discovered constraints by the query optimizer. Relational query optimization  , however  , impacts XQuery semantics and introduces new challenges. SQL Server 2005 also introduces optimizations for document order by eliminating sort operations on ordered sets and document hierarchy  , and query tree rewrites using XML schema information. In their relational test implementation they also consider only selection and join. On the other hand  , database systems provide many query optimization features  , thereby contributing positively to query response time. Compared with DBMS based systems Minerva and DLDB  , it greatly reduced the load time. We have generalized the notion of convex sets or version spaces to represent sets of higher dimensions. This includes the grouping specified by the group by clause of the query  , if any exists.  A thread added to lock one of the two involved tables If the data race happens  , the second query will use old value in query cache and return wrong value while not aware of the concurrent insert from another client. Set special query cache flags. The query cache is a common optimization for database server to cache previous query re- sults. Our experiments show that query-log alone is often inadequate  , combining query-logs  , web tables and transitivity in a principled global optimization achieves the best performance. Second  , the query-expansion feature used is in fact often derived from query co-clicks 13   , thus similar to our query log based positive signals. The well-known inherent costs of query optimization are compounded by the fact that a query submitted to the database system is typically optimized afresh  , providing no opportunity to amortize these overheads over prior optimizations . Concurrently  , the query feature vector is stored in the Query Cluster Database  , as a new cluster representative. The inclusive query planning idea is easier to exploit since its outcome  , the representation of the available query tuning space  , can also be exploited in experiments on best-match IR systems. The query optimization operation in the proposed form is restricted to the Boolean IR model since it presumes that the query results are distinct sets. But in parametric query optimization  , we need to handle cost functions in place of costs  , and keep track of multiple plans  , along with their regions of optimality  , for each query/subexpression. In a conventional optimizer we have a single value as the cost for an operation or a plan and a single optimal plan for a query/sub-query expression. At query execution time  , when the actual parameter values are known  , an appropriate plan can be chosen from the set of candidates  , which can be much faster than reoptimizing the query. For each relation in a query  , we record one possible transmission between the relation and the site of every other relation in the query  , and an additional transmission to the query site. This approach recognizes the interdependencies between the data allocation and query optimization problems  , and the characteristics of local optimum solutions. Query compilation produces a single query plan for both relational and XML data accesses  , and the overall query tree is optimized as a whole.  Set special query cache flags. The query cache is a common optimization for database server to cache previous query re- sults. This bug corresponds to mysqld-1 in Table 3  Enable the concurrent_insert=1 to allow concurrent insertion when other query operations to the same table are still pending. The query optimizer shuffles operators around in the query tree to produce a faster execution plan  , which may evaluate different parts of the query plan in any order considered to be correct from the relational viewpoint. As the accuracy of any query optimizer is dependent on the accuracy of its statistics  , for this application we need to accurately estimate both the segment and overall result selectivities. We develop a query optimization framework to allow an optimizer to choose the optimal query plan based on the incoming query and data characteristics. To control the join methods used in the query plans  , each plan was hand-generated and then run using the Starburst query execution driver. Figure 5shows the DAG that results from binary scoring assuming independent predicate scoring for the idf scores of the query in Figure 3. A simple way to implement this optimization is to convert the original query into a binary predicate query  , and build the relaxation DAG from this transformed query. Hence the discussion here outlines techniques that allow us to apply optimizations to more queries. Note that our optimization techniques will never generate an incorrect query — they will either not apply in which case we will generate the naive query or they will apply and will generate a query expected to be more efficient than the naive query. The conventional approach to query optimization is to examine each query in isolation and select the execution plan with the minimal cost based on some predcfincd cost flmction of I0 and CPU requirements to execute the query S&79. One thus needs to consider all query types together. In this section  , we discuss how the methods discussed to up to this point extend to more general situations. If alternative QGM representations are plausible depending upon their estimated cost  , then all such alternative QGMs are passed to Plan Optimization to be evaluated  , joined by a CHOOSE operator which instructs the optimizer to pick the least-cost alternative. QGM Optimization then makes semantic transformations to the QGM  , using a distinct set of sophisticated rewrite rules that transform the QGM query into a " better " one  , i.e. , one that is more efficient and/or allows more more leeway during Plan Optimization . Further  , the benefits of " plan hints "   , a common technique for influencing optimizer plan choices for specific queries  , automatically percolate to the entire set of queries that are associated with this plan. The optimization prohlem then uses the response time from the queueing model to solve for an improved solution. The Iirst part is the optimization just dcscrihcd which uses an assumed response time for each query type  , and the second part is a queueing model to solve for the rcsponse t.ime based on the access plan selections and buf ?%r allocation from the first part the optimization prohlcm. Yet another important advantage is that the benefits of " plan hints "   , a common technique for influencing optimizer plan choices for specific queries  , automatically percolate to the entire set of queries that are associated with this plan. As optimizers based on bottom-up Zou97  , HK+97  , JMP97 and top-down Ce96  , Gra96 search strategies are both extensible Lo88  , Gra95 and in addition the most frequently used in commercial DBMSs  , we have concentrated our research on the suitability of these two techniques for parallel query optimization. To overcome the shortcomings of each optimization strategy in combination with certain query types  , also hybrid optimizers have been proposed ON+95  , MB+96. In Tables 8 and 9 we do not see any improvement in preclslon at low recall as the optimization becomes more aggressive. Second  , at high recall  , precision becomes significant y worse as the optimization becomes more aggressive  , This is because we are not considering documents which have a strong combined belief from all of the query terms  , but lack a single query term belief strong enough to place the document in the candidate set. Once we have added appropriate indexes and statistics to our graph-based data model  , optimizing the navigational path expressions that form the basis of our query language does resemble the optimization problem for path expressions in object-oriented database systems  , and even to some extent the join optimization problem in relational systems. Statistics describing the " shape " of a data graph are crucial for determining which methods of graph traversal are optimal for a given query and database. In this paper  , we present a new architecture for query optimization  , based on a blackbonrd xpprowh  , which facilitates-in combination with a building block  , bottom-up arrscrnbling approach and early aqxeasiruc~~l. The search strategy-also proposed for multi query optimization 25-that will be applied in our sample optimizer is a slight modification of A*  , a search technique which  , in its pure form  , guarantaes to find the opt ,irnal solution 'LO. Although catalog management schemes are of great practical importance with respect to the site auton- omy 14  , query optimization 15  , view management l  , authorization mechanism 22   , and data distribution transparency 13  , the performance comparison of various catalog management schemes has received relatively little attention 3  , 181. Using the QGM representation of the query as input  , Plan Optimization then generates and models the cost of alternative plans  , where each plan is a procedural sequence of LOLEPOPs for executing the query. Ignoring optimization cost is no longer reasonable if the space of all possible execution plans is very large as those encountered in SQOS as well as in optimization of queries with a large number of joins. Conventional query optimizers assume that the first part is negligible compared to the second  , and they try to minimize only the execution cost instead of the total query evaluation cost. Some of the issues to consider are: isolation levels repeatable read  , dirty read  , cursor stability  , access path selection table scan  , index scan  , index AND/ORing MHWC90  , Commit_LSN optimization Mohan90b  , locking granularity record  , page  , table  , and high concurrency as a query optimization criterion. While it is sometimes merely a performance advantage to take such an integrated view  , at other times even the correctness of query executions depends on such an approach. The optimization techniques being currently implemented in our system are : the rewriting of the FT 0 words into RT o   , a generalization of query modification in order to minimize the number of transitions appearing in the query PCN  , the transformation of a set of database updates into an optimized one as SellisgS does  , and the " push-up " of the selections. -The optimizer can use the broad body of knowledge developed for the optimization of relational calculus and relational algebra queries see  JaKo85  for a survey and further literature. Therefore defining the semantics of an SQL query by translation into relational algebra and relational calculus opens up new optimization oppor- tunities: -The optimizer can investigate the whole query and is no longer constrained to look at one subquery at a time. second optimization in conjunction with uces the plan search space by using cost-based heuristics. Que TwigS TwigStack/PRIX from 28  , 29 / ToXinScan vs. X that characterize the ce of an XML query optimizer that takes conjunction with two summary pruning ugmented with data r provides similar se of system catalog information in optimization strategy  ,   , which reduces space by identifying at contain the query a that suggest that  , can easily yield ude. Let us mathematically formulate the problem of multi-objective optimization in database retrieval and then consider typical sample applications for information systems: Multi-objective Retrieval: Given a database between price  , efficiency and quality of certain products have to be assessed  Personal preferences of users requesting a Web service for a complex task have to be evaluated to select most appropriate services Also in the field of databases and query optimization such optimization problems often occur like in 22 for the choice of query plans given different execution costs and latencies or in 19 for choosing data sources with optimized information quality. The optimization problem can be solved by employing existing optimization techniques  , the computation details of which  , though tedious  , are rather standard and will not be presented here. Note that we can use different feature sets for different query topics by using this method  , but for simplicity  , we didn't try it in this work. It is not our goal in this paper to analyze optimization techniques for on-disk models and  , hence  , we are not going to compare inmemory and on-disk models. Queries over Changing Attributes -The attributes involved in optimization queries can vary based on the iteration of the query. In a case where we want half of the participants to be male and half female  , we can adjust weights of the objective optimization function to increase the likelihood that future trial candidates will match the currently underrepresented gender. In this paper we present a general framework to model optimization queries. This makes the framework appropriate for applications and domains where a number of different functions are being optimized or when optimization is being performed over different constrained regions and the exact query parameters are not known in advance. As such  , the framework can be used to measure page access performance associated with using different indexes and index types to answer certain classes of optimization queries  , in order to determine which structures can most effectively answer the optimization query type. However  , if the optimal contour crosses many partitions  , the performance will not be as good. Each query was executed in three ways: i using a relational database to store the Web graph  , ii using the S-Node representation but without optimization  , and iii using S- Node with cluster-based optimization. To generate Figure 12b  , we executed a suite of 30 Web queries over 5 different 20-million page data sets. The purpose of this example is not to define new optimization heuristics or propose new optimization strategies. In this section we give a design for a simple query rewrite system to illustrate the capabilities of the Epoq architecture and  , in particular  , to illustrate the planning-based control that will be presented in Section 5. Formulation A There are 171 separate optimization problems  , each one identical to the traditional  , nonparametric case with a different F vector: VP E  ?r find SO E S s.t. In general  , for every plan function s  , 7 can be partiof parametric query optimization. However  , the discussion of optimization using a functional or text index is beyond the scope of this paper. For an XML input whose structure is opaque  , the user can still use a functional index or a text index to do query optimization. The leftmost point is for pure IPC and the rightmost for pure OptPFD. In fact  , this hybrid index optimization problem motivated the optimization problem underlying the size/speed tradeoff for OptPFD in Figure 2per query in milliseconds  , for a hybrid index involving OptPFD and IPC. In this paper we proposed a general framework for expressing and analyzing approximate predicates  , and we described how to construct alternate query plans that effectively use the approximate predicates. In addition to considering when such views are usable in evaluating a query  , they suggest how to perform this optimization in a cost-based fashion. Therefore  , we need to find a priori which tables in the FROM clause will be replaced by V. Optimization of conjunctive SQL queries using conjunctive views has been studied in CKPS95. The results also shows how our conservative local heuristic sharply reduces the overhead of optimization under varying distributions. The second set of experiments shed light on how the distribution of the user-defined predicates among relations in the query influences the cost of optimization. However   , the materialized views considered by all of the above works are traditional views expressed in SQL. Optimization using materialized views is a popular and useful technique in the context of traditional database query optimization BLT86  , GMS93  , CKPS95  , LMSS95  , SDJL96 which has been successfully applied for optimizing data warehouse queries GHQ95  , HGW + 95  , H R U96  , GM96  , GHRU97. Note that even our recipes that do not exploit this optimization outperform the optimized VTK program and the optimized SQL query. The bars labelled with the 'o' suffix make use of a semantic optimization: We restrict the grid to the relevant region before searching for cells that contain points. Some of the papers on query evaluation mentioned in section 4.2 consider this problem. It is an interesting optimization problem to decide which domains to invert a static optimization and how to best evaluate the qualification given that only some of the domains are inverted. Concerning query optimization  , existing approaches  , such as predicate pushdown U1188 and pullup HS93  , He194  , early and late aggregation c.f. , YL94  , duplicate elimination removal PL94  , and DISTINCT pullup and pushdown  , should be applied to coalescing. In terms of future research  , more work is needed to understand the interplay of coalescing and other temporal operators with respect to queSy optimization and evaluation. Putting these together   , the ADT-method approach is unable to apply optimization techniques that could result in overall performance improvements of approximately two orders of magnitude! Traditional query optimization uses an enumerative search strategy which considers most of the points in the solution space  , but tries to reduce the solution space by applying heuristics. As the solution space gets larger for complex queries  , the search strategy that investigates alternative solutions is critical for the optimization cost. We have chosen not do use dynamic optimization to avoid high overhead of optimization at runtime. one for each resolvent of a late bound function  , and where the total query plan is generated at start-up time of the application program. Our approach exploits knowledge from different areas and customizes these known concepts to the needs of the object-oriented data models. Using a realistic application  , we measure the impact of parallelism on the optimization cost and the op- timization/execution cost trade-off using several combinations of search space and search strategy. In this way  , after two optimization calls we obtain both the best hypothetical plan when all possible indexes are present and the best " executable " plan that only uses available indexes. For that reason  , we would require a second optimization of the query  , this time using only the existing indexes. We describe our evaluation below  , including the platform on which we ran our experiments  , the test collections and query sets used  , the performance measured. Any evaluation of an unsafe optimization technique requmes measuring the execution speeds of the base and optimized systems  , as well as assessing the impact of the optimization technique on the system's retrieval effectiveness. In the following  , we focus on such an instantiation   , namely we employ as optimization goal the coverage of all query terms by the retrieved expert group. Obviously  , by defining a specific optimization goal  , we get different instantiations of the framework  , which correspond to different problem statements. In this optimization  , we transform the QTree itself. Our ideas are implemented in the DB2 family. We performed experiments to 1 validate our design choices in the physical implementation and 2 to determine whether algebraic optimization techniques could improve performance over more traditional solutions. Since the execution space is the union of the exccution spaces of the equivalent queries  , we can obtain the following simple extension to the optimization al- gorithm: 1. Thus  , the ecectllion space consists of the space of all join trees* for each equivalent query obtainrtl from Step 1 of optimization Section 4. Although this approach is effective in the database domain  , unfortunately  , in knowledge base systems this is not feasible. This is necessary to allow for both extensibility and the leverage of a large body of related earlier work done by the database research community. Such machinery needs to be based on intermediate representations of queries that are syntactically close to XQuery and has to allow for an algebraic approach to query optimization  , with buffering as an optimization target. The second issue  , the optimization of virtual graph patterns inside an IMPRECISE clause  , can be addressed with similarity indexes to cache repeated similarity computations—an issue which we have not addressed so far. The first issue can be addressed with iSPARQL query optimization  , which we investigated in 2 ,22. The goal is to keep the number of records Note that optimizing a query by transforming one boolean qualification into another one is a dynamic optimization that should be done in the user-to- LSL translator. For instance: with 4 levels  , the corresponding SEQUIN query is PROJECT count* FROM PROJECT * FROM PROJECT * FROM 100K~10flds~100dens , S; ZOOM ALL; We disabled the SEQ optimization that merges consecutive scans which would otherwise reduce all these queries to a common form. Besides these works on optimizer architectures  , optimization strategies for both traditional and " nextgeneration " database systems are being developed. The technique provides optimization of arbitrary convex functions  , and does not incur a significant penalty in order to provide this generality. In summary  , navigation profiles offer significant opportunities for optimization of query execution  , regardless of whether the XML view is defined by a standard or by the application. We argue that complex view queries contain many such tradeoffs; balancing them is part of the optimization space explored by ROLEX. This example illustrates the applicability of algebraic query optimization to real scientific computations  , and shows that significant performance improvements can result from optimization. Finally  , the reduction in the number of merge operations from 3 to 2 results in less copying of data  , and thus better performance. However  , this only covers a special case of grouping  , as we will discuss in some detail in Section 3. Parallel optimization is made difficult by the necessary trade-off between optimization cost and quality of the generated plans the latter translates into query execution cost. To copy otherwire  , or to republish  , requires a fee and/or rpecial permirrion from Ihe Endowment. In Section 3  , we show how our query and optimization engine are used in BBQ to answer a number of SQL queries  , 2 Though these initial observations do consume some energy up-front  , we will show that the long-run energy savings obtained from using a model will be much more significant. We discuss this optimization problem in more detail in Section 4. The basic idea behind our approach is similar in spirit to the one proposed by Hammcr5 and KingS for knowledge-based query optimization  , in the sense that we are also looking for optimization by semantic transformation. Finally  , Hammer only supports restricted forms of logically equivalent transformations because his knowledge reprsentation is not suitable for deductive use. Other types of optimizations such as materialized view selection or multi-query optimization are orthogonal to scan-related performance improvements and are not examined in this paper. Exactly this type of optimization lies in the heart of a read-optimized DB design and comprises the focus of this paper. The horizontal optimization specializes the case rules of a typeswitch expression with respect to the possible types of the operand expression. The structural function inlining yields an optimal expression for a given query by means of two kinds of static optimization  , which are horizontal and vertical optimizations. This optimization problem is NP-hard  , which can be proved by a reduction from the Multiway Cut problem 3 . Then the optimization target becomes F = arg max F ∈F lF  , where F is the set of all possible query facet sets that can be generated from L with the strict partitioning constraint. What differentiates MVPP optimization with traditional heuristic query optimization is that in an MVPP several queries can share some After each MVPP is derived  , we have to optimize it by pushing down the select and project operations as far as possible. The relation elimination proposed by Shenoy and Ozsoyoglu SO87 and the elimination of an unnecessary join described by Sun and Yu SY94 are very similar to the one that we use in our transformations. 11 ,12 a lot of research on query optimization in the context of databases and federated information systems. The introduction of an ER schema for the database improves the optimization that can be performed on GraphLog queries for example  , by exploiting functional dependencies as suggested in 25  , This means that the engineer can concentrate on the correct formulation of the query and rely on automatic optimization techniques to make it execute efficiently. For practical reasons we limited the scalability and optimization research to full text information re-trieval IR  , but we intend to extent the facilities to full fledged multimedia support. Distribution and query optimization are the typical database means to achieve this. This gives the opportunity of performing an individual  , " customized " optimization for both streams. The bypass technique fills the gap between the achievements of traditional query optimization and the theoretical potential   , In this technique  , specialized operators are employed that yield the tuples that fulfll the operator's predicate and the tuples that do not on two different  , disjoint output streams. This study has also been motivated by recent results on flexible buffer allocation NFSSl  , FNSSl. The first and simplest level is trying RaPiD7 out according to the general idea of RaPiD7. Three different levels of achievement can be perceived in implementing RaPiD7. Typically  , the teams being unsuccessful in applying RaPiD7 have not received any training on RaPiD7  , and therefore the method has not been applied systematically enough. Negative experiences in using RaPiD7 exist  , too. Furthermore  , JAD sessions are always somewhat formal  , whereas RaPiD7 sessions vary in formality depending on the case. The goal in RaPiD7 is to benefit the whole project by creating as many of the documents as possible using RaPiD7. Other approaches similar to RaPiD7 exist  , too. The following lists the key differences identified between RaPiD7 and JAD: However  , RaPiD7 is not focusing on certain artifacts or phases of software development  , and actually does not state which kind of documents or artifacts could be produced using the method  , but leaves this to the practitioner of the method. The obvious similarity with RaPiD7 is the idea of having well structured meetings in RaPiD7 called workshops in order to work out system details. JAD provides many guidelines for the pre-session work and for the actual session itself  , but the planning is not step based  , as is the case with RaPiD7. Different JAD sessions are not said to be alike 6  , and while this is true for RaPiD7 too  , the way RaPiD7 workshops and JAD sessions are planned is different. An acceptable level of quality in the documentation can be reached in a rather short time frame using a method called RaPiD7 Rapid Production of Documents  , 7 steps. The steps of RaPiD7 method are presented in figure 1. The following lists the key differences identified between RaPiD7 and JAD: JAD provides many guidelines for the pre-session work and for the actual session itself  , but the planning is not step based  , as is the case with RaPiD7. After all  , if projects are planned according to RaPiD7 methodology there will be a number of workshops to participate in. In the following  , two approaches  , namely JAD and Agile modeling  , are discussed shortly in terms of main similarities and differences with RaPiD7. Furthermore  , RaPiD7 is characterized by the starting point of its development; problems realizing in inspections. Although the methods resemble each other in many ways  , the differences are evident. This can be perceived from results already. For the teams applying RaPiD7 systematically the reward is  , however  , significant. On the other hand  , formal RaPiD7 workshops and JAD sessions can be quite alike. This is probably why more efforts are put into the preparation work when using JAD  , and why with JAD the typical " from preparation to a finished document -time " is longer than with RaPiD7. Now hundreds of cases exist in Nokia where different artifacts and documents have been authored using RaPiD7 method. In addition  , more work was put into developing the method and training RaPiD7 coaches that could independently take the method into use in their projects. Joint application development JAD is a requirements-definition and user-interface design methodology according to Steve McConnell 4. Although it might be difficult to get people to change their ways of doing everyday work  , typically the teams trying out RaPiD7 for some time would not give up using it. On the other hand  , it is apparent that to fully benefit from RaPiD7 training is required  , too. In JAD  , the general idea is to have a workshop or a set of workshops rather than having unlimited number of workshops throughout the project. This means in practice that a person uses approximately a day to finalize the work. Specifically  , I would like to name some key people making RaPiD7 use reality. Without the users the method would merely be a theory. The idea behind the method is relatively simple  , but the effective use of it is not. The people who would traditionally participate the inspections are the people who will participate the RaPiD7 workshops  , too. Typically  , authoring a document takes less than a week in calendar time. The last and final level is to utilize RaPiD7 in a full-scale software project  , and plan the documentation authoring in projects by scheduling consecutive workshops. To enable this some training is typically needed. On the other hand  , there is a clear and valid reason for the aforementioned hesitancy for the applicability of agile modeling. RaPiD7 has been developed and used in Nokia  , which can be referred to as being a large telecommunications company. The deployment of the method would not have taken place without contribution from Nokia management. Without the efforts of these users we would not have such good results nor would we have RaPiD7 as an institutionalized way of working. The cases differ by the time required  , the people participating the workshops and the techniques used in the workshops. The way RaPiD7 is applied varies significantly depending on the case. The key contributors in developing the method itself have been Riku Kylmäkoski  , Oula Heikkinen  , Katherine Rose and Hanna Turunen. The workshops are well prepared  , and innovative brainstorming and problem solving methods are used. Simply put  , RaPiD7 is a method in which the document in hand is authored in a team in consecutive workshops. Differences are related to the goals of the methods and the scope of using the methods in software development projects. Hundreds of people have been involved in making RaPiD7 as a working practice in Nokia. Finally  , I would like to thank Tuomas Lamminpää  , Kai Koskimies and Ilkka Haikala for giving solid contribution by reviewing this paper several times. However  , it suits best for documents that are not product-like in nature. By using RaPiD7 method  , the following benefits are expected to realize:  Artifacts and specifications will be produced in a relatively short time from couple of days to one week  Inspecting the documents will not be typically needed after the document has been authored in a workshop  Communication in projects will be easier and more effective  People can work more flexibly in teams as they all share the same information  The overall quality of artifacts and specifications will be improved  No re-work is needed and hence time is saved  Schedules for workshops in projects are known early enough to plan traveling efficiently  , and thus costs can be reduced 3URFHHGLQJVVRIIWKHWKK ,QWHUQDWLRQDO&RQIHUHQFHHRQ6RIWZDUHHQJLQHHULQJJ ,&6 ¶  , Workshop n. Finished Figure 2  , Creating a document using RaPiD7 RaPiD7 method can be applied for authoring nearly all types of documents. Invitation Figure 1  , Steps of RaPiD7 1 Preparation step is performed for each of the workshops  , and the idea is to find out the necessary information to be used as input in the workshops. Therefore  , the key issue seems to be getting the teams to try out RaPiD7 long enough to see the benefits realizing. These results indicate that higher use rate will give better results in terms of improved communication  , authoring efficiency and defect rate reduction. Furthermore  , I would like to thank the pilot users and teams in Nokia  , especially I would like to thank Stephan Irrgang  , Roland Meyer  , Thomas Wirtz  , Juha Yli-Olli and Miia Forssell. In addition  , agile modeling does not provide ways to plan the modeling sessions in your software projects whereas in JAD and RaPiD7 the planning is seen crucial for success. Even if you could hire only " good developers "   , as Ambler suggests for effective formation of an agile modeling team  , in a large company these good developers will still have different backgrounds and knowledge base. The first workshops  , when trying to find out the right approach for a specific document type  , are the most difficult ones. It is no surprise that these different methods provide and promote similar kind of techniques for effective documentation work. The results from the initial workshops were encouraging and the method was taken into use in several other teams  , too. It is also important to make sure that people participate the workshops only as long as their input is needed  , in order to minimize the idle time of participants. This usually requires approximately two to three days of work for the first workshop  , and a few hours for the following workshops. On the other hand  , agile modeling provides a number of pragmatic ideas how to perform agile modeling sessions to produce certain kind of models. However  , agile modeling does not provide a cookbook type of approach for authoring documents  , as RaPiD7 does. These properties are considered as random influence. quality of indexing  , or of relevance judgement influencing the retrieval outputs 1 ,18. The requirement for random access can be accommodated with conventional indexing or hashing methods. The databases are relatively small. One method  , the VP-tree 36  , partitions the data space into spherical cuts by selecting random reference points from the data. Reference-based indexing 7  , 11  , 17  , 36  can be considered as a variation of vector space indexing. With regard to the unexpectedness of the highly relevant results relevancy>=4 Random indexing outperforms the other systems  , however hyProximity offers a slightly more unexpected suggestions if we consider only the most relevant results relevan- cy=5. While hyProximity scores best considering the general relevance of suggestions in isolation  , Random Indexing scores best in terms of unexpectedness. It offers a scalable approach to the construction of document signatures by applying random indexing 30  , or random projections 3 and numeric quantization. We use a binary signature representation called TopSig 3 18. Random " subsequent queries are submitted to the library  , and the retrieved documents are collected. average indexing weights  , document frequencies  automatically in non-co-operating environments 1. " Recently  , several approaches have been developed for selecting references for reference-based indexing 11  , 17. bound3 is the bound obtained using a random point rand inside the hull. 9 proposed a block-based index to improve retrieval speed by reducing random accesses to posting lists. In the area of indexing and retrieval  , Bast et al. The key of most techniques is to exploit random projection to tackle the curse of dimensionality issue  , such as Locality-Sensitive Hashing LSH 20   , a very well-known and highly successful technique in this area. Instead of solving the exact similarity search for high dimensional indexing  , recent years have witnessed active studies of approximate high-dimensional indexing techniques 20  , 14  , 25  , 3  , 8  , 11. Finally  , comparing the different reaulta for 11 and A1 in table -4  , it can be aeen that indexing A1 provides better retrieval results than 11. weight 0 random ord. Hence  , in the DocSpace the similarity between documents is computed by the traditional cosine similarity. To build the DocSpace  , Semantic Vectors rely on a technique called Random Indexing 4  , which performs a matrix reduction of the term-document matrix. Users also indicated that Random Indexing provided more general suggestions  , while those provided by hyProximity were more granular. The unexpectedness of the most relevant results was also higher with the Linked Data-based measures. To simulate the distributed environment  , the documents were allocated into 32 different databases using a random allocator with replication. The experimental results were achieved by indexing 1991 WSJ documents TREC disk 22 with Webtrieve using stemming and stopwords remotion. As shown in Table 2  , the Linked Data measures outperform the baseline system across all criteria. This demonstrates the real ability of Linked Data-based systems to provide the user with valuable relevant concepts. The difference in unexpectedness is significant only in the case of Random Indexing vs. baseline. With regard to recall  , Random Indexing outperforms the other approaches for 200 top-ranked suggestions. This measure should therefore be used in the end-user applications  , as the users can typically consult only a limited number of top-ranked suggestions. Our indexing structure simply consists of l such LSH Trees  , each constructed with an independently drawn random sequence of hash functions from H. We call this collection of l trees the LSH Forest. We call this tree the LSH Tree. The two most important exceptions that require special attention are historical data support and geometric modellii. HyProximity measures improve the baseline across all performance measures  , while Random indexing improves it only with regard to recall and F-measure for less than 200 suggestions. After this threshold the mixed hyProximity is a better choice. Then  , the distribution of the scores of all documents in a library is modelled by the random variable To derive the document score distribution in step 2  , we can view the indexing weights of term t in all documents in a library as a random variable X t . It is especially useful in cases when it is possible to consider a large number of suggestions which include false positives -such as the case when the keyword suggestions are used for expert crawling. It seems clear that patlems occurring in random indexing can be profitably exploited  , and surprisingly quickly. This pattern is revealed tnost strongly by the mattix of retrieval weights  , which in all cases correctly relate documents to requests in agreement with our relevance assumptions. We combine two retrieval strategies that work at two different To compute the inter-document similarities we build a vector space DocSpace where similar documents are represented by close vectors by means of the Semantic Vectors package 13. The gold standard-based evaluation reveals a superior performance of hyProximity in cases where precision is preferred; Random Indexing performed better in case of recall. Our results show that both proposed methods improve the baseline in different ways  , thus suggesting that Linked Data can be a valuable source of knowledge for the task of concept recommendation. The shakwat group University of Paris 8 experimented with a random-walk approach using a space built using semantic indexing  , and containing the blog posts  , as well as the headlines  , in a window around the date of the topic. They did not diversify the ranking of blog posts. Those models are based on the Harris Harris  , 1968 distributional hypothesis  , which states that words that appear in similar context have similar meanings. LSA Landauer and Dumais  , 1997  , Hyper Analog to Language Lund and Burgess  , 1996 and Random Indexing Kanerva et al. , 2000 are some exemplars of Word Vectors. For the chosen innovation problem  , the evaluators were presented with the lists of 30 top-ranked suggestions generated by ad- Words  , hyProximity mixed approach and Random Indexing. This generated a total of 34 problem evaluations  , consisting of 3060 suggested concepts/keywords. saving all the required random edge-sets together during a single scan over the edges of the web graph. Furthermore  , at the end of the indexing the individual fingerprint trees can be collected with sorting and merging operations  , as the longest possible path in each fingerprint tree is due to Lemma 2 the labels are strictly increasing but cannot grow over . On the 99-node cluster  , indexing time for the first English segment of the ClueWeb09 collection ∼50 million pages was 145 minutes averaged over three trials; the fastest and slowest running times differed by less than 10 minutes. In addition  , we expect random access latencies to improve over time as developers continue to improve HDFS. Details of these datasets appear in Appendix A. In addition to this ultra heterogeneous data  , we created a very large database of Random Walk data RW II  , since this is the most studied dataset for indexing comparisons 5  , 6  , 17  , 24  , 25  , 34 and is  , by contrast with the above  , a very homogeneous dataset. The Semantic space method we use in the context of the Blog-Track'09 is Random Indexing RI  , which is not a typical method in the family of Semantic space methods. The construction of a semantic space with RI is as follows: This representation is finally translated into a binary image signature using random indexing for efficient retrieval. Then each sub-image is represented by those visual words from these vocabularies through codebook lookup of each raw image feature and finally the full image feature set is constructed. The significance of differences is confirmed by the T-test for paired values for each two methods p<0.05. We then asked them to rate the relevancy and unexpectedness of suggestions using the above described scales. According to the preference towards more general or more specific concepts  , it is therefore possible to advise the user with regard to which of the two methods is more suitable for the specific use case. HyProximity suggestions were most commonly described as " really interesting " and " OI-oriented "   , while the suggestions of Random Indexing were most often characterized as " very general " . We used Random Indexing 6  to build distributional semantic representations i.e. , vectors of terms from a large corpus of Mayo Clinic clinical notes. While the baseline and previous approaches directly used the text of the queries with stop word removal to search documents  , here we modified the queries. We tested the differences in relevance for all methods using the paired T-test over subjects individual means  , and the tests indicated that the difference in relevance between each pair is significant p <0.05. In addition  , our user study evaluation confirmed the superior performance of Linked Data-based approaches both in terms of relevance and unexpectedness. We took great care to match the SHORE/C++ implementation as closely as possible  , including using the same C library random number generator and initializing it with the same seed so as to generate the same sequence of random numbers used to build the OO7 benchmark database and to drive the benchmark traversals. The PM3 Modula-3 compiler was also invoked with a flag that disables runtime checks on indexing arrays out of bounds and to catch certain type errors  , so as to give a fairer comparison with C++. Query type Q1 of the QUERY test represents a sequence of random proximity queries details below. The trace files were stored on a 7200 RPM SCSI disk whose data transfer rate far exceeded the update performance of the indexing methods  , guaranteeing that the testbed was Update cost  , index size  , and other metrics measured by the LOCUS testbed were collected at an interval of 2500 updates. We present two Linked Data-based methods: 1 a structure-based similarity based solely on exploration of the semantics defined concepts and relations in an RDF graph  , 2 a statistical semantics method  , Random Indexing  , applied to the RDF in order to calculate a structure-based statistical semantics similarity. A concept  , in our context  , is a Linked Data instance  , defined with its URI  , which represents a topic of human interest. Therefore  , starting with S1 document removal  , we began by indexing a random selection of 10% of the documents from the document collection. In each scenario we had 10 indexes for each team member and 55 different access combinations  , although the indexes in S4 are of different size to S1  , S2 and S3 because in S1  , S2 and S3 we can theoretically exclude everything from the collection whereas for S4 this is dependent on the query pool. We show later that the ALSH derived from minhash  , which we call asymmetric minwise hashing MH-ALSH  , is more suitable for indexing set intersection for sparse binary vectors than the existing ALSHs for general inner products. For sparse and high-dimensional binary dataset which are common over the web  , it is known that minhash is typically the preferred choice of hashing over random projection based hash functions 39. In general  , our methods start from a set of Initial/seed Concepts IC  , and provide a ranked list of suggested concepts relevant to IC. Commercial systems like AltaVista Image Search only index the easy-to-see image captions like text-replacement  " ALT "  strings  , achieving good precision accuracy in the images they retrieve but poor recall thoroughness in finding relevant images. Just indexing multimedia through text search engines is quite imprecise; in a random sample we took  , only 1.4% of the text on Web pages with images described those images. For instance  , in a sample of 38720 documents drawn at random from the Online Public Access Catalogue OPAC of the Universitätsbibliothek at Karlsruhe University TH  , 11594 approximately 30% had no keyword  , although the library has the reputation for having the best catalogue in Germany. As of today  , the index quality of catalogues in scientific libraries is deplorable: Large parts of the inventory are not indexed and will probably never be  , since manual indexing is a time-consuming and thus expensive task. The first method called hyProximity  , is a structure-based similarity which explores different strategies based on the semantics inherent in an RDF graph  , while the second one  , Random Indexing  , applies a well-known statistical semantics from Information Retrieval to RDF  , in order to identify the relevant set of both direct and lateral topics. We propose two independently developed methods for topic discovery based on the Linked Data. As the baseline we use the state of the art adWords keyword recommender from Google that finds similar topics based on their distribution in textual corpora and the corpora of search queries. We rst describe  , in the next section  , how collection indexing was performed. One formula we have formally derived and successfully tested on previous TREC collections is: Our term weight w of Formula 2 will be thus a function of 6 random variables: w = wF; tfn; n; N = wF ; t f ; n ; N ; l ; a v g l where l is the document length avg l is the length mean We postpone the discussion about the probability functions used to instantiate this framework and the choice of parameter c to Section 4.2. Considering SAE with k layers  , the first layer will be the autoencoder  , with the training set as the input. A SAE model is a series of autoencoder. A denoising autoencoder DAE is an improvement of the autoencoder  , which is designed to learn more robust features and prevent the autoencoder from simply learning the identity. A hidden unit is said to be active or firing if it's output is close to 1 and inactive if it's output is close to 0. 1a  , the autoencoder is trained with native form and its transliterated form together. As shown in Fig. The autoencoder tries to minimize Eq. 2 is the regularization term and λ is the weight decay parameter. Table I also presents some key configurations of the autoencoder . " Multiple " indicates various resolutions used in the global methods. Meanwhile   , other machine learning methods can also reach the accuracy more than 0.83. The stacked autoencoder as our deep learning architecture result in a accuracy of 0.91. The anomaly score is simply defined as autoencoder trains a sparse autoencoder 21 with one hidden layer based on the normalized input as x i ← xi−mini maxi−mini   , where max i and min i are the maximum and minimum values of the i-th variable over the training data  , respectively. However  , the fully connected AE ignores the high dimensionality and spatial structure of an image. The fully connected AE is a basic form of an autoencoder. The autoencoder is still able to discover interesting patterns in the input set. In that case a sparsity constraint is imposed on the hidden units. autoencoder trains a sparse autoencoder 21 with one hidden layer based on the normalized input as x i ← xi−mini maxi−mini   , where max i and min i are the maximum and minimum values of the i-th variable over the training data  , respectively. The same values of ρ and K as GMRFmix are used for the 1 regularization coefficient and U  , respectively. The architecture of the autoencoder is shown Fig. In this way  , the model is able to learn character level " topic " distribution over the features of both scripts jointly. Map Size " denotes to the height and width of the convolutional feature maps to be pooled. " An autoencoder can also have hidden layer whose size is greater than the size of input layer. 2 by gradient descent. We use stacked RBMs to initialize the weights of the encoder we can also optionally further use a deep autoencoder to find a better initialization. Transliteration: http://transliteration.yahoo.com/ x= x q = Figure 1: The architecture of the autoencoder K-500-250-m during a pre-training and b fine-tuning. 4 Yahoo! 6demonstrates the fact that more than 60% of features are zero when the sparsity constraint is utilized in the autoencoder combined with the ReLU activation function. Fig. For fair comparison  , all the methods are conducted on the same convolved feature maps learned by a single-hidden-layer sparse autoencoder with a KL sparse constraint. The global R 2 FP is compared with spatial pyramid pooling SPP. A large number of languages  , including Arabic  , Russian  , and most of the South and South East Asian languages  , are written using indigenous scripts. The mixed-script joint modelling technique using deep autoencoder. The loss function of an autoencoder with a single hidden layer is given by  , The hidden layer gets to learn a compressed representation of the input  , such that the original input can be regenerated from it. From the experimental results   , we can see that SAE model outperforms other machine learning methods. These models utilize the bilingual compositional vector model biCVM of 9 to train a retrieval system based on a bilingual autoencoder. This results in the following regularized hinge-loss objective: Then we fine-tune the weights of the encoder by minimizing the following objective function: We use stacked RBMs to initialize the weights of the encoder we can also optionally further use a deep autoencoder to find a better initialization. The results obtained using the remaining methods are presented in Table 2. The autoencoder was found to be computationally infeasible when applied to the described datasets and therefore its retrieval performance is not presented. Other iterative online methods have been presented for novelty detection  , including the Grow When Required GWR self-organizing map 13 and an autoencoder  , where novelty was characterized by the reconstruction error of a descriptor 14. Previous work 1 approximated the PDF using weighted Parzen windows. However there are a very few extreme rainfall cases compared to normal or no rainfall cases  , that is the data set is biased. The weather parameters are fed to the stacked autoencoder and the reduced feature space is obtained for further classification into extreme and non-extreme events. Post training  , the abstract level representation of the given terms can be obtained as shown in c. Transliteration: http://transliteration.yahoo.com/ x= x q = Figure 1: The architecture of the autoencoder K-500-250-m during a pre-training and b fine-tuning. To avoid simply learning the identity function  , we can require that the number of hidden nodes be less than the number of input nodes  , or we can use a special regularization term. In training  , an autoencoder is given the input x ∈ R n as both training instance and label. Dropout technique is utilized in all the experiments in the hidden layer of the sparse autoencoder and the probability of omitting each neural unit is set as 0.5. WD " denotes the weitht decay term used to constrain the magnitude of the weights connecting each layer. Mean Average Precision MAP and Precision at N P@N  are used to summarise retrieval performance within each category. We regularize the features to be smaller than 1 by dividing the sum of all the selected features. Note that the value of local features may be larger than 1 as the activation function used in the autoencoder is ReLU for better sparsity. Session: LBR Highlights March 5–8  , 2012  , Boston  , Massachusetts  , USA  Multiple autoencoders can be stacked so that the activations of hidden layer l are used as inputs to the autoencoder at layer l + 1. However  , denoising autoencoders avoid these approaches by randomly corrupting the input x prior to training. Local R 2 FP selects the most conductive features in the sub-region and summarizes the joint distribution of the selected features  , which enhances the robustness of the final representation and promotes the separability of the pooled features. In this paper  , we have introduced a novel pooling method R 2 FP  , together with its local and global versions  , for extracting features from feature maps learned through a sparse autoencoder. As the local R 2 FP deals with the sparse features in the sub-region and the sparseness of features is a vital start point that inspires the proposed method  , it can be assumed that K opt can be affected by the sparsity of the feature maps  , which is determined by the target response of each hidden neuron ρ in the autoencoder. In this section  , we conduct experiments on MNIST dataset to investigate the discipline of the optimal number K opt of selected features in the sub-region  , which is the key factor in the proposed local R 2 FP. Genetic programming GP is a means of automatically generating computer programs by employing operations inspired by biological evolution 6. GGGP is an extension of genetic programming. The term "Genetic Programming" was first introduced by Koza 12 and it enables a computer to do useful things by automatic programming. The proposed approach provides the generation of the error recovery logic using a method called Genetic Programming GP. However  , whether the balance can be achieved by genetic programming used by GenProg has still been unknown so far. Compared to random search  , genetic programming used by GenProg can be regard as efficient only when the benefit in terms of early finding a valid patches with fewer number of patch trials  , brought by genetic programming  , has the ability of balancing the cost of fitness evaluations  , caused by genetic programming itself. The problems all shared a common set of primitives. We used strongly typed genetic programming Finally  , GGGP was applied to create reference models. The core of this engine is a machine learning technique called Genetic Programming GP. 10. Given a problem  , the basic idea behind genetic programming 18 is to generate increasingly better solutions of the given problem by applying a number of genetic operators to the current population . As we have formalized link specifications as trees  , we can use Genetic Programming GP to solve the problem of finding the most appropriate complex link specification for a given pair of knowledge bases. For simplification  , we can measure the efficiency of GenProg using the NTCE when a valid patch is found 39. Communication fitness for controller of Figure  93503 for a mobile robot via genetic programming with automatically defined functions  , Table 5. In Section 2  , we provide background information on term-weighting components and genetic programming. This paper is organized as follows. l   , who used genetic programming to evolve control programs for modular robots consisting of sliding-style modules 2  , 81. al. Several program repair approaches assume the existence of program specification. We have compared our technique with genetic programming 2  , 6. First  , the initial population is generated  , and then genetic operators  , such as Genetic programming GP is a means of automatically generating computer programs by employing operations inspired by biological evolution 6. One of the key problems of genetic programming is that it is a nondeterministic procedure. Genetic operators simulate natural selection mechanisms such as mutation and reproduction to enable the creation of individuals that best abide by a given fitness function. These primitives were d e signed to aid genetic programming in finding a solution and either encapsulated problem specific information or low-level information that was thought to be helpful for obtaining a solution. In addition  , gradient primitives   , shown to be effective for communication in modular robots We also gave the genetic programming runs additional primitives for each problem. In this paper  , however  , we plan to further investigate whether genetic programming used by GenProg has the better performance over random search  , when the actual evolutionary search has started to work. They doubted that the promising results may not be brought by genetic programming used by GenProg  , because the patch search problem can be easy when random search would have likely yielded similar results. Determining which information to add was the result of parallel attempts to examine the unsuccessful results produced by the genetic programming and attempts to hand code problem solutions. We defer discussing the possible reason to Section 6. Answer for RQ1: In our experiment  , for most programs 23/24  , random search used by RSRepair performs better in terms of requiring fewer patch trials to search a valid patch than genetic programming used by GenProg  , regardless of whether genetic programming really starts to work see Figure 1 or not. In this paper  , we try to investigate the two questions via the performance comparison between genetic programming and random search. Furthermore  , the question of whether the benefit brought by genetic programming can balance the cost caused by fitness evaluations is not addressed. One novel part of our work is that we use a Genetic Programming GP based technique called ARRANGER Automatic geneRation of RANking functions by GEnetic pRogramming to discover ranking functions automatically Fan 2003a. But most of those ranking functions are manually designed by experts based on heuristics  , experience  , observations  , and statistical theories. Ranking functions usually could not work consistently well under all situations. We compared EAGLE with its batch learning counterpart. In this paper we presented EAGLE  , an active learning approach for genetic programming that can learn highly accurate link specifications. Other researchers used classifier systems 17  or genetic programming paradigm 3  to approach the path planning problem. Both approaches assume a predefined map consisting of fixed knot points. RQ2 is designed to answer the question. proposed GenProg  , an automatic patch generation technique based on genetic programming. However  , we could not fully verify the qualifications of the survey participants.  In Section 3  , we present our Combined Component Approach for similarity calculation. Individuals in the new generation are produced based on those in the current one. Genetic Programming searches for the " optimal " solution by evolving the population generation after generation. 19  select ranking functions using genetic programming   , maximizing the average precision on the training data. As a follow-on to this work  , Lacerda et al. GP maintains a population of individual programs. Genetic programming GP is a computational method inspired by biological evolution  , which discovers computer programs tailored to a particular task 19. Individuals in a new generation are produced based on those in the previous one. Genetic Programming searches for an " optimal " solution by evolving the population generation after generation. The entity resolution ER problem see 14 ,3  for surveys shares many similarities with link discovery. Later  , approaches combining active learning and genetic programming for LD were developed 10 ,21. An individual represents a tentative solution for the target problem. In Genetic Programming  , a large number of individuals  , called a population  , are maintained at each generation. We are not surprised for this experimental results. Then  , why does genetic programming  , a fitness evaluation directed search  , perform worse than a purely random search in our experiment ? 17  propose matching ads with a function generated by learning the impact of individual features using genetic programming. A. Lacerda et al. This approach randomly mutates buggy programs to generate several program variants that are possible patch candidates. The 'Initial Repair' heading reports timing information for the genetic programming phase and does not include the time for repair minimization. Successful repairs were generated for each program. for a mobile robot via genetic programming with automatically defined functions  , Table 5. collision avoidance as well as helping achieve the overall task. 7  proposed a new approach to automatically generate term weighting strategies for different contexts  , based on genetic programming GP. Fan et al. Interested readers can reference that paper or  The details of our system and methodology for Genetic Programming GP are discussed in our Robust track paper. Generate an initial population of random compositions of the functions and terminals of the problem solutions. Genetic programming uses four steps to solve problems: 1. GP is expansion of GA in order to treat structural representation. Genetic ProgrammingGP is the method of learning and inference using this tree-based representation". As our time and human resources were limited for taking two tasks simultaneously  , in this task we only concentrate on testing our ranking function discovery technique  , ARRANGER Automatic Rendering of RANking functions by GEnetic pRogramming Fan 2003a  , Fan 2003b  , which uses Genetic Programming GP to discover the " optimal " ranking functions for various information needs. We submitted results on both topic distillation and home page/named page finding tasks. Given the problem  , RQ1 asks whether genetic programming used by GenProg works well to benefit the generation of valid patches. Although some promising results for GenProg have been presented in some recent serial papers 40  , 23  , 21  , 38  , 10  , 22  , the problem of whether the promising results are got based on the guidance of genetic programming or just because the mutation operations are powerful enough to tolerate the inaccuracy of used fitness function has never been studied. Although promising results have been shown in their work  , the problem of whether the promising results are caused by genetic programming or just because the used mutation operations are very effective is still not be addressed. Both GenProg and Par use the same fault localization technique to locate faulty statements  , and genetic programming to guide the patch search  , but differ in the concrete mutation operations. Genetic Programming GP 14 is a Machine Learning ML technique that helps finding good answers to a given problem where the search space is very large and when there is more than one objective to be accomplished. Having this in mind  , we propose a genetic programmingbased approach to handle this problem. Also  , the work in 24  applies Genetic Programming to learn ranking functions that select the most appropriate ads. However  , we propose a learning method to maximize Click-through-Rate CTR for impressions. The experimental results show that the matching function outperforms the best method in 21 in finding relevant ads. We also compared our method with genetic programming based repair techniques. To assess the efficiency and effectiveness of our technique  , we employed SEMFIX tool to repair seeded defects as well as real defects in an open source software. Genetic programming approaches support more complex repairs but rely on heuristics and hence lack these important properties. Our focus on constant prints allows us to perform exhaustive search for repairs  , ensuring both completeness and minimality. GP makes it possible to solve complex problems for which conventional methods can not find an answer easily. The return value of a fitness function must appropriately measure how well an individual  , which represents a solution  , can solve the target problem. Our first approach extends a state-of-the-art tag recommender based on Genetic Programming to include novelty and diversity metrics both as attributes and in the objective function 1. We have already proposed and evaluated two different strategies. Koza applied GP Genetic Programming to automatic acquisition of subsum tion architecture to perform wall-following behavior  ?2. So far  , many researchers applied GA to motion acquisition problems for robots or virtual creatures. Given that genetic programming is non-deterministic  , all results presented below are the means of 5 runs. All non-RDF datasets were transformed into RDF and all string properties were set to lower case. Each experiment was ran on a single thread of a server running JDK1.7 on Ubuntu 10.0.4 and was allocated maximally 2GB of RAM. We also employed GenProg to repair the bugs in Coreutils. This confirms that if the repair expression does not exist in other places of the program  , genetic programming based approaches have rather low chance of synthesizing the repair. Learning approaches based on genetic programming have been most frequently used to learn link specifications 5 ,15 ,17. The idea behind active learners also called curious classifiers 18 is to query for the labels of In addition  , it usually requires a large training data set to detect accurate solutions. Another genetic programming-based approach to link discovery is implemented in the SILK framework 15. Thus  , it is only able to learn a subset of the specifications that can be generated by EAGLE. This representation is used as knowledge representation and is considered to suit as knowledge re~resentation~l. Since an appropriate stopping rule is hard to find for the Genetic Programming approach  , overtraining is inevitable unless protecting rules are set. They form the ranking function candidate pool. Finally  , we applied data mining DM techniques based on grammar-guided genetic programming GGGP to create reference models useful for defining population groups. This way  , symbolic sequences can be automatically compared to detect similarities  , class patients  , etc. The average time required by SEMFIX for each repair is less than 100 seconds. Out of the 90 buggy programs  , with a test suite size of 50 — SEMFIX repaired 48 buggy programs while genetic programming repaired only 16. We also notice that GenProg failed for all arithmetic bugs. There has also been work on synthesizing programs that meet a given specification. These functions are discovered using genetic programming GP and a state-of-the-art classifier optimumpath forest OPF 3  , 4. The method detects these cases by exploiting a combination of automatically generated similarity functions. We use genetic programming to evolve program variants until one is found that both retains required functionality and also avoids the defect in question. Instead  , it works on off-the-shelf legacy applications and readily-available testcases . A framework for tackling this problem based on Genetic Programming has been proposed and tested. In this paper  , we considered the problem of classification in the context of document collections where textual content is scarce and imprecise citation information exists. Furthermore  , we will aim at devising automatic configuration approaches for EAGLE. The following experiments were run by connecting FX- PAL'S genetic programming system to a modular robot simulator  , built by J. Kubica and S. Vassilvitskii. The expansion and contraction of these arms provide the modules with their only form of motion. Active learning approaches based on genetic programming adopt a comitteebased setting to active learning. In this setting  , the information content of a pair s  , t is usually inverse to its distance from the boundary of C t . As the planning motion  , we give this system vertical movement and one step walk. With this system  , we simulate motion generation hierarchically for six legged locomotion robot using Genetic Programming. Sims studied on co-evolution of motion controller and morphology of rirtual creatures 3. All the experiments were conducted on a Core 2 Quad 2.83GHz CPU  , 3GB memory computer with Ubuntu 10.04 OS. The classifier uses these similarity functions to decide whether or not citations belong to a same author. To this purpose we have proposed randomized procedures based on genetic programming or simulated annealing 8  , 9. Thus  , the choice of the optimal feature sets may require a preliminary feature construction phase. Subsequently  , we give some insight in active learning and then present the active learning model that underlies our work. We show how the discovery of link specifications can consequently be modeled as a genetic programming problem. GP is a machine learning technique inspired by biological evolution to find solutions optimized for certain problem characteristics. To give proper answers for these questions  , we propose a new approach to content-targeted advertising based on Genetic Programming GP. The main inconvenient of this approach is that it is not deterministic. Here  , the mappings are discovered by using a genetic programming approach whose fitness function is set to a PFM. Using an error situation obtained with the sampled parameters  , a fitness unction based on the allowed recovery criteria can be defined. In Genetic Programming  , each member in the population is a computer program for the solution of the problem. We used strongly typed genetic programming The specific primitives added for each problem are discussed with setup of the the initial population  , results of crossover and mutation  , and subtrees created during mutation respectively . The three most common and most important methods are: Genetic programming applies a number of different possible conditions to the best solutions to create the next generation of solutions. The goal of grammarguided genetic programming is to solve the closure problem 7. However  , the computational cost of this approach is extremely high for problems requiring large population sizes 6 . External validity is concerned with generalization. In addition  , in the future we will investigate whether genetic programming has the advantage over random search on fixing bugs existing in multi files. Other approaches based on genetic programming e.g. , 17 detect matching properties while learning link specifications  , which currently implements several time-efficient approaches for link discovery. For example  , 16 relies on the hospital-residents problem to detect property matches. Although they also used genetic programming  , their evaluation was limited to small programs such as bubble sorting and triangle classification  , while our evaluation includes real bugs in open source software. introduced an automatic patch generation technique 5. The 'Time' column reports the wall-clock average time required for a trial that produced a primary repair. Since an appropriate stopping rule is hard to find for the Genetic Programming approach  , over-training is inevitable unless protecting rules are set. Only the most robust and consistent functions are selected and they form the ranking function candidate pool. Realizing this  , we use tree-based representation as motion knowledge and construct the system using tree-based representation. Similarly  , the approach presented in 21 assumes that a 1-to-1 mapping is to be discovered. Supervised batch learning approaches for learning such classifiers must rely on large amounts of labeled data to achieve a high accuracy. This paper has reported our initial experiments aimed at investigating whether evolutionary programming  , and genetic programming in particular can evolve multiple robot controllers that utilise communication to improve their ability to collectively perform a task. Communication fitness for controller of Figure  93503 For the last 2 programs in Figure 1b  , the advantage of RSRepair is statistical significance; although there exists no significant difference for the remaining 4 programs due to too small sample sizes no more than 20 in the " Size " column of Figure 1b  , RSRepair has the smaller NCP in terms of Mean and Median. For the representation problem  , GenProg represents each candidate patch as the Abstract Syntax Tree AST of the patched program. As described in 15  , GenProg needs to implement two key ingredients before the application of genetic programming: 1 the representation of the solution and 2 the definition of the fitness function. However  , Andrea Arcuri and Lionel Briand found that GenProg often searched valid patches in the random initialization of the first population before the actual evolutionary search even starts to work. With the hypothesis that some missed important functionalities may occur in another position in the same program  , GenProg attempts to automatically repair defective program with genetic programming 38. GenProg 2 has the ability of fixing bugs in deployed  , legacy C programs without formal specifications. Then  , in this subsection we plan to investigate to what extent genetic programming used by GenProg worsens the repair efficiency over random search used by RSRepair. " Hence  , it is not surprising that GenProg  , most often  , took more time to repair successfully faulty programs  , on average  , in Table  2. Our technique takes as input a program  , a set of successful positive testcases that encode required program behavior  , and a failing negative testcase that demonstrates a defect. Our classification approach combines a genetic programming GP framework  , which is used to define suitable reference similarity functions   , with the Optimum-Path Forest OPF classifier  , a graph-based approach that uses GP-based edge weights to assign input references to the correct authors. The proposed system uses that information along with pure training samples defined by an unsupervised approach   , in a hybrid classification scheme. As we can see  , Genetic Programming takes a so-called stochastic search approach  , intelligently  , extensively  , and " randomly " searching for the optimal point in the entire solution space. Better solutions are obtained either by inheriting and reorganizing old ones or by lucky mutation  , simulating Darwinian Evolution. Further  , given the negative impact of irrelevant ads on credibility and brand of publishers and advertisers  , how to design functions that minimize the placement of irrelevant ads  , especially when the relevant ones are not available ? Then  , we give an overview of the grammar that underlies links specifications in LIMES and show how the resulting specifications can be represented as trees. sKDD transforms the original numerical temporal sequences into symbolic sequences  , defines a symbolic isokinetics distance SID that can be used to compare symbolic isokinetics sequences   , and provides a method  , SYRMO  , for creating symbolic isokinetics reference models using grammar-guided genetic programming. This paper has focused on the I4 project's sKDD subsystem. We developed a genetic programming approach to finding consensus structural motifs in a set of RNA sequences known to be functionally related. Knowing the common structural motifs in a set of coregulated RNA sequences will help us better understand the regulation mechanism. Of these techniques  , GenProg and Par  , the two awardwinning patch generation techniques  , presented the very promising results. The results show that genetic programming finds matching functions that significantly improve the matching compared to the best method without page side expansion reported in 18. Arithmetic operators and the log function are internal nodes while different numerical features of the query and ad terms can be leafs of the function tree. Their approach relies on formal specifications  , which our approach does not require. Recent work has addressed this drawback by relying on active learning  , which was shown in 15 to reduce the amount of labeled data needed for learning link specifications. For example  , the genetic programming approach used in 7 has been shown to achieve high accuracies when supplied with more than 1000 positive examples. For example   , the approach presented in 5 relies on large amounts of training data to detect accurate link specification using genetic programming. Although unsupervised techniques were newly developed see  , e.g. , 17  , most of the approaches developed so far abide by the paradigm of supervised machine learning. In this paper we have introduced a new approach based on the combination of term weighting components  , extracted from well-known information retrieval ranking formulas  , using genetic programming. Finally  , but not less important  , we also intend to examine closely the discovered best ranking functions to understand better how they work and the reasons for their effectiveness. Genetic Programming has been widely used and approved to be effective in solving optimization problems  , such as financial forecasting  , engineering design  , data mining  , and operations management. Computer programs that evolve in ways that resemble natural selection can solve complex problems even their creators do not fully understand " Holland  , 1975. Genetic Programming shows its sharp edge in solving such kind of problems  , since its internal tree structure representation for " individuals " can be perfectly used for describing ranking functions. We can actually treat the ranking function space as a space consists of all kinds of tree structures. Section 2 of the paper gives an overview of the I4 Intelligent Interpretation of Isokinetics Information system  , of which this research is part. A follow-up work 13 proposes a method to learn impact of individual features using genetic programming to produce a matching function. To solve this problem  , Ribeiro- Neto et al expand the page vocabulary with terms from other similar pages weighted based on the overall similarity of the origin page to the matched page  , and show improved matching precision. Guided by genetic programming  , GenProg has the ability to repair programs without any specification  , and GenProg is commonly considered to open a new research area of general automated program repair 26  , 20  , although there also exists earlier e.g. , 5  , 2 and concurrent work on this topic 6. Automated repair techniques have received considerable recent research attentions. Construct validity threats concern the appropriateness of the evaluation measurement. In addition  , in this paper we focus only on the comparison between random search and genetic programming  , in our future work we plan to study random search with the comparison on other repair techniques such as 12  , 5  , 28. In a follow-up work 7 the authors propose a method to learn impact of individual features using genetic programming to produce a matching function. To solve this problem  , Ribeiro-Neto et al expand the page vocabulary with terms from other similar pages weighted based on the overall similarity of the origin page to the matched page  , and show improved matching precision.   , but none of these strategies reaches the level of applicability and the speed of execution of random testing. Several other strategies for input generation have been proposed symbolic execution combined with constraint solving 30  , 18  , direct setting of object fields 5  , genetic programming 29  , etc. In order to answer these questions  , we choose ARRANGER – a Genetic Programming-based discovery engine 910 to perform the ranking function tuning. To copy otherwise  , or republish  , to post on servers or to redistribute to lists  , requires prior specific permission and/or a fee. Genetic Programming has been widely used and proved to be effective in solving optimization problems  , such as financial forecasting  , engineering design  , data mining  , and operations management 119. Computer programs that evolve in ways that resemble natural selection can solve complex problems even their creators do not fully understand " 16. In the following  , we present our implementation of the different GP operators on link specifications and how we combine GP and active learning. This approach is yet a batch learning approach and it consequently suffers of drawbacks of all batch learning approaches as it requires a very large number of human annotations to learn link specifications of a quality comparable to that of EAGLE. The robot modules we consider are the TeleCube modules currently being developed at Xerox PARC 13 and shown in Figure 1 . Still  , none of the active learning approaches for LD presented in previous work made use of the similarity of unlabeled link candidates to improve the convergence of curious classifiers. While the first active genetic programming approach was presented in 4  , similar approaches for LD were developed later 7 ,15 . For example  , the approach presented in 8 relies on large amounts of training data to detect accurate link specification using genetic programming. With regard to the generation of link specifications  , some unsupervised techniques were newly developed see  , e.g. , 22  , but most of the approaches developed so far abide by the paradigm of supervised machine learning. This absence of any system in choosing inputs is also what exposes random testing to the most criticism. 15 proposes an approach based on the Cauchy-Schwarz inequality that allows discarding a large number of superfluous comparisons. Particularly  , we investigate an inductive learning method – Genetic Programming GP – for the discovery of better fused similarity functions to be used in the classifiers  , and explore how this combination can be used to improve classification effectiveness . In this work we try to overcome these problems by applying automatically discovered techniques for fusion of the available evidence. Genetic Programming takes a so-called stochastic search approach  , intelligently  , extensively  , and " randomly " searching for the optimal point in the entire solution space. It provides sound solutions to many difficult problems  , for which people have not found a theoretical or practical breakthrough. Another approach to contextual advertising is to reduce it to the problem of sponsored search advertising by extracting phrases from the page and matching them with the bid phrase of the ads. The results show that genetic programming finds matching functions that significantly improve the matching compared to the best method without page side expansion reported in 8. These primitives were largely derived directly from the basic actions and abilities of the modules and simple computational constructs. The best computer program that appeared in any generation  , the best-so-far solution  , is designated as the result of genetic programming Koza 19921. Additionally  , our approach synthesizes grasps  , with no a priori constraints on initial grasps  , as opposed to lo  , in which grasp primitives are learned based on a given set of grasp primitives. In addition  , similar to other search-based software engineering SBSE 15  , 14 approaches  , genetic programming often suffers from the computationally expensive cost caused by fitness evaluation  , a necessary activity used to distinguish between better and worse solutions. valid patches much faster  , in terms of requiring fewer patch trials 1   , than random search. That is  , compared to random search  , genetic programming does not bring benefits in term of fewer NCP in this case to balance the cost caused by fitness evaluations. As presented in RQ1  , to find a valid patch  , GenProg  , in most cases  , requires not fewer NCP than RSRepair. 26  introduced the idea of program repair using genetic programming  , where existing parts of code are used to patch faults in other parts of code and patching is restricted to those parts that are relevant to the fault. Weimer et al. We note that this weakness is inherent in any test suite based program repair  , since no formal program specification is given and repairs can only be generated with respect to limited number of given tests. This is the major motivation to choose GP for the ranking function discovery task. Based on the plaintext collection  , our ARRANGER engine  , a Genetic Programming GP based ranking function discovery system  , is used to discover the " optimal " ranking functions for the topic distillation task. Instead  , we construct a " surrogate " plaintext collection by merging full text content with all the anchor information for a page. The function is represented as a tree composed of arithmetic operators and the log function as internal nodes  , and different numerical features of the query and ad terms as leafs. With flexible GP operators and structural motif representations  , our new method is able to identify general RNA secondary motifs. We choose not to record the genetic programming operations performed to obtain the variant as an edit script because such operations often overlap and the resulting script is quite long. For example  , our variants often include changes to control flow e.g. , if or while statements for which both the opening brace { and the closing brace } must be present; throwing away part of such a patch results in a program that does not compile. The isolation of the search strategies from the search space makes the solution compatible with that of Valduriez891 and thus applicable to more general database programming languages which can be deductive or object-oriented Lanzelotte901. This is illustrated by modeling within the same framework different enumerative  , randomized and genetic search strategies  , Furthermore  , we show how the search strategies thus produced can be controlled in the sense that successful termination can be enforced by assertions. Yet  , so far  , none of these approaches has made use of the correlation between the unlabeled data items while computing the set of most informative items. Several approaches that combine genetic programming and active learning have been developed over the course of the last couple of years and shown to achieve high F-measures on the deduplication see e.g. , 4 and LD see e.g. , 15 problems. Furthermore  , affected by GenProg  , Par also uses genetic programming to guide the patch search in the way like GenProg. After that  , general automated program repair has gone from being entirely unheard of to having its own multi-paper sessions  , such as " Program Repair " session in ICSE 2013  , in many top tier conferences 20  , and many researchers justify the advantage of their techniques  , such as Par and SemFix  , via the comparison with GenProg. The fact that it has been successfully applied to similar problems  , has motivated us to use it as a basis for discovering good similarity functions for record replica identification. This approach captures the novelty and diversity of a list of recommended tags implicitly  , by introducing metrics that assess the semantic distance between different tags diversity and the inverse of the popularity of the tag in the application novelty. AutoFix-E 37 can repair programs but requires for the contracts in terms of pre-and post-conditions. Running test cases typically dominated GenProg's runtime " 22  , which is also suitable for RSRepair  , so we use the measurement of NTCE to compare the repair efficiency between GenProg and RSRepair  , which is also consistent with traditional test case prioritization techniques aiming at early finding software bugs with fewer NTCE. Short titles may mislead the results  , specially generic titles such as Genetic Programming  , then we add the publication venue title to this type of query. If ti comprises only one or two words  , the query is formed by the quoted title ti followed by the first four author names  , similar to the previously described query  , also not permitting one word error  , followed by an AND statement using the first four words from the publication venue title vi. That is  , RSRepair immediately discards one candidate patch once the patched program fails to pass some test case. Unlike genetic programming which requires fitness evaluation in the sense that GenProg has to run fixed size of test cases to compute the fitness of a candidate patch even if GenProg has been aware that the patch is invalid i.e. , the patched program has ever failed to pass some test case  , random search has no such constraint. Recently  , in the paper 40 genetic programming is proposed to fix automatically the general bugs  , and a prototype tool called GenProg based on this technique is implemented. We conducted a set of experiments aiming to evaluate the proposed disambiguation system in comparison with stateof-the-art methods on two well-known datasets. To the best of our knowledge  , the problem of discovering accurate link specifications has only been addressed in very recent literature by a small number of approaches: The SILK framework 14  now implements a batch learning approach to discovery link specifications based on genetic programming which is similar to the approach presented in 6. Note that the task of discovering links between knowledge bases is closely related with record linkage 30 ,10 ,5 ,17. These are supervised approaches that begin with a small number of labeled links and then inquire labels for data items that promise to improve their accuracy. The evaluation has shown that the numerical and symbolic reference models generated from isokinetics tests on top-competition sportsmen and women are  , in the expert's opinion  , similar. Our method is similar to these methods as we directly optimize the IR evaluation measure i.e. , NDCG by using the Simulated Annealing which uses a modification of downhill Simplex method for the next candidate move to find the global min- imum. Some other approaches for directly optimizing IR measures use Genetic Programming 1  , 49 or approximate the IR measures with the functions that are easy-to-handle 44  , 12. We conduct CLIR experiments using the TREC 6 CLIR dataset described in Section 5.1. In this section  , CLQS is tested with French to English CLIR tasks. Xu and Weischedel 19 estimated an upper bound on CLIR performance. The impact of disambiguation for CLIR is debatable. Probabilistic CLIR. We chose probabilistic structured queries PSQ as our CLIR baseline because among vector space techniques for CLIR it presently yields the best retrieval effectiveness. A major motivation for us to develop the cross-language meaning matching model is to improve CLIR effectiveness over a strong CLIR baseline. It does not occur in an operational CLIR setting. According to Hull and Grefenstette 1996 human translation in CLIR experiments is an additional source of error. The simpler MoIR models may be directly derived from the more general CLIR setting. For simplicity  , we only discuss CLIR modeling in this section. Section 7 and 8 compare our system with structural query translation and MTbased CLIR. Section 6 compares CLIR performance of our system with monolingual IR performance. For commercial reasons  , we have developed technology for English  , Japanese  , and Chinese CLIR. TREC-8 marks the first occasion for CLARITECH to participate in the CLIR track. We ran CLIR and computed MAP at different Cumulative Probability Thresholds CPT. Figure 4 shows the relative English-French CLIR effectiveness as compared to the monolingual French baseline. Retrieval results show that their impact on CLIR is very small. We also studied the impact of spelling normalization and stemming on Arabic CLIR. Table 4shows a comparison of the recall precision values for the English-Chinese CLIR experimental results. Our English-Chinese CLIR experiments used the MG 14 search engine. Despite the big differences between the two language pairs  , our experiments on English- Chinese CLIR consistently confirmed these findings  , showing the proposed cross-language meaning matching technique is not only effective  , but also robust. Our experiments with an English-French test collection for which a large number of topics are available showed that CLIR using bidirectional translation knowledge together with statistical synonymy significantly outperformed CLIR in which only unidirectional translation knowledge was exploited  , achieving CLIR effectiveness comparable to monolingual effectiveness under similar conditions. In this paper we report results of an experimental investigation into English-Japanese CLIR. While most existing studies have concentrated on CLIR between English and one or more European languages  , there is a need to develop methods for CLIR between European and Asian languages . We propose an approach to estimate the translation probability of a query term according to its effect on CLIR. We are interested in realizing 1 the possibility of predicting a query term to be translated or not; 2 whether the prediction can effectively improve CLIR performance; and 3 how untranslated OOV and various translations of non-OOV terms affect CLIR performance. Interest in Cross-Language Information Retrieval CLIR has grown rapidly in recent years l 2 3 . To perform such benchmark  , we use the documents of TREC6 CLIR data AP88-90 newswire  , 750MB with officially provided 25 short French-English queries pairs CL1-CL25. Based on the observation that the CLIR performance heavily relies on the quality of the suggested queries  , this benchmark measures the quality of CLQS in terms of its effectiveness in helping CLIR. The actual CLIR research seeks to answer the question how fuzzy translation should be applied in an automatic CLIR query formulation and interactive CLIR to achieve the best possible retrieval performance. One promising method is LCS longest common subsequence and another skipgrams 8. Both CLIR and CLTC are based on some computation of the similarity between texts  , comparing documents with queries or class profiles. On the other hand  , there is a rich literature addressing the related problem of Cross-Lingual Information Retrieval CLIR. The main difficulties for CLIR are the disambiguation of the query term in the source and target language and the identification of the query language. CLIR is characterized by differences in query and document language 3. Next we examined transitive retrieval to gauge its impact on notranslation CLIR. We first explored the viability of no-translation CLIR on a broader range of disparate language pairs than has been heretofore reported. Based on this fundamental idea of CLIR  , we can define a corresponding Mixed-script IR MSIR setup as follows. Note that all the documents in a typical CLIR setup are assumed to be written in the corresponding native scripts. What is shown at each point in the figure is the monolingual percentage of the CLIR MAP. The resulting good performance of CLIR corresponds to the high quality of the suggested queries. In this paper  , we propose to use CLQS as an alternative to query translation  , and test its effectiveness in CLIR tasks. In the other experiments  , the English queries are translated into French and French queries are translated into English using various tools: 2. This is not CLIR  , but is used as a reference point with which CLIR performance is compared. Since monolingual retrieval is a special case of CLIR  , where the query terms and document terms happen to be of the same language e.g. The retrieval model was originally proposed for CLIR. Nonetheless  , the results suggest that a simple dictionary-based approach can be as effective as a sophisticated MT system for CLIR. Therefore  , we cannot draw a firm conclusion about the retrieval advantage of probabilistic CLIR without further study. In order to analyze how good our query translation approach for CLIR  , we display in Fig. It is also interesting to find that the best CLIR performance is over 100% of the monolingual. We performed three official automatic CLIR runs and 29 post-hoc automatic CLIR runs. It therefore seems to be a good candidate for further study  , and an appropriate choice if a method Overall  , English-French CLIR was very effective  , achieving at least 90% of monolingual MAP when translation alternatives with very low probability were excluded. This is also one of very few recent studies to empirically explore the value of multilingual thesauri or controlled vocabularies for CLIR. We also show that such dictionaries contribute to CLIR performance . Such effectiveness is consistent across different translation approaches as well as benchmarks. Experiments on NTCIR-4 and NTCIR-5 English- Chinese CLIR tasks show that CLIR performance can be significantly improved based on our approach. Paradoxically  , technical terms and names are not generally found in electronic translation dictionaries utilised by MT and CLIR systems. Their correct translation therefore is crucial for good performance of machine translation MT and cross-language information retrieval CLIR systems. In dictionary-based CLIR queries are translated into the language of documents through electronic dictionaries. In CLIR a user may use his or her native language in searching for foreign language documents 4. In each case  , we formed title+description queries in the same manner as for the automatic monolingual run. Our most relevant work 10  presented a method to predict the performance of CLIR according to translation quality and ease of queries. Few did pose the problem of predicting CLIR performance or whether to translate a query term or not. The reader is referred to the technical report by Oard and Dorr for an excellent review of the CLIR literature 18. Our goal is to assess the UMLS Metathesaurus based CLIR approach within this context. Moreover we investigate how a controlled vocabulary can be used to conduct free-text based CLIR. For English-Chinese CLIR  , we accumulated search topics from TREC-5 and TREC-6  , which used the same Chinese document collection. The French queries serve to establish a useful upper baseline for CLIR effectiveness. The effectiveness of the various query translation methods for CLIR was then investigated. A CLIR BMIR-J2 collection was constructed by manually translating the Japanese BMIR-J2 requests into English. Most present CLIR methods fall into three categories: dictionary-based  , MT-based and corpus-based methods 1 . CLIR is to retrieve documents in one language target language providing queries in another language source language. The most challenging aspect is the search capability of the system  , which is referred to as crosslingual information retrieval CLIR. Table 5shows that probabilistic CLIR using our system outperforms the three runs using SYSTRAN  , but the improvement over the combined MT run is very small. WEAVER was used to induce a bilingual lexicon for our approach to CLIR. RUN1: To provide a baseline for our CLIR results  , we used BableFish to " manually " translate each Chinese query. The following three runs were performed in our Chinese to English CLIR experiments: 1. Applications for alignments other than CLIR  , such as automatic dictionary extraction  , thesaurus generation and others  , are possible for the future. Use of the alignments for CLIR gives excellent results  , proving their value for realworld applications. Multilingual thesauri or controlled vocabularies   , however  , are an underrepresented class of CLIR resources. CLIR methods involving machine translation systems  , bilingual dictionaries  , parallel and comparable collections are currently being  explored. Our approach to CLIR in MEDLINE is to exploit the UMLS Metathesaurus and its multilingual components. From our perspective  , it is evident that given the nature of the TREC collections  , CLIR approaches based upon multilingual thesauri remain difficult to explore. Research in the area of CLIR has focused mainly on methods for query translation. With the explosion of on-line non-English documents  , crosslanguage information retrieval CLIR systems have become increasingly important in recent years. However  , previous work showed that English- Chinese CLIR using simple dictionary translation yields a performance lower than 60% of the monolingual performance 14. Bilingual dictionaries have been used in several CLIR experiments. Although the principle of using parallel texts in CLIR is similar  , the approaches used may be very different. Parallel texts have been used in several studies on CLIR 2  , 6  , 19. tasks. There might be two possible reasons. 2 11 queries with monolingual Avg. P lower than CLIR. The multilingual information retrieval problem we tackle is therefore a generalization of CLIR. 11. Section 2 introduces the statistical approach to CLIR. The paper is arranged as follows. Table 6shows examples of queries transformed through both alternatives. CLIR performance observed for this query set. 2 11 queries with monolingual average precision lower than CLIR. cross-language performance is 87.94% of the monolingual performance. Table 5: Performances of the CLIR runs. Similar as for MoIR  , the combined CLIR models are also compared. Test II: Combined Models. Based on the results of this study our future research will involve the identification of language pairs for which fuzzy translation is effective  , the improvement of the rules for example  , utilising rule co-occurrence information  , testing the effects of tuning a confidence factor by a specific language pair  , selecting the best TRT and fuzzy matching combination  , and testing how to apply fuzzy translation in actual CLIR research. The Ad Hoc task provides a useful opportunity for us to get new people familiar with the tools that we will be using in the CLIR track|this year we submitted a single oocial Ad Hoc run using Inquery 3.1p1 with the default settings. For TREC-7 and TDT-2 we had been using PRISE  , but our interest in trying out Pirkola's technique for CLIR led to our choice of Inquery for CLIR TREC-8. Because the commercial versions of the dictionaries were converted automatically to CLIR versions  , with no manual changes done to the dictionaries or the translations  , the performance level of the CLIR queries achieved in the study can be achieved in practice in an operational CLIR setting. The results presented in this paper show that MRD-based CLIR queries perform almost as well as monolingual queries  , if domain specific MRD is used together with general MRD and queries are structured on the basis of the output of dictionaries . The study used a structuring method  , in which those words that were derived from the same Finnish word were grouped into the same facet. Even though a common approach in CLIR is to perform query translation QT using a bilingual dictionary 32  , there were studies showing that combining both QT and document translation DT improved retrieval performance in CLIR by using bilingual representations in both the source and target language 28  , 19  , 7  , 4. Cross-lingual information retrieval CLIR addresses the problem of retrieving documents written in a language different from the query language 30. We then showed that the probabilistic structured query method is a special case of our meaning matching model when only query translation knowledge is used. The CLIR experiments reported in this section were performed using the TREC 2002 CLIR track collection  , which contains 383 ,872 articles from the Agence France Press AFP Arabic newswire  , 50 topic descriptions written in English  , and associated relevance judgments 12. The documents were stemmed using Al-Stem a freely available standard resource from the TREC CLIR track  , diacritics were removed  , and normalization was performed to convert the letters ya Experiments for English and Dutch MoIR  , as well as for English-to-Dutch and Dutch-to-English CLIR using benchmarking CLEF 2001-2003 collections and queries demonstrate the utility of our novel MoIR and CLIR models based on word embeddings induced by the BWESG model. C3 We construct a novel unified framework for ad-hoc monolingual MoIR and cross-lingual information retrieval CLIR which relies on the induced word embeddings and constructed query and document embeddings. Section 5 presents the results  , Section 6 suggests future work  , and Section 7 concludes. Section 4 discusses our CLIR approaches. Cross Language Information Retrieval CLIR refers to retrieval when the query and the database are in different languages. 2 In this section  , we show the effectiveness of our approach for CLIR. 5illustrates the impact of the variable k. The results are available in tab. 1997 found that their corpus-based CLIR queries performed almost as well as the monolingual baseline queries. Sheridan et al. Our approach is independent of stemmers  , part of speech taggers and parsers. We investigate query translation based CLIR here. Cross-Language Information Retrieval CLIR remains a difficult task. This makes using methods developed for automatic machine translation problematic. Hence  , CLIR experiments were performed with different translations: i.e. First comparative experiments only focused on the querytranslation model. Phrasal translation approach 17  , 11 was inspected for improving CLIR performance. Improving translation accuracy is important for query translation . However  , our approach is unique in several senses. This approach uses intuition similar to He's work on CLIR 9. This is importmt in a CLIR environment. 'h LCA expansion has higher precision at low recall levels. Automatic phrase identification methods have been developed for CLIR environment Ballesteros & Croft  , 1997 . This results in decreased precision. It also shows that monolingual performance is not necessarily the upper bound of CLIR performance. This result confirms the intuition. Moving from the global perspective to an individual level  , CLIR is useful  , for example  , for the people  , who are able to understand a foreign language  , but have difficulty in using it actively. The need of CLIR systems in today's world is obvious. The focus of this paper is on machine learning-based CLIR approaches and on metrics to measure orthogonality between these systems. Various publications have investigated different methods of system combination for CLIR  , including logical operations on retrieved sets 3   , voting procedures based on retrieval scores 1  , or machine learning techniques that learn combination weights directly from relevance rankings 14. Translation experiments and CLIR experiments are based on the CLEF topic titles C041-C200  , which are capitalized  , contain stopwords and full word forms. The effect of QR for NLP is investigated by evaluating the baseline method for query translation  , which is a typical task for CLIR. Technical terms and proper names constitute a major problem in dictionary-based CLIR  , since usually just the most commonly used technical terms and names are found in translation dictionaries. Direct comparison to techniques based on language modeling would be more difficult to interpret because vector space and language modeling handle issues such as smoothing and DF differently. In addition to the ambiguity problem  , each of the approaches to CLIR has drawbacks associated with the availability of resources. Despite promising experimental results with each of these approaches   , the main hurdle to improved CLIR effectiveness is resolving ambiguity associated with translation. In this paper  , we look at CLIR from a statistical modelling perspective  , similarly to how the problems of part-of-speech tagging  , speech recognition  , and machine translation have been  , successfully  , approached. The problem of Cross-Language Information Retrieval CLIR extends the information retrieval framework by assuming that queries and documents are not in the same language. Dictionary based CLIR was explored by several groups including New Mexico State University 8  , University of Massachusetts l  , and the Xerox Research Center Europe ll. Thirteen groups participated in the CLIR track introduced in TREC-6  , with documents and queries in German   , English  , French and queries in Dutch and Spanish as well. An overview of the technical issues involved in supporting CLIR within the European Library with a specific focus on user query translation can be found in Agosti1. Much of the research conducted in this area has focused on supporting more effective cross-language information retrieval CLIR. It is certainly true that nonparticipants might have more difficulties in interpreting their results based on the small size of the CLIR pool  , as Twenty-One points out. We are however confident that participants receive valuable results from their evaluation through the CLIR track. In English-Chinese CLIR  , pre-translation query expansion means using a separate English collection for pretranslation retrieval in order to expand the English query with highly associated English terms. In CLIR  , queries can be expanded prior to translation  , after translation or both before and after translation. Our experiments of CLIR showed that the triple translation has a positive impact on the query translation  , and results in significant improvements of CLIR performance over the co-occurrence method. The translations using triples showed three main benefits: a more precise translation; an extension of the coverage of the bilingual dictionary; and the possibility to train the model using unrelated bilingual corpora. Finding translations in general dictionaries for CLIR encounters the problems of the translation of unknown queries -especially for short queries and the availability of up-to-date lexical resources. The obtained experimental results have shown its effectiveness in efficiently generating translation equivalents of various unknown query terms and improving retrieval performance for conventional CLIR approaches. It has been suggested that CLIR can potentially utilize the multiple useful translations in a bilingual lexicon to improve retrieval performance Klavans and Hovy  , 1999. The major difference between MT-based CLIR and our approach is that the former uses one translation per term and the latter uses multiple translations. Section 3 then introduces our meaning matching model and explains how some previously known CLIR techniques can be viewed as restricted implementations of meaning matching . In Section 2  , we review previous work on CLIR using query translation  , document translation  , and merged result sets. Research on CLIR has therefore focused on three main questions: 1 which terms should be translated ? In order to create broadly useful systems that are computationally tractable  , it is common in information retrieval generally  , and in CLIR in particular  , to treat terms independently . Although not strictly an upper bound because of expansion effects  , it is quite common in CLIR evaluation to compare the effectiveness of a CLIR system with a monolingual baseline. We therefore feel that our monolingual baseline for Chinese is a reasonable one. More generally  , this research is motivated by the fact that  , relative to dictionaries and collection based strategies  , thesauri remain unexplored in the recent CLIR context. Our thesaurus based CLIR approach seeks to overcome both problems  , allowing free-text user queries and considering the free-text portions of documents during retrieval. Combining the UMLS Metathesaurus with a MEDLINE test database enables an empirical investigation of a high quality multilingual thesaurus as a resource for free-text based CLIR using two broad approaches: document translation and query translation. Even though precomputation can improve the efficiency of our system as we discussed earlier  , we expect MT-based CLIR would still be faster due to a sparser term-document matrix. It is about 10 times as fast as our CLIR system in the above experiments. One might wonder whether we can use the Arabic monolingual thesaurus to improve CLIR. Although their impact on CLIR performance is small  , spelling normalization and stemming are still useful because they reduce the need for memory because there are fewer entries in the lexicon and they improve the retrieval speed by simplifying the score computation. The left graph shows a comparison of doing English-German CLIR using the alignments  , the wordlist or the combination of both. Some caution is appropriate with regard to the scope of the conclusions because this was the first year with a CLIR task at the TREC conference  , and the size of the query set was rather small. Research in CLIR explores techniques for retrieving documents in one language in response to queries in a different language. This challenge has contributed to the increasing popularity of Cross-Language Information Retrieval CLIR among researchers in the Information Retrieval IR community in recent years. The most obvious approach to CLIR is by either translating the queries into the language of the target documents or translating the documents into the language of the queries. The problem of multilingual text retrieval has a long history. Other specific works on CLIR within the multilingual semantic web may be found in 17 and 18   , while a complete overview of the ongoing research on CLIR is available at the Cross-Language Evaluation Forum CLEF 3   , one of the major references concerning the evaluation of multilingual information access systems. Cross Language Information Retrieval CLIR addresses the situation where the query that a user presents to an IR system  , is not in the same language as the corpus of documents being searched. However  , as shown in various submissions to the CLIR tracks of TREC  , researchers often failed to locate resources  , either free or commercial  , for translating directly between major However  , as the translation resource is constant across the experiments in the paper  , we were confident this would not affect the comparison of triangulation to other CLIR techniques. EuroWordNet has a small phrase vocabulary  , which we anticipated would reduce the effectiveness of our CLIR system. Thus  , it is important for a translation system based CLIR approach to maintain the uncertainty in translating queries when queries are ambiguous. However  , when a query is truly ambiguous and multiple possible translations need to be considered  , a translation based CLIR approach can perform poorly. OOV problem consists of having a dictionary that is not able to completely cover all terms of a language or  , more generally  , of a domain . MRD-based approaches demonstrated to be effective for addressing the CLIR problem ; however  , when CLIR systems are applied to specific domains  , they suffer of the " Out-Of-Vocabulary " OOV issue 7. These components interact  , respectively  , with the MT services and with the domain-specific ontology deployed on the CLIR system. The proposed CLIR system provides two different components for transforming the queries formulated by users into the final ones performed on the index. The goal of the presented study was the investigation on the effectiveness of integrating semantic domain-specific resources  , like ontologies  , into a CLIR context. In this work  , we have presented a CLIR system based on the combination of the usage of domain-specific multilingual ontologies i for expanding queries and ii for enriching document representation with the index in a multilingual environment. We obtained monolingual baselines for each language pair by retrieving documents with TD queries formulated from search topics that are expressed in the same language as the documents. Scanning the papers of CLIR Track participants in TREC-9 and TREC-2001  , we observe a trend toward the fusion of multiple resources in an attempt to improve lexical coverage. It has even been suggested that CLIR evaluations may be measuring resource quality foremost or equivalently  , financial status 7. We also show how to use the alignments to extend the classical CLIR problem to a scenario where mono-and cross-language result lists are merged. The alignments are then used for building a cross-language information retrieval system  , and the results of this system using the TREC-6 CLIR data are given. In CLIR  , queries are translated from the source language to the target language  , and the original and translated queries are used to retrieve documents in both the source and targeted languages. Cross-language information retrieval CLIR has emerged as an important research area since the amount of multilingual web resources is increasing rapidly. This strategy works well with many relevant documents retrieved in the initial top n  , but is less successful when the initial retrieval effectiveness is poor  , which is commonly the case in CLIR where initial retrieval performance is affected by translation accuracy see  , e.g. In CLIR  , PRF can be used prior or post translation or both for pre/post-translation query expansion see  , 16. However  , research funding by such projects as TIDES 1   , indicates that there is a need  , within intelligence organisations at least  , for CLIR systems using poor translation resources and pivots. Many applications of CLIR rely on large bilingual translation resources for required language pairs. Contributions. Indeed  , the impressive CLIR performance was typically observed in the following settings: 1 test documents were general-domain news stories i.e. CLIR systems' proven ability to rank news stories might not transfer readily to other genres such as medical journal articles – a point also raised by 16. Let L1 be the source language and L2 be the target language in CLIR  , all our corpus-based methods consist of the following steps: 1. We outline the corpus-based CLIR methods and a MT-based approach  , with pointers to the literature where detailed descriptions can be found. Benchmarked using TREC 6 French to English CLIR task  , CLQS demonstrates higher effectiveness than the traditional query translation methods using either bilingual dictionary or commercial machine translation tools. To further test the quality of the suggested queries  , CLQS system is used as a query " translation " system in CLIR tasks. Davis and Dunning 1996 and Davis 1997 also found that the performance of MRD-based CLIR queries was much poorer than that of monolingual queries. Ballesteros and Croft 1997 studied the effect of corpus-based query expansion on CLIR performance  , and found that expansion helped to counteract the negative effects of translation failures. A common problem with past research on MT-based CLIR is that a direct comparison of retrieval results with other approaches is difficult because the lexical resources inside most commercial MT systems cannot be directly accessed. Past studies that used MT systems for CLIR include Oard  , 1998; Ballesteros and Croft  , 1998. Tools for CLIR such as dictionaries are not universally available in every language needed or in every domain covered in digital libraries. More recently the generalized vector space model has shown good potential for CLIR 6. Interestingly  , this assumption yielded good results in the English-F'rench CLIR runs. As reported in 24  , another interesting angle in the CLIR track is the approach taken by Cornell University wherein they exploit the fact that there are many similar looking words between French and English   , i.e. , near cognates. The purpose of this paper is to investigate the necessity of translating query terms  , which might differ from one term to another. This paper has proposed an approach to automatically translate unknown queries for CLIR using the dynamic Web as the corpus. The most important difference between them is the fact that CLIR is based on queries  , consisting of a few words only  , whereas in CLTC each class is defined by an extensive profile which may be seen as a weighted collection of documents. In developing techniques for CLTC  , we want to keep in mind the lessons learned in CLIR. At query time  , the CLIR system may perform the construction of three types of queries  , starting from the ones formulated by users  , based on the system configuration: 1. CLIR systems need to be robust enough to tackle textual variations or errors both at the query end and at the document end. Our method of fuzzy text search could be used in any type of CLIR system irrespective of their underlying retrieval models. Explicitly expressing term dependency relations has produced good results in monolingual retrieval 9  , 18   , but extending that idea to CLIR has not proven to be straightforward. Although word-by-word translation provides the starting point for query translation approaches to CLIR  , there has been much work on using term co-occurrence statistics to select the most appropriate translations 10  , 15  , 1  , 21 . Query translation approaches for cross-language information retrieval CLIR can be pursued either by applying a machine translation MT system or by using a token-to-token bilingual mapping. Experiments on the TREC-5/6 English-Chinese CLIR task show that our new approach yields promising although not statistically significant improvements over that baseline. Even for Spanish- Chinese CLIR  , we used the English projection to place documents of both languages in the reduced space where the actual CLIR-task is performed. In our method k is a parameter of the MDS-projection and results were computed by placing all test documents into the English maps. However  , when MRD translation was supplemented with parts-of-speech POS disambiguation  , or POS and corpus-based disambiguation   , CLIR queries performed much better. We propose that translating pieces of words sequences of n characters in a row  , called character n-grams can be as effective as translating words while conveying additional benefits for CLIR. Most cross-language information retrieval CLIR systems work by translating words from the source i.e. , query language to the target i.e. , document language. In pure thesaurus based retrieval  , documents and queries are matched through their thesaurus based representations   , with document representations derived by an indexer and query representations provided by users. In Oard's hierarchical classification scheme of the CLIR methods 17  , our work falls under the thesaurus based free-text CLIR category. As summarized by Schauble and Sheridan 24  the TREC- 6 CLIR results appear consistent with previous results in that the performances typically range between 50 and 75% of the corresponding monolingual baselines. Soergel describes a general framework for the use of multilingual thesauri in CLIR 27   , noting that a number of operational European systems employ multilingual thesauri such as UDC and LCSH for indexing and searching. Query translation  , which aims to translate queries in one language into another used in documents  , has been widely adopted in CLIR. We highlight that query terms needing no translation may result from intrinsical ineffectiveness in CLIR  , semantic recovery by query expansion  , or poor translation quality. Its correct Chinese translations result in average precision AP of 0.5914 for CLIR. Consider the query: " Peru President  , Fujimori  , bribery scandal  , the 2000 election  , exile abroad  , impeach  , Congress of Peru "   , which is obtained based on the description field from a NTCIR-5 English-Chinese CLIR topic after stop words removal. Section 4 then describes the design of an experiment in which three variants of meaning matching are compared to strong monolingual and CLIR baselines. Therefore  , our findings should only be interpreted as the meaning matching technique could potentially outperform one of the best known query translation techniques. One of the simplest yet well performing approaches to CLIR is based on query translation using an existing Statistical Machine Translation SMT system which is treated as a black box. Cross-Lingual Information Retrieval CLIR addresses the problem of ranking documents whose language differs from the query language. The fact that our approach outperformed one of the best commercial MT systems indicates that some specific translation tools designed for query translation in CLIR may be better than on-the-shelf MT systems. Through our experiments  , we showed that each of the above methods leads to some improvement  , and that the combined approach significantly improves CLIR performance. For evaluating the effectiveness of the CLIR system  , different standard metrics have been adopted. Since the evaluation of the Organic . Lingua CLIR system is based on the methodology introduced by CLEF 21 ,22  , the same metrics will be used for evaluating the described system. We also presented a revised version of the co-occurrence model. For patent search in compounding languages  , the CLIR effectiveness is usually lower than for other language pairs 3  , 7 . Since patents are often written in different languages  , cross-language information retrieval CLIR is usually an essential component of effective patent search. Recently  , approaches exploiting the use of semantics have been explored. In both works  , the results demonstrated that the idea of using domain specific resources for CLIR is promising. Both NUS and NIfWP queries were divided into two subtypes  , structured and unstructured queries. However  , the performance can be improved by supplemental methods and by structuring of queries. Thus  , the previous studies show that simple MRD-based CLIR queries perform poorly. CLIR typically involve translating queries from one language to another. Furthermore   , these texts are often mixed with English  , which makes detection of transliterated text quite difficult. We proposed and evaluated a novel approach to extracting bilingual terminology from comparable corpora in CLIR. Ongoing research includes word sense disambiguation  , phrasal translation and thesauri enrichment. Exploiting different translation models revealed to be highly effective. In this paper  , we proposed a method to leverage click-through data to extract query translation pairs. Moreover  , it can extract semantically relevant query translations to benefit CLIR. 2 reports the enhancement on CLIR by post-translation expansion. This observation has led to the development of cross-lingual query expansion CLQE techniques 2  , 16  , 18. From a statistical perspective  , the CLIR problem can be formulated as follows. English  , within a collection D. More formally  , documents should be ranked according to the posterior probability: This phenomenon motivates us to explore whether a query term should be translated or not. In other words  , query translation may cause deterioration of CLIR performance. Still others are affected by the translation quality obtained. Some should-not-betranslated terms inherently suffer from their ineffectiveness in CLIR. Score normalisation is not necessary for the web task  , but is relevant for other tasks like CLIR and topic tracking. A second experiment dealt with score normalisation. Conventionally CLIR approaches 4 ,7 ,8 ,12 ,21 have focused mainly on incorporating dictionaries and domain-specific bilingual corpora for query translation 6 ,10 ,18. Finally  , in Section 6  , we present our conclusions. This is also observed in our experiments. However  , recent studies show that CLIR results can be better than monolingual retrieval results 24. The advantage of the dictionary-based approach is also twofold. These properties make it the ideal search strategy in an interactive CLIR environment. We contrast and compare our recent work as CLIR/DLF postdoctoral fellows placed in three different institutions 2. Here  , we approach these questions from a practical standpoint. On English-Chinese CLIR  , our focus was put on finding effective ways for query translation. The best combination of them is used for our Chinese monolingual IR. 4a comparison of the retrieval results for the 25 queries. We have a large English-Chinese bilingual dictionary from LDC. Finally  , Section 8 states some conclusions. Section 6 presents experimental results and Section 7 compares the presented CLIR method to other statistical approaches found in the literature . The above experiment demonstrates the effectiveness of using CLQS to suggest relevant queries for CLIR enhancement. The 11-point P-R curves are drawn in Figure 3. The runs which do candidate selection fig. On the other hand  , a few topics especially topics 209 and 229 benefit strongly from the CLIR approach. Corpus based methods have also been investigated independent of dictionaries. Their research also supports the findings of Hull and Grefenstette 14 that phrase translations are important for CLIR. Another possible solution to the problem of translation ambiguity is by using word sense disambiguation. Applying the research results in that area will be helpful. Term disambiguation has been a subject of intensive study in CLIR Ballesteros  , 1998. The effect on CLIR queries was small  , as the Finnish queries did not have many phrases. Phrase identification probably favoured the baseline queries. Therefore the main task in CLIR is not translating sentences but translating phrases. Most IR queries are quite short  , i.e. , they are most words or phrases. Cross-Language Information Retrieval CLIR deals with the problem of finding documents written in a language different from the one used for query formulation. Finally  , Section 6 concludes. Two reports have measured retrieval performance as a function of resources for English-Chinese retrieval. A few investigations have examined the effect of resource size on CLIR performance. Therefore  , it gives a good indication on the possible impact on query translation. This expansion task is very similar to the translation selection in CLIR. The CLIR model described in 5 is based on the following decomposition: In particular  , the models proposed in 5  , 18  , 1 are considered. Large English- Chinese bilingual dictionaries are now available. For example  , " violation " in query #56 is translated to the more common " " rather than " -- " . Despite the reasonable average percentual increase  , most of the differences are not significant. The numbers in table 1 show that the CLIR approach in general outperforms our baseline. Groups experimenting with such approaches during this or former CLIR tracks include Eurospider  , IBM and the University of Montreal. Corpus-based approaches are also popular. A second approach we used for translation is based on automatic dictionary lookup. Overall  , both translations are quite adequate for CLIR. In this paper  , we explore several methods to improve query translation for English-Chinese CLIR. Finally  , we present our conclusion in Section 6. We evaluate the three proposed query translation models on CLIR experiments on TREC Chinese collections. Statistical significance test i.e. , t-test is also employed. This probably favoured the baseline queries. Pair-wise pvalues are shown in Table 4. Statistical t-test 13 is conducted to indicate whether the CLQS-based CLIR performs significantly better. In brief sum  , " to-translate-or-not-to-translate " is influenced by various and complicated causes. In CLIR  , given the expense of translation  , a user is likely to be interested in the top few retrieved documents. 1.0. However  , it is often a reasonable choice to transliterate certain OOV words  , especially the Named Entities NEs. There are several ways to cross the language barriers in CLIR systems. Nie 2 exposes in detail the need for cross-language and multilingual IR. Section 2 presents an overview of the works carried out in the field of CLIR systems. The remainder of the paper is structured as follows. The Arabic topics were used in our monolingual experiments and the English topics in our CLIR experiments. Each topic has three versions  , Arabic  , English and French. Within the project Twenty-One a system is built that supports Crosslanguage Information Retrieval CLIR. Currently disambiguation in Twenty-One can be pursued in four ways: Query translation is usually selected for practical reasons of eeciency. In CLIR  , essentially either queries or documents or both need to be translated from one language to another. Thecompared AveP and G AveP. Table 3summarises the results of our " swap " experiments using the NTCIR-3 CLIR Chinese and Japanese data. However  , it should be stressed that MT and IR have widely divergent concerns. At first glance  , MT seems to be the ideal tool for CLIR. Half of the topics shows an increase in average precision  , the other half a decrease. 5b and 5c seem to benefit more from the CLIR approach. Section 3 describes our CLIR experiments with and without our automatically discovered dictionary entries. In Section 2  , we present our transliteration techniques. Results are presented and discussed in Section 4. They concluded that even if the translation ambiguity were solved correctly  , only limited improvement can be obtained. 3 9 queries with monolingual average precision higher than CLIR. Our method is unable to deal with the translation of non-compositional NPs. This makes it worth finding how effective CHI is in CLIR when compared to WM1. The advantage of this calculation is its efficiency  , compared to that of WM1. 16  develops a cross-lingual relevancy model by leveraging the crosslingual co-occurrence statistics in parallel texts. In Section 3  , we presented a discriminative model for cross lingual query suggestion. The following list of user requirements related to CLIR was derived: Together with the observation notes  , the scenarios served to identify key factors for system design. In cross-language IR either documents or queries have to be translated. Other examples of the use of CLIR are given by Oard and Dorr 1996. The last section summarizes this work and outlines directions for future work. We induced a bilingual lexicon from the translated corpus by treating the translated corpus as a pseudo-parallel corpus. The system achieved roughly 90% of monolingual performance in retrieving Chinese documents and 85% in retrieving Spanish documents. We proposed and evaluated a probabilistic CLIR retrieval system. We define translation  , expansion  , and replacement features. CLIR features are the key to learning what characteristics make a term favorable or adverse for translation. Another group of useful features are CLIR features. Context features are effective through inspecting retrieval results  , but such features meantime suffer from higher cost of computation. To overcome the language barrier in cross-language information retrieval CLIR  , either queries or documents are translated into the language of their counterparts. Section 5 concludes this work. Thus we argue that the DICT model gives a reasonable baseline. However  , the MorphAdorner dictionary is an unusually large and clean knowledge base by CLIR standards. One reason is simply the cost of existing linguistic resources  , such as dictionaries. Another problem associated with the dictionary-based method is the problem in translating compound-noun phrases in a query. This is called the ambiguity problem in CLIR. In this section  , we describe the approach we have adopted for addressing the CLIR problem. Both resources are expressed with SKOS format. A gold standard that  , for each query  , provides the list of the relevant documents used to evaluate the results provided by the CLIR system. 3. SMT-based CLIR-methods clearly outperform all others. Table 2shows the performance of single retrieval systems according to MAP  , NDCG  , and PRES. In the following sections  , we first describe the system and the language resources employed for the TREC-8 CLIR track. Finally  , we summarize our work. In TREC-10 the Berkeley group participated only in the English-Arabic cross-language retrieval CLIR track. We refer the readers to the paper in 1 for details. one such technique of implementing fuzzy text search for CLIR to solve the above mentioned problems. ACM 978-1-60558-483-6/09/07. For the former  , the average precision was 0.28  , and for the latter 0.20. Previous studies McCarley  , 1999 suggested that such a combination can improve CLIR performance. Documents were then ranked based on the combined scores. Research in the area of cross-language information retrieval CLIR has focused mainly on methods for translating queries. Work at ETH has focused SB96  on using In particular we concentrate on the comparison of various query translation methods. We have explored a CLIR method for MEDLINE using only the multilingual Metathesaurus for query translation . The second and third query versions Q' Such approaches pursue the reduction of erroneous or irrelevant translations in hope that the CLIR performance could approach to that of monolingual information retrieval MIR. words translation 7. NTCIR test collection and SMART retrieval system were used to evaluate the proposed strategies in CLIR. EDR and EDICT bilingual Japanese-English dictionaries were used in translation. Results showed that larger lexicon sources  , phrase translation  , and disambiguation techniques improve CLIR performance significantly and consistently on TREC-9 corpus. Our results confirmed our intuition. However  , MT systems are available for only a few pairs of languages. A good MT system  , if available  , may perform query translation of reasonable quality for CLIR purposes. In comparison with MT  , this approach is more flexible. In this paper  , we investigated the possibility of replacing MT with a probabilistic model for CLIR. Many questions need to be answered. 3 Finally  , there are still rooms to improve the utilization of a probabilistic model for CLIR. This year we approached TREC Genomics using a cross language IR CLIR techniques. We expect better results when the initial concept recognition is more complete. The results have shown that the use of domain-specific resources for enriching the document representation and for performing a semantic expansion of queries is a suitable approach for improving the effectiveness of CLIR systems. However  , except for very early work with small databases 22   , there has been little empirical evaluation of multilingual thesauri controlled vocabularies in the context of free-text based CLIR  , particularIy when compared to dictionary and corpus-based methods. There have been three main approaches to CLIR: translation via machine translation tectilques ~ad94; parallel or comparable corpora-based methods lJX195aj LL90  , SB96  , and dictionary-based methods Sa172 ,Pev72  , HG96  , BC96. Increased availabMy of on-line text in languages other than English and increased multi-national collaboration have motivated research in cross-language information retrieval CLIR -the development of systems to perform retrieval across languages. CLIR has received more attention than any other querytime replacement problem in recent years  , and several effective techniques are now known. In this paper  , presently known techniques for query-time replacement are reviewed  , new techniques that leverage estimates of replacement probabilities are introduced  , and experiment results that demonstrate improved retrieval effectiveness in two applications Cross-Language Information Retrieval CLIR and retrieval of scanned documents based on Optical Character Recognition OCR are presented. In the last decade  , however  , with the growth in the number of Web users  , the need of facing the problem of the language barriers for exchanging information has notably increased and the need for CLIR systems in everyday life has become more and more clear the recent book by J.-Y. International organizations  , governments of multi-lingual countries  , to name the most important ones  , have been traditional users of CLIR systems. Since the main goal of the presented work consists of exploring the impact of domain-specific semantic resources on the effectiveness of CLIR systems  , in our investigations we will focus on the strategies for matching textual inputs to ontological concepts applied to both the query and the documents in the target collection rather than on the translation of the textual query. It is caused by that statistical features reflect the underlying distribution of translated terms in the document collection  , and also that CLIR features reveal the degree of translation necessity. Statistical features consistently achieve better R 2 than CLIR features  , which are followed by linguistic features R 2 of linguistic features is the same across different corpora since such properties remain still despite change of languages. We have demonstrated that using statistical term similarity measures to enhance the dictionary-based query-translation CLIR method  , particularly in term disambiguation and query expansion  , can significantly improve retrieval effectiveness. The result of our study suggests that the two major research issues in CLIR  , namely  , term ambiguity and phrase recognition and translation 3  , 4  , 10  , are also the main sources of problem in dictionary-based query translation techniques. Post-hoc CLIR results are reported on all 75 topics from TREC 2001 and TREC 2002. For the official CLIR runs we tried these following configurations: For the post-hoc experiments  , we used PSE  , pre-translation query expansion  , one of four methods Pirkola's method  , Weighted TF  , Weighted DF  , or Weighted TF/DF  , and a probability threshold that was varied between 0.1 and 0.7 in increments of 0.1. One reason is that ad-hoc CLEF tasks evaluate CLIR systems as a whole; there is no direct comparison of alternative solutions for specific system components  , such as translation strategies given a fixed set of translation resources  , or resource acquisition techniques given a fixed translation strategy. The CLEF evaluation campaigns are  , probably  , the largest and most comprehensive research initiative for CLIR; but they are far from being complete. Queries were automatically formed from the title and description elds  , and we automatically performed limited stop structure removal based on a list of typical stop structure observed in earlier TREC queries e.g. , A relevant document will contain". Examples of these approaches are presented in 3 and 4 where frequency statistics are used for selecting the translation of a term; contrariwise  , in 5 and 6 more sophisticated techniques exploiting term co-occurrence statistics are described. A plethora of literature about cross lingual information retrieval CLIR exists. Tanaka- Ishii and Nakagawa 32 developed a tool for language learners to perform multilingual search to find usage of foreign languages. These methods follow a very similar pattern: the query 28 or the target document set 3 is automatically translated and search is then performed using standard monolingual search. Migration requires the repeated conversion of a digital object into more stable or current file formats  , such as e.g. The Council of Library and Information Resources CLIR presented different kinds of risks for a migration project 18. Jeff Rothenberg together with CLIR 25  envision a framework of an ideal preservation surrounding for emulation. A example is to run Microsoft WORD 1.0 on a Linux operating system emulating Windows 3.1. Emulation requires sufficient knowledge from the user about the computer environment and dependencies of components. In 15 a cross-language medical information retrieval system has been implemented by exploiting for translations   , a thesaurus enriched with medical information. Since then  , research in CLIR has grown to cover a wider variety of languages and techniques. Their results showed that the effectiveness of cross-language retrieval was almost the same as that of monolingual retrieval. Such a technique has been shown to improve CLIR performance. A technique that can be used to alleviate the impact of the above problems is by identifying phrases in the query and translating them using a phrase dictionary. This more general problem will also be investigated in the CLIR track for the upcoming TREC-7 conference. In this case  , the alignments help overcome the problem of different RSV scales. Quality assessment independent of a specific application will be discussed in the following  , whereas an evaluation of the alignments for use in CLIR can be found in section 4. In this paper  , both ideas are investigated. There are various reasons for textual variations like spelling variations  , dialectal variations  , morphological variations etc. The English NL/S and NUWP queries that provided the basis for Finnish queries  , were also used as baselines for CLIR queries see Figure 1. The test MRDs were not used in this phase. The syn-operator treats its operand search keys as instances of the same key. The syn-operator was used in structured CLIR queries; the words of the same facet were combined by the syn-operator. One possible way by which structuring disambiguates CLIR queries is that it enforces " conjunctive " relationships between search keys. The latter requires a human interpreter to identify the concepts in the requests. Translating pieces of words seems odd. In this paper  , we proposed several approaches to improve dictionary-based query translation for CLIR. We also presented a method of translation selection based on the cohesion among translation words. The latter runs the decoder directly with the new weights. The former reuses hypergraphs/lattices produced with the MIRA-tuned weights and applies new weights to find an alternative  , CLIR-optimized  , derivation. WE-VS. Our new retrieval model which relies on the induction of word embeddings and their usage in the construction of query and document embeddings is described in sect. use the same families of models for both MoIR and CLIR. Sometimes such expressions are written identically in different languages and no translation is needed. We argue that the above conclusion does not hold in general. Translating the query  , while preserving the weights from 1. All of the correlation values exceed 0.6  , and therefore are statistically highly significant. shows Kendall's rank correlations with the NTCIR-3 CLIR Chinese data for all pairs of IR metrics considered in this study. We propose a novel approach to learning from comparable corpora and extracting a bilingual lexicon. Evaluations on Cross-Language Information Retrieval CLIR  , which consists of retrieving documents written in one language using queries written in another language  , is another interest. Successful translation of OOV terms is one of the challenges of CLIR. In Section 5  , we detail our experiments and the results we obtained; and Section 6 concludes the paper. A Chinese topic contains four parts: title  , description  , narrative and key words relevant to whole topic. We used the English document collection from the NTCIR- 4 1 CLIR task and the associated 50 Chinese training topics. This implies users would prefer them  , but the technique is rarely deployed in actual IR systems. Using these measures  , PRF appears beneficial in most CLIR experiments  , as using PRF seems to consistently produce higher average precision than baseline systems. So it is very interesting to compare the CLQS approach with the conventional query expansion approaches. A related research is to perform query expansion to enhance CLIR 2  , 18. Interestingly  , both systems obtained best results by using French as source language 4 . It can be seen that the proposed CLIR model favourably compares with competitors on both evaluation sets  , even if score differences are not statistically significant. Disambiguation of multiplesense terms by estimating co-occurrence for each chandi- date3 has also shown evident accuracy enhancement. In this section  , we discuss the effect of translating OOV and non-OOV query terms on CLIR. Given a query topic Qs = {s1  , s2  , ..  , sn}  , we denote its correct translation as For those ineffective OOV terms LRMIR < 0  , not-translating such terms is beneficial to CLIR performance. An extremely-effective OOV term sj LRMIR 0 is the term whose semantics cannot be recovered well r1 0. We also verify that translating should-be-translated terms indeed helps improve CLIR performance across various translation methods   , retrieval models  , and benchmarks. It shows that T is influenced by intrinsic ineffectiveness  , semantic recovery by query expansion  , or poor translation quality. According to the authors  , it appears that document translation performs at least as well as query translation. Unique angles in TREC-6 include document translation based CLIR 19  explored by the University of Maryland using the LO- GOS system. For TREC-6  , the CLIR track topics were developed centrally at NIST Schäuble and Sheridan  , 1998. For German  , texts from the Swiss newspaper "Neue Zürcher Zeitung" NZZ for 1994 were also added. They found that users were able to reliably assess the topical relevance of translated documents . Ogden and Davis 19 were among the first to study the utility of CLIR systems in interactive settings. Only the umd99b1" and umd99c1" runs contributed to the relevance assessment pools. We submitted ve oocial CLIR runs and scored an additional four unoocial runs locally  , as shown in Table 2. But in our CLIR system  , in some degree  , word disambiguation has not taken some obvious affect to retrieval efficiency. For machine translation  , word disambiguation should be a very important problem. After that  , we submit four runs for CLIR official evaluation this year. Finally  , our best run has achieved the mAP mean average precision of 0.3869  , which is about the same as the best result at that time. With our TREC-8 submission  , we are in a position to assess how well our techniques extend to European languages. For CLIR involving more than two languages  , we decompose the task into bilingual retrieval from the source language to the individual target languages  , then merge the retrieval results. 1998. While there is little research on using syntactic approaches for resolving translation ambiguity for CLIR  , linguistic structures have been successfully exploited in other applications. This model is adopted in this study for triple translations. This study explores the relationship between the quality of a translation resource and CLIR performance. The effect of resource quality on retrieval efficacy has received little attention in the literature. The upper two figures are for AP88-89 dataset  , and the lower two are for WSJ87-88 dataset. As indicated in Table 1Figure 1: Comparison of CLIR performance on homogeneous datasets using both short and long queries. Arabic  , the same retrieval system was also used for monolingual experiments. The TREC-9 collection contains articles published in Hong Kong Commercial Daily  , Hong Kong Daily News  , and Takungpao. As discussed earlier  , direct comparisons with other techniques have been a problem because lexicons in most MT systems are inaccessible. Studies that used MT systems for CLIR include Ballesteros and Croft 1998; Oard 1998. Table 3shows the retrieval results of our CLIR system on TREC5C and TREC9X. Throughout this paper  , we will use the TREC average noninterpolated precision to measure retrieval performance Voorhees  , 1997. Once a list of monolingual results has been retrieved in each collection   , all the lists are merged to produce a multilingual result list. CLIR on separate collections  , each for a language. In cultures where people speak both Chinese and English  , using mixed language is a common phenomenon. However  , the user of a CLIR system may be bilingual to some extent. The Council of Library and Information Resources CLIR presented different kinds of risks for a migration project 11. On English-Chinese CLIR of TREC5 and TREC6  , we obtained 75.55% of monolingual effectiveness using our approach. This is favorably comparable to the best effectiveness achieved in the previous Chinese TREC experiments. The main aim of our participation in the cross-language track this year was to try different combinations of various individual cross-language information retrieval CLIR approaches. We also revisited our merging approach  , trying out an alternative strategy. This is because even though we invested considerable effort  , we were not able to locate an offthe-shelf German Italian machine translation system. However  , we believe that MT cannot be the only solution to CLIR. We can thus quantify the accuracy of an observed rank correlation usingˆseusingˆ usingˆse boot . The average length of the titles is 3.3 terms which approximates the average length of short web queries. We therefore omitted Model 4 for the English- Chinese pair. Model 4 seeks to achieve better alignments by modeling systematic position variations; that is an expensive step not commonly done for CLIR experiments . The wordlist contains about 145 ,000 entries. Our baseline bilingual CLIR lexicon is based on EDICT 4   , a widely used Japanese-to-English wordlist that contains a list of Japanese words and their English translations. Thus  , we both use a Japanese corpus to validate the hypothetical katakana sequences. They use probabilities derived from the target language corpus to choose one transliteration  , reporting improved CLIR results  , similar to ours. The co-occurrence technique can also be used to reduce ambiguity of term translations. Combining phrase translation via phrase dictionary and co-occurrence disambiguation brings CLIR performance up to 79% of monolingual. Similarly to last year  , CLIR track participants were asked to retrieve documents from a multilingual pool containing documents in four different languages. The topic creation and results assessment sites for TREC-8 were: Another advantage of the proposed method is that it can automatically extract the popular sense of the polysemous queries. So they may help improve CLIR by leveraging the relevant queries frequently used by users. Nevertheless  , it is arguable that accurate query translation may not be necessary for CLIR. In addition  , word co-occurrence statistics in the target language has been leveraged for translation disambiguation 3  , 10  , 11  , 19. For example  , in 12  , syntactic dependency was exploited for resolving word sense ambiguity. Our experimental results will show that the probabilistic model may achieve comparable performances to the best MT systems. In this paper  , we will describe the construction of a probabilistic translation model using parallel texts and its use in CLIR. The results we have obtained already showed clearly the feasibility of using Web parallel documents for model training. We used a part of the parallel texts to train a small model  , and used the model for CLIR. Clearly for such a small collection the specific figures are neither reliable nor significant  , reported results should thus be regarded only as indicative. In previous work on direct word-for-word translation  , Ballesteros and Croft 1 reported CLIR effectiveness 60% below monolingual. Consequently  , we do not repeat the monolingual result in the rest of this paper. Our comparable results for the direct run indicated performance 81% below monolingual. A third approach receiving increasing attention is to automatically establish associations between queries and documents independent of language difference 6  , 10  , 211. Disambiguation strategies are typically employed to reduce translation errors. Thus  , our second measure is average interpolated precision at 0.10 recall. We utilize linguistic Ling  , statistical Stat  , and CLIR features f si of query term si to capture its characteristics from different aspects. Given two sets of terms x and y  , we measure their co-existence level by We compute TFIDF in both source and target language corpora for each term. Note that the English and Chinese documents are not parallel texts. NTCIR-4 and NTCIR-5 CLIR tasks also provide English and Chinese documents  , which are used as the source and target language corpora  , respectively. Realizing what factors determine translation necessity is important. Both tasks use topic models to retrieve similar documents. We first showcase DO and HSA on two document similarity tasks: prior-art patent search 10 and the cross-language IR CLIR task of finding document translations 4. We distinguish preretrieval and post-retrieval data merging methods. Multilingual data merging needs to be addressed in this work because the CLIR track requires a single ranked list of retrieved documents from data collections in four languages. A key resource for many approaches to cross-language information retrieval CLIR is a bilingual dictionary bidict. ABET also comes with a library of commonly used transformations  , e.g. , kill_parens to remove parenthesized expressions. Query translation research has developed along two broad directions  , typically referred to as " dictionary-based " and " corpus-based " techniques. The bad effectiveness in these cases is not due to translation  , but to the high difficulty of query topics. Researchers have used various language pairs Copyright is held by the author/owner. The aim of cross-language information retrieval CLIR is to use a query in one language to search a corpus in a different language. The probabilistic approach will be compared empirically with two popular CLIR techniques  , structural query translation and machine translation MT. The focus of this study is on empirical evaluation of the proposed system. In order to keep the size of the induced lexicon manageable  , a threshold 0.01 was used to discard low probability translations. Its performance is around 85% of monolingual retrieval. One area for future work is to improve our retrieval model by incorporating contextual information for better term translation. The paper will also offer explanations  , why these methods have positive effects. The use of the special dictionary and the general dictionary in query translation and structuring of queries are highly effective methods to improve the CLIR performance. This has a depressing effect on CLIR performance  , as such expressions are often prime keys in queries. Technical terms and proper names are often untranslatable due to the limited coverage of translation dictionaries. Besides the above phrase translation method  , we also use another two methods in our Chinese-English CLIR system: CEMT-based method and dictionary-based method. These results confirm our expectation. These context-sensitive token translation probabilities can then be used in the same way as context-independent probabilities. This technique is now routinely used in speech retrieval 7  , but we are not aware of its prior use for CLIR. Migration requires the repeated conversion of a digital object into more stable or current file format. The Council of Library and Information Resources CLIR presented different kinds of risks for a migration project 6. Section 3 discusses methods for evaluating the alignments and section 4 shows the application of alignments in a CLIR system. The remainder of the paper is structured as follows: section 2 discusses the approach for computing alignments. For application in a CLIR system  , pairs from classes 1 through 4 are likely to help for extracting good terms. This seems a bit low  , so that AP and SDA are probably too dissimilar for such use. A technique for translating queries indirectly using parallel corpora has been proposed by Sheridan & Ballerini 19  , 20. In this paper  , we present an approach facing the third scenario. We can group the possible CLIR scenarios into the following three main settings: 1. the document collection is monolingual  , but users can formulate queries in more than one language. This problem has been addressed in two different ways in the literature. Several studies recognized that the problem of translating OOV has a significant impact on the performance of CLIR systems 8 ,9. In the provided evaluation   , the gold standard was manually created by the domain experts. This further enrichment of the documents representation permits to increase the effectiveness of the CLIR system. Indeed  , while the Agrovoc ontology is used only for the automatic annotation of documents  , the Organic. Lingua one is exploited also for performing manual annotations. However  , the combined use of the two ontologies is destructive with respect to the use of the sole Organic. Lingua one. We have used it for three popular languages Hindi  , Bengali and Marathi which use Brahmi origin scripts. Additionally   , we test two decoding evaluation setups of search space rescoring and redecoding. The user may not be proficient at reading a foreign language  , so could not be expected to look through more than the top retrieved documents. Figure 2: Comparison of CLIR performance on heterogeneous datasets using both short and long queries. The left two figures are for short queries  , and the right two are for long queries. In 19  , for example  , an IR-like technique is used to find statistical association between words in two languages. The use of the combined dictionary is motivated by previous studies 9  , 17  , which showed that larger lexicon resource improves CLIR performance significantly. The resulting combined dictionary contains 401 ,477 English entries  , including 109 ,841 words  , and 291 ,636 phrases. However  , for the purposes of the experiments described here  , it was treated as a series of simple bilingual dictionaries 1 . There was some suggestion in the results that the three-way triangulated queries may have outperformed the direct translation. Hence  , this approach bears high potential for CLIR tasks. Table 8  , both in terms of the number of languages being covered and the number of alignment units available e.g. , about 5 million for Eurodicautom . Particular difficulties exist in languages where there are no clearly defined boundaries between words as is the case with Chinese text. For example  , AbdulJaleel and Larkey describe a transliteration technique 1  that they successfully applied in English- Arabic CLIR. Depending on the language  , it may be possible to deduce appropriate transliterated translations automatically. English stop words were removed from the English document collection  , and the Porter stemmer 13  was used to reduce words to stems. Our CLIR experiments used the Lucy search engine developed by the Search Engine Group 5 at RMIT University. This was done by adding the English OOV terms to the English queries and using our system to translate and then retrieve Chinese documents EO-C. No statistically significant improvements over the baseline were observed for the fine fax resolution or the standard fax resolution not shown. Therefore  , as with CLIR  , WTF/DF is clearly the preferred technique in this application. In section 4  , we describe the use of query expansion techniques. In section 3  , we describe in detail the proposed method --improved lexicon-based query term translation  , and compare with the method using a machine translation MT system in CLIR. These terms may help focus on the query topic and bring more translated terms that together are useful for disambiguating the translation. 3 9 queries with monolingual Avg. P higher than CLIR. This situation is very similar to some cases observed in TREC5&6  , where we encountered the terms such as " most-favor nation "  On its own the CLIR approach gives varying results: some topics benefit from the reweighting of important query terms and the expansion with tokens related to the detected biomedical concepts. The labels show the topic numbers. One Arabic monolingual run and four English-Arabic cross-language runs were submitted. Research on technical preservation issues is focused on two dominant strategies  , namely migration and emulation. the selection of the correct translation words from the dictionary. extracted from parallel sentences in French and English  , the performance of CLIR is improved. We investigate the retrieval ability of our new vector space retrieval model based on bilingual word embeddings by comparing it to the set of standard MoIR and CLIR models. 3.2. LM-UNI  , which was the best scoring MoIR model  , is now outscored by the other two models which rely on structured semantic representations. i WE-VS is now the best scoring single CLIR model across all evaluation runs. For a parallel corpus  , we use Brown et al's statistical machine translation models Brown et al  , 1993 to automatically induce a probabilistic bilingual lexicon. Levow and Oard  , 1999 studied the impact of lexicon coverage on CLIR performance. The system uses it automatically when no operator is specified. Disambiguation through increasing the weight of relevant search keys is an important way of disambiguation Hull  , 1997. This is made more critical as the number of languages represented in electronic media continues to expand . Combining either of these two expansion methods with query translation augmented by phrasal translation and co-occurrence disambiguation brings CLIR performance above 90% monolingual. Post-translation expansion and combining pre-and post-translation expansion enhance both recall and precision. Furthermore  , post-translation expansion is capable of improving CLQS-based CLIR. This indicates the higher effectiveness of CLQS in related term identification by leveraging a wide spectrum of resources. This study explores the effects of transitive retrieval and triangulation on no-translation cross-language retrieval. However  , the accuracy of query translation is not always perfect. Usage of correct translations shall help reveal the necessity of translation. The coefficient of determination R 2 measures how well future outcomes are likely to be predicted by the statistical models. As linguistic  , statistical and CLIR features are complementary  , we use all of the features in the following experiments. We use NTCIR-4 and NTCIR-5 English-Chinese tasks for evaluation and consider both <title> and <desc> fields as queries. One of our merits is that we consider comprehensive factors including linguistic   , statistical  , and CLIR aspects to predict T . In general these strategies yield performance scores in the range of 50 to 75% of the corresponding monolingual baselines. Another unique feature is the exploration of a new and automatic method for deriving word based transfer dictionaries from phrase based transfer dictionaries. In this study we presented a novel fuzzy translation technique based on automatically generated transformation rules and fuzzy matching. One of them is based on cognates  , for which untranslatable and/or similar terms in case of close languages are used for matching the query. According to 3  , four different strategies are typically used for CLIR. We argue that these variations can be captured by successfully matching training resources to target corpora. Challenges for domainspecific CLIR  , in particular the problem of distinguishing domainspecific meanings  , have been noted in 12. This simple scenario is modified in the context of CLIR  , where   , dN } consists of only those documents that are in the same language and script  , i.e. , for all k  , d k ∈ l1  , s1. This paper's main contribution is a novel approach to CTIR. This task is similar to cross-language information retrieval CLIR  , and so we will refer to it as cross-temporal retrieval CTIR. Since all of our models require large sets of relevance-ranked training data  , e.g. The evaluation metric is Mean Average Precision MAP. We evaluated our system on the TREC-5/6 CLIR task  , using a corpus of 164 ,778 Chinese documents and titles of the 54 English topics as queries. Informal tests " viewing the interaction with a CLIR system available on the Web ARCTOS and machine-translated web pages Google. The initial interface layout was based on proposed scenarios 2. " Therefore  , when translating these queries  , we use example-based method that may generate accurate translations. Although different resources or techniques are used  , all these methods try to generate the best target queries. Only the title and description fields of the topics were used in query formulation. Clearly a need for enhanced resources is felt. As a result  , queries translated using this method typically perform worse than the equivalent monolingual queries -referred to here as monolingual retrieval performance. Related work on alignment has been going on in the field of computational linguistics for a number of years. Evaluating document-level alignments can have fundamentally different goals. The full topic statements were used for all runs  , and the evaluation used relevance assessments for 21 queries. This makes the results directly comparable to the ones reported by participants of the TREC-6 CLIR task. This work is also situated within the general landscape of multilingual digital libraries. The knowledge source used in English-Chinese-oriented CLIR system mainly includes dictionary knowledge and Chinese Synonym Dictionary. In fact  , dictionary is a carrier of knowledge expression and storage  , which involves almost all information about vocabulary  , namely static information. In addition  , stopword list and word morphological resumption list are also utilized in our system. The studies reported in this paper continue to broaden the perspective by adding a focus on complex tasks with live multimedia content. In Section 2  , we describe the various components of CLIR systems  , existing approaches to the OOV problem  , and explain the ideas behind the extensions we have developed. The structure of the paper is a as follows. While each of the above phases involve different tech-niques  , they are all inter-related. Several authors 4  , 5  , 1  , 11  have proposed techniques to deal with OOV terms in CLIR  , and we summarize these below. Table 1provides some statistics of the data. Our experiments use two sets of data test collections and submitted runs from the NTCIR-3 CLIR track 9  , provided by National Institute of Informatics  , Japan. 3 report on CLIR experiments for French and Spanish using the same test collection as we do OHSUMED  , and the UMLS Metathesaurus for query translation  , achieving 71% of baseline for Spanish and 61 % for French. Eichmann et al. One principled solution to this problem is Pirkola's structured query method 6. One of the key challenges in CLIR is what to do when more than one possible translation is known. This is an encouraging result that shows the approach based on a probabilistic model may perform very well. Their system was one of the bests in TREC7 CLIR runs. multi Searcher deals with several CLIR issues. Due to the availability of the language resources needed for Arabic dictionary and parallel corpora aligned at sentence level 1  English was selected as test languages. We proposed a context-based CLIR tool  , to support the user  , in having a certain degree of confidence about the translation. A larger user study has already been designed and is underway. We performed one Chinese monolingual retrieval run and three English-Chinese cross-language retrieval runs. In TREC-9 we only participated in the English-Chinese cross-language information retrieval CLIR track. These are some of the questions we will address in our future research. We return to the issue of vocabulary coverage later in the paper. An underlying assumption in this approach is that the initial manual translation is accurate  , and that it can be unambiguously translated back to the original Japanese query. We had found that dividing the RSV by the query length helps to normalize scores across topics. However  , CLIR is a difficult problem to solve on the basis of MT alone: queries that users typically enter into a retrieval system are rarely complete sentences and provide little context for sense disambiguation. We employed the query translation approach to CLIR by translating the English queries and retrieve in monolingual Chinese. Twenty-five queries #55 to #79 were provided in both English and Chinese. The HuaJian MT translation is also shown  , and it is seen that it picks up 'air pollution' correctly but misses out the 'automobile' sense of 'auto'. The Natural Language Systems group at IBM participated in three tracks at TREC-8: ad hoc  , SDR and cross-language. Our CLIR participation involved both the French and English queries and included experiments with the merging strategy. Full document translation for large collections is impractical  , thus query translation is a viable alternative. Newly borrowed technical words and foreign proper names are often written in Japanese using a syllabic alphabet called katakana. We present here a case where new CLIR dictionary entries can be found with confidence. The results from our experimental evaluation shows our approach to be a promising alternative to the standard pipeline approach. In this section  , we present the results of our CLIR experiments on TREC Chinese corpora. In our experiments  , we used two versions of queries  , short only titles and long all the three fields. We focused on translation of phrases  , which has been demonstrated to be one of most effective ways to obtain more accurate translations. CLIR experiments in the literature have used multilingual   , document-aligned corpora  , where documents in one language are paired with their translation in the other. Weaker invariance will show up as less overlap in the band pattern. We used four graded-relevance data sets from the TREC robust track and the NTCIR CLIR task: some statistics are shown in Table 1. It can be observed that reducing the pool depth does N and R denote the number of judged nonrelevant and relevant documents. In many documents and requests for information  , technical terms and proper names are important text elements. This can be attributed to the presence of compounds  , which leads to higher rates of OOV compound For patent search in compounding languages  , the CLIR effectiveness is usually lower than for other language pairs 3  , 7 . However  , objectively benchmarking a query suggestion system is not a trivial task. This also shows the strong correspondence between the input French queries and English queries in the log. The improvement on TREC French to English CLIR task by using CLQS demonstrates the high quality of the suggested queries. For each English word a precise equivalent was given. As a result  , many nonrelevant documents are ranked high. Thus  , in unstructured CLIR queries unimportant search keys and irrelevant translation equivalents tend to dominate and depress the effect of important keys. McCarley  , 1999 studied both query and document translations and concluded the combination of the two translations can improve retrieval performance. In our experiments  , we used SYSTRAN version 3.0 http://www.systransoft.com for query and document translation. Extending this to CLIR is straightforward given a multilingual thesaurus. Selected English Phrases: therapy  , replacement Final English Query: causation  , cancer  , thorax  , estrogens   , therapy  , replacement Since we have follow up refinement steps in our CLIR approach  , we set M  , the number of concepts identified for each query  , to 15. Their concern was evaluated on a whole query  , whereas we think every single term has its own impact on CLIR performance. The translation quality and ease of query were taken into account. We denote tj as the corresponding translation of si in target language. Roughly speaking  , overall classification accuracy climbs up to 80.15% when all features are adopted. Based on the pre-trained model  , we'd like to test if we can improve the CLIR performance with 4 different translation strategies. Table 5shows the MAP results using translated queries for search. Each strategy generates its own tj given source term si. " Context features are useful for predicting translation quality. For a non-OOV term  , we show that if there exists an effective translation in dictionaries  , it is suggested that translating si would help CLIR performance. It is not worth taking a risk to translate a term if the term probably perform poorly in CLIR. How to efficiently translate unknown terms in short queries has  , therefore  , become a major challenge for real CLIR systems 4 ,7. These English terms were potential queries in the Chinese log that needed correct cross-language translations. This section presents two methods of combining dictionary and spelling evidence in the framework given by Eq. But combining these sources would presumably improve effectiveness of CTIR  , much as evidence combination has aided CLIR 25. In such a case  , thanks to using date windows  , the alignments could be extended without the need to discard old pairs. However  , the degrees of improvement are not similar for all the query sets. The results of our experiments demonstrate that the term-similarity based sense disambiguation does improve the retrieval performance of dictionary based CLIR performance. CLIR is concerned with the problem of a user formulating a query in one language in order to retrieve documents in several other languages. These translations can be used in normal search engines  , reducing the development costs. The proposed CLIR system manages a collection of documents containing multilingual information as well as user queries that may be performed in any language supported by the system. The CLIR system has been evaluated by adopting three different configurations and the results have been compared with the gold standard  , according to the metrics described above. Finally  , queries are performed on the Organic. Lingua document collections. This paper explores flat and hierarchical PBMT systems for query translation in CLIR. In terms of translation quality  , efficiency   , and practicality  , flat and hierarchical PBMT systems have become very popular  , partly due to successful open-source implementations. The lower similarity between CVMR and CVMF M can be explained by training data Table 3: Test results for combined CLIR models see Table 2. On Wikipedia data  , shown in the lower part of Table  3  , we find similar relations. Some dictionary-based and corpus-based methods perform almost as well as monolingual retrieval 7  , 8  , 9. From the CLIR viewpoint  , MT is not regarded as a promising approach. Future research includes collecting more interview data and developing a thesaurus of English terms used in CLIR to enhance traditional or monolingual controlled vocabularies. The contradictions identified from this study can inform the development of discovery platforms for multilingual content. There are three broad types of CLIR systems: those based on query translation  , those based on document translation  , and those that use some aspects of both 15. Rosetta uses real-time document translation and incremental indexing to accommodate live content. Our tests in TREC8 showed that using Web documents to train a probabilistic model is a reasonable approach. This suggests that probabilistic models are translation tools that are as valuable as MT systems for the CLIR purposes. We evaluated three multilingual data merging methods to obtain a single ranked list for the purpose of TREC-8 CLIR track submission. Table 2presents the retrieval performance statistics for the three runs. In distinction from the earlier TREC-5/6 Chinese corpus  , these sources were written in the traditional Chinese character set and encoded in BIG5. For TREC-9  , the CLIR task used Chinese documents from Hong Kong. In addition to the specific results reported by each research team  , the evaluation produced the first large Arabic information retrieval test collection. The TREC-2001 CLIR track focussed this year on searching Arabic documents using English  , French or Arabic queries. Finding a good monolingual IR method is a prerequisite for CLIR. Our work involved two aspects: Finding good methods for Chinese IR  , and finding effective translation means between English and Chinese. The MAP were cross-language runs  , not monolingual runs. There was some concern over the test collection built in the TREC 2001 CLIR track in that the judgment pools were not as complete as they ideally would be. It can reduce translation error by 45% over automatic translation bringing CLIR performance up from 42% to 68% of monolingual performance. Combming pre-and posttranslation expansion is most effective and improves precision and recall. This is still well below a monolingual baseline  , but irnprovedphrasrd translations should help to narrow the gap. This amounts to no sense disambiguation for query words. The simplest approach toward dictionary-based CLIR is to use all the translations of query words provided by the dictionary equally 5  , 6 . Table 1lists the average precision across 11 recall points for both the homogeneous collections and the heterogeneous collections. The second approach is to project document vectors from one language into another using cross-language information retrieval CLIR techniques. The downside  , however  , is that machine translation is typically time-consuming and resource-intensive. When we embarked on this line of research  , we did not find any publications addressing the area of Cross-Lingual Text Categorization as such. In CLIR  , we need a relevance model for both the source language and the target language. In monolingual IR this relevance model is estimated by taking a set of documents relevant to the query. The second can be obtained using either a parallel corpus or a bi-lingual lexicon giving translation probabilities. In CLTC  , for performing translations we shall have to use similar linguistic resources as in CLIR. In fact  , a class profile can be seen as an approximative unigram Language Model for the documents in that particular class. Since our resources are less than ideal  , should we compensate by implementing pre-and post-expansion ? Finally  , we combined the various transitive runs to determine whether triangulated retrieval is useful in the absence of translation resources. The corpora consisted of comparable news articles in Hindi  , Bengali  , and Marathi collected during 2004 to 2007. For evaluation  , we used the CLIR data released at the FIRE 1 workshop  , 2008. Our paired T-test results indicate that our retrieval scores are statistically significant. In this paper  , we described a Surface Similarity based method for fuzzy string matching for performing CLIR and were able to show good improvement in performance. The previous two subsections introduced sources of evidence that might help cross-temporal IR. More specifically  , the problem is considered solved if high-quality training resources parallel text  , online dictionaries  , multi-lingual thesauri  , etc. For some researchers  , these observations have lead to the optimistic conclusion that the CLIR problem is basically solved. Challenges for domainspecific CLIR  , in particular the problem of distinguishing domainspecific meanings  , have been noted in 12. more likely to be a person or entity vs. medical domain documents more likely to be a chemical. The remainder of the paper is organized as follows: we present our training and testing data in Section 2  , and our weighting criteria in Section 3. , di ,N } are documents in language li  , i.e. , for all k  , d i ,k ∈ li  , si. The commercial versions of the dictionaries were converted automatically to CLIR versions by removing from them all other material except for actual dictionary words. The medical dictionary contained 67 ,000 Finnish and English entry words. For these reasons  , a special dictionary alleviates the translation polysemy problem  , in which the translation of one source language word to many target language words causes fuzziness in CLIR queries. The terms of special dictionaries are often unambiguous. Future research should concentrate on finding methods by which the performance of CLIR queries could be improved further. By these means  , it is possible to solve successfully the translation polysemy and the dictionary coverage problems. Still another method that would be worth studying is data fusion; different translation methods produce different result lists. Also weighting methods should be tested: Does weighting affect CLIR queries similarly as monolingual queries ? There are two main scenarios where the user input could be incorporated into the system to enhance multilingual information retrieval: 1. In CLIR systems  , interactive components are crucial to accomplish search tasks 2. The test collections are the TREC5 Chinese track  , the TREC9 cross-lingual track and the TREC5 Spanish track Voorhees and Harman  , 1997; Voorhees and Harman  , 2000. It also appears that  , with this approach  , additional bilingual lexicons and parallel text improve performance substantially in spite of the increased ambiguity. To our knowledge  , this is the first systematic comparison of those models on the task of English to Chinese CLIR on gold test sets. We evaluate our query translation models using TREC collections . Kraaij 8 showed successful use of the widely used BableFish 6 translation service based on Systran. Without any English OOV terms  , our translated queries achieved 86.7% of the monolingual result. In order to assess the value of what we have done  , we tested the usefulness of the newly derived dictionaries on a medical document collection. Fujii and Ishikawa 7  use a different one-tomany English-string-to-Japanese-string mapping model. This work evaluated a number of search strategies for the retrieval of Arabic documents  , using the TREC Arabic corpus as the test bed. Example-based method can provide very good translation results but the similarity computation between sentences is quite complex. Hull & Grefenstette 10 demonstrated that the retrieval performance of queries produced using manual phrase translation was significantly better than that of queries produced by simple word-forword  dictionary-based translation. No tools such as part of speech taggers  , stemmers and separate corpora are involved. Though these works have brought significant improvement in translation accuracy  , they eventually tried to translate as many terms as possible  , which we believe is not always an effective approach in CLIR. able for short  , context-inadequate queries. Particularly  , they incorporate dictionaries   , bilingual corpora  , or the Web to estimate the probability of translation ptj|si  , Qs. Various translation methodologies such as phrasal translation or sense disambiguation have brought significant improvements in CLIR. The correct translations are available since NTCIR-4 and NTCIR-5 CLIR tasks provide both English and Chinese topics at the same time. The basic formulae are a straightforward generalization of Darwish's PSQ technique with one important difference: no translation direction is specified. The key insight between what we call meaning matching is to apply that same perspective directly to CLIR. An important reason for this is that there is an implicit query expansion effect during translation because related words/phrases may be added. This indicates that the coverage of the dictionary is still an important problem to be solved to improve the performance of CLIR. If these NPs are not stored in the dictionary  , they are most likely to be translated incorrectly. We will extensively use this property during the construction of our MoIR and CLIR models. Since all words share the embedding space  , semantic similarity between words may be computed both monolingually and across languages. Research on technical preservation issues is focused on two dominant strategies   , namely migration and emulation. Besides the well-known Precision and Recall measure  , other metrics are widely used in the IR community. We expect similar improvements on CLIR  , and this will be confirmed by our experiments. A similar idea has been applied successfully to statistical language modeling 5  , showing improved performance of the cache language model. For each query term  , we expand it by an additional term that has the highest cohesion value with the other words of the original query. This section tries to point out similarities and differences of the presented approach with respect to other statistical IR models presented in the literature. The CLIR model described in 5 is based on the following decomposition: Already  , the current results indicate that an automatically constructed parallel corpus may be a reasonable resource for CLIR. We expect that the model trained with all the parallel documents from the Web will perform better. It is difficult to construct more good MT systems to cover other languages. Consequently  , the performance on this topic is drastically reduced by incorporating the concept language model. In this paper  , decompounding German words is realized by an approach which has been employed in domain-specific CLIR 2. They conclude that translation could help patent retrieval  , but not always. The decompounding is based on selecting the decomposition with the smallest number of words and the highest decomposition probability . 2 It is helpful for CLIR since it can extract semantically relevant queries in target language. 1 It can acquire translations for some out of vocabulary OOV queries without any need for crawling web pages. One advantage of the proposed method is that it can extract relevant translations to benefit CLIR. The results indicate that our method can achieve acceptable results for queries in and out of dictionary. Typically  , queries are translated either using a bilingual dictionary 22  , a machine translation software 9 or a parallel corpus 20. Most approaches to CLIR perform a query translation followed by a monolingual IR. Even if this point of view is not original  , neither for IR 1 nor for CLIR Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. We sought to answer three questions: 1 what is the best that can be done using freely available resources; 2 how w ell does Pirkola's method for accommodating multiple candidate translations work on the TREC CLIR collection; and 3 would building a single index be more eeective than building separate indices for each language ? We participated in the main task of the CLIR track  , using an English query to create a single merged ranked list of English  , French  , German and Italian news stories for each of the 28 topics. McCarley 28 trained a statistical MT system from a parallel corpus  , applied it to perform QT and DT  , and showed that the combination of scores from QT and DT drastically improved either method alone. This work falls in both the last two streams of works  , borrowing from the former the advantages deriving from the usage of domain-specific terms in the query translation and from the latter the capability to exploit semantic knowledge for retrieving information. The assumption behind such mechanism is that queries are consistently used in one language. Only Translations: query terms are translated into the reference language used for retrieving documents. Prec@10 is the precision after 10 docs and the Mean Average Precision MAP. The question of how well the findings apply to a range of different collections remains open; however  , the fact that AP and SDA are quite dissimilar gives hope that a lot of data can be aligned. Translation polysemy is a phenomenon   , in which the number of word senses increases when a source language word is translated to a target language by replacing it with all of its target language equivalents. They found a 55% loss in average precision in queries translated word-by-word compared to the original queries. Often those search keys that have only one or two translations are the most important words of a request and  , vice versa  , those keys that have many translations are unimportant words. Note how the term o~feoporosis has relatively more weight in the structured queries. The following queries sd and gd translation = sd + gd translation of the topic " osteoporosis " represent all CLIR query types of the study and demonstrate the importance of structure in cross-language queries. This shows that even if a high-quality MT system is available  , our approach can still lead to additional improvement. In other words  , given the rank order produced through the use of one translation  , what would be the effect of treating the other word as part of the same cluster ? For the purposes of CLIR  , it seems clear that the appropriate basis for constructing a similarity function is the differential effect on retrieval if both terms were considered to represent the same concept. Thus  , a monolingual retrieval engine does not need to be altered after translating queries into the target language. where f w ,k ∈ R denotes the score for the k-th inter-lingual feature associated with w within the dim-dimensional shared inter-lingual embedding space. For EN→DE  , MAP is even slightly higher  , due to hyphenated compounds in the German translation of recovered topics  , i.e. For DE→EN  , QR achieves almost the same MAP compared to using OQ  , which demonstrates the usefulness of QR for CLIR. To investigate the scientific knowledge inherent in patent retrieval  , we also used the NTCIR-3 CLIR test collection consisting of two years of newspaper articles  , and compared the results obtained with different genres of documents. For these experiments  , we used open software toolkits to implemented nine existing retrieval models and re-examined the effectiveness of those models in the context of patent retrieval. Finally  , CLIR can be achieved by using the described document placement methods to place documents of different languages in the same map. Optionally  , an optimization procedure could be used to place a new document in the map preserving the ratios of its distances to all anchor documents as much as possible with respect to the distances in the original vector space. While languages like Chinese and Japanese use multiple scripts 24  , they may not illustrate the true complexity of the MSIR scenario envisaged here because there are standard rules and preferences for script usage and well defined spellings rules. Although MSIR has attained very little attention explicitly   , many tangentially related problems like CLIR and transliteration for IR do discuss some of the issues of MSIR. 10 used CLIR followed by MT to find domain-specific articles in a resource-rich language  , in order to use them for language modeling in a resource-poor language. Domain-specific language modeling has been used in speech recognition 1123  , with encouraging results. This calculation results in a matrix of term-term associations  , which we use for query translation in the same manner as the matrix of translation probabilities in WM1. In this paper  , we return to first principles to derive an approach to CLIR that is motivated by cross-language meaning matching. McCarley found that merging ranked lists generated using query translation and document translation yielded improved mean average precision over that achieved by either approach alone 11  , which suggests that bidirectional techniques are worth exploring . For example  , to find documentlangauge synonyms  , we computed: Because statistical wordto-word translation models were available for use in our CLIR experiments  , we elected to find candidate synonyms by looking for words in the same language that were linked by a common translation. We ran our Chinese-English experiments after the English- French experiments with the goal of confirming our results using a different language pair  , so we made a few changes to reduce computational costs. Figure 3shows the MAP of the top five official monolingual French runs from CLEF 2001. For the English-French CLIR experiments  , we computed the mean average precision MAP over 50 queries formulated from the CLEF 2001 topic set Topics 41-90. In addition  , the baseline PSQ technique exhibited the same decline in MAP near the tail of the translation probability distribution i.e. , at high cumulative probability thresholds that Darwish and Oard reported 4. The latter finding suggests the necessity of combining bidirectional translation with synonymy knowledge. We did run experiments for both language pairs and found PDT was at least as effective as PSQ  , but adding statistical synonymy knowledge to unidirectional translation could hurt CLIR performance. For the experiments reported below  , a greedy method was used  , with replacements retained in order of decreasing probability until a preset threshold on the cumulative probability was first exceeded. Two teams from the University of Massachusetts 9 and the University of Maryland 2 tried variants of this approach for Text Retrieval Conference's CLIR track in 2002. Even with a higher baseline of monolingual with expansion  , combining the CO method with expansion can still yield up to 88% of monolingual performance . Cross language information retrieval CLIR is often based on using a bilingual translation dictionary to translate queries from a source language to the target language in which the documents to be retrieved are written e.g. , 1. We discuss related work and future directions for this research in Section 5 and Section 6  , respectively. These solutions  , and others  , such as considering CLIR as spell- correction 2  , will all work reasonably well if the two languages in question are linguistically historically related and possess many cognates. Some implemented approaches to this problem are to pass an unknown query word unchanged into the translated query  , or to find a closest match to a known target word 4. We have been experimenting with a method for automatically creating candidate Japanese transliterated versions of English words. As proper names and technical terms are very important in many information retrieval queries  , for dictionary-based CLIR between Japanese and English  , it is imperative that foreign words be properly transliterated into and out of katakana. During term translation  , the translations of a term are also retrieved from this same bilingual lexicon. However the issue is more difficult in Chinese as many characters have the same sound  , and many English syllables do not have equivalent sounds in Chinese  , meaning that selecting the correct characters to represent a transliterated word can be problematic. As shown in Table 2  , the extracted top translations are closely related to the source query  , even though sometimes they are not the translation equivalent of the source query. Therefore  , if a candidate for CLQS appears often in the query log  , then it is more likely the appropriate one to be suggested. In particular  , if the user intends to perform CLIR  , then original query is even more likely to have its correspondent included in the target language query log. Despite the various types of resources used  , out-of-vocabulary OOV words and translation disambiguation are the two major bottlenecks for CLIR 20. To overcome this knowledge bottleneck  , web mining has been exploited in 7  , 27  to acquire English- Chinese term translations based on the observation that Chinese terms may co-occur with their English translations in the same web page. OOV word translation is a major knowledge bottleneck for query translation and CLIR. In the future  , we will build CLQS system between languages which may be more loosely correlated  , e.g. , English and Chinese  , and study the CLQS performance change due to the less strong correspondence among queries in such languages. This  , however  , does not compromise our results since our experiments are aimed at comparing the performance of two different CLIR methods and not at comparing different search engine architectures. With the vector space engine they employ  , their overall 11pt performance 0.24 is slightly above the one for the search engine we use 0.20. Moreover  , the search engine we employ is more in line with current clinical and Web retrieval engines and the requirements they have to fulfill. Given a query topic Qs = {s1  , s2  , ..  , sn} in source language   , conventional query translation methods endeavor to find a set of translated terms Qt = {t1  , t2  , ..  , tm} in target language. Translations with non-negative LRT D are regarded having good translation quality  , as they perform as well as or better than correct translation in the benchmarks. LRT D sj tells the influence of translating sj to t k Ds j  in CLIR. Documents of a comparable collection may be aligned at the document  , sentence or even word level. Major approaches for CLIR include bilingual dictionaries 3  , 7  , 141  , parallel collections 4  , 7  , 10  , 61 and comparable collections 26 or some combination of these. On the CLIR task  , due to the nature of the evaluation metric  , the computation time for MAP  , DO and HSA  , while being different for each metric  , is equal across the different model configurations. It is evident from this table that  , both DO and HSA  , are the most efficient metrics to compute compared to MAP and perplexity. The issue of CLIR has also been explored in the cultural heritage domain. Rather than seeking to map multilingual query terms  , Wang 50 studies the use of a web-based term translation approach to find translations for unknown cross-language queries in digital libraries. The question answering task in the interactive track of the Cross-Language Evaluation Forum iCLEF is an example of that more comprehensive perspective 8 . As the quality of machine translation improved  , the focus of CLIR user studies expanded from merely enabling users to find documents e.g. , for subsequent human translation to also support information use e.g. , by translating the full text. In TREC-9  , Microsoft Research China MSRCN  , together with Prof. Jian-Yun Nie from University of Montreal  , participated for the first time in the English- Chinese Cross-Language Information Retrieval CLIR track. We reused the same corpus-based methods that we utilized last year with considerable success  , while experimenting with using a number of off-the-shelf machine translation products.  The Salmone Arabic-to-English dictionary  , which was made available for use in the TREC-CLIR track by Tufts University. Together  , the two term lists covered about 15% of the unique Arabic stems in the AFP collection measured by using light stemming on both the term list and the collection. The first experiment CLARITdmwf used preretrieval data merging  , i.e. , we merged collections of English  , French  , German  , and Italian documents into a single multilingual data collection  , and indexed the multilingual collection. The goal of the track is to facilitate research on systems that are able to retrieve relevant documents regardless of the language a document happens to be written in. The task in the CLIR track is an ad hoc retrieval task in which the documents are in one language and the topics are in a different language. The LDC assessors judged each document in the pools using binary relevant/not relevant assessments. This presents a number of challenges  , primarily the problem of translation. Ballesteros 3 researched a transitive scheme and techniques to overcome word ambiguity. Using pivots doubles the number of translations performed in a CLIR system  , therefore  , increasing the likelihood of translation error  , caused mainly by incorrect identification of the senses of ambiguous words. The availability of test collection and translation resources was the overriding factor determining our choice of languages. Thus  , the collection used for this investigation was the English corpus from the TREC8 CLIR Track and the 28 German and English queries from the same track for which relevance judgements are available. Examination of it suggested that the best choice of query language was German  , as its vocabulary coverage in EuroWordNet was reasonable. The translation resource was EuroWordNet  , a multilingual thesaurus consisting of WordNets for various European languages including those used in TREC CLIR queries 20. Further examination indicated that Dutch  , Spanish  , and Italian were good choices as pivot languages since they offered the next best coverage in EuroWordNet. In this paper  , we investigate several approaches to translate an IR query into a different language. In addition to the classical IR tasks  , cross-language IR CLIR also requires that the query or the documents 7 be translated from a language into another. Compared to the dictionary-based translation  , a full-scale machine translation system has the advantage in that it can reduce the translation ambiguity of a query using the context information. Because of the first point  , the rarity of electronic sources for translation  , investigators may be drawn to use the resources most readily available to them  , rather than those best suited for bilingual retrieval. Regarding translation resources for CLIR  , we believe that two points are widely agreed upon:  resources are scarce and difficult to use; and  resources with greater lexical coverage are preferable. In order to differentiate the source language from the target language  , a superscript s is used for any variable related to the source language and a superscript t is used for any variable related to the target language. Similar to other CLIR papers  , " source language " refers to the language of queries  , and " target language " refers to the language of documents. All three were formed from the UN parallel corpus and the Buckwalter lexicon using the same procedure described in Section 3. To explore the impact of spelling normalization and Arabic stemming on CLIR  , we have compared three versions of bilingual lexicon creation for term translation. Apparently  , the small benefit of stemming and spelling normalization was canceled by the introduced ambiguity. In CEMT-based method  , we use a CEMT system named TransEasy 4 to translate the queries into English. We use a probabilistic cross-lingual retrieval system  , whose theoretical basis is probabilistic generation of a query in one language from a document in another. The goal of cross-lingual information retrieval CLIR is to find documents in one language for queries in another language. Following TREC-8  , the venue for European-language retrieval evaluation moved to Europe with the creation of the Cross-Language Evaluation Forum CLEF  , first held in Lisbon in September 2000 1. Once we had a dictionary in a suitable format  , we used it with our existing Dictionary-based Query Translation DQT routines to translate the query from English into the language of one of the four language-speciic CLIR subcollections no translation was needed for the English subcollection. We enabled English stemming for all runs and did not use any stopword lists. The first three of them are automatic query translation run  , using our word segmentation approach for indexing  , while the monolingual run we submit uses n-gram based segmentation. Figure 1.4 is the official precision and recall curve and the mAP score of our 4 CLIR runs. As an alternative  , we also explored three ways of incorporating translation probabilities directly into the formulae: 1. We implemented this by starting with the most likely translation and adding additional translations in order of decreasing probability until the cumulative probability of the selected translations reached a preset threshold that was determined through experimentation using the TREC-2001 CLIR collection. Experimental evaluation of the CLIR model were performed on the Italian-to-English bilingual track data used in the CLEF 2000 C0 and CLEF 2001 C1 evaluations. The number of relevant documents to be retrieved are 579 for C0 and 856 for C1. Darwish later extended Kwok's formulation to handle the case in which translation probabilities are available by weighting the TF and DF computations  , an approach he called probabilistic structured queries PSQ 4 In monolingual IR it is common to treat words that share a common stem as if they expressed the same meaning  , and some automated and interactive query expansion techniques can also be cast in this framework. However  , in both cases  , the best DAMM was statistically indistinguishable from the best IMM. However  , MAP of the best PSQ was just about 82% Chinese CLIR with 19% relative improvement  , achieving cross-language MAP comparable to monolingual baselines in both cases. There are other variants of cross-language meaning matching  , depending on translation in which direction is used and synonymy knowledge in which language is used. Same comparison of the best DAMM and the best PSQ in the English-Chinese CLIR experiments confirmed this finding. But for unrelated languages  , such as English and Japanese  , a word missing from the dictionary has little chance of matching any pertinent string in the other language text. Methods for translation have focused on three areas: dictionary translariun  , parallel or comparable corpora for generating a translation model  , and the employment of mnchine franslution MT techniques. It also played a large role in the TREC-8 experiments of a number of groups. As a result  , a query written in a source language likely has an equivalent in a query log in the target language. Our experiments showed that the decaying co-occurrence model performs better than the standard co-occurrence model  , and brings significant improvements over the simple dictionary approaches in CLIR. It differs from previous ones in that it includes a distance component that decays the mutual information between terms when the distance between them increases. The test written collection was from TREC-8 composed of English documents and queries in a number of European languages. However  , the relatively poor performance of the translation component of our test CLIR system was not a major concern to us  , as it remained a constant throughout our experiments. The other factor concerns the ability to choose the most common sense of a word  , this was not attempted using EuroWordNet and resulted in considerable erroneous translations. Such a study will help identify good candidate pivot languages. A non-technical issue of use of pivots that must be examined is a study of existing translation resources to determine the range of resources available to researchers and users of CLIR systems. When compared with previous results we see that Spanish CLIR using the Metathesaurus for query translation is on the high end of the performance range of 50- 75% of baseline scores observed with approaches based on dictionaries with or without information extracted from corpora 12  , 3  , 7  , 14. The must likely cause is difference in linguistic features. Yet 10  focused merely on evaluating the performance of a whole query and did not give insight into the effect of translation for each query term. The focus of previous works1  , 4 did key-term selection in the mono-lingual environment; however  , our discovery of various causes such as pre-and post-translation query expansion would influence the preference of translation in CLIR. Our work is also related to term selection from a query. Finally   , a larger R 2 can be achieved by including more features for training. InQuery's synonym operator was originally designed to support monolingual thesaurus expansion  , so it estimates TF and DF as follows 11 Pirkola appears to have been the first to try separately estimating TF and DF for query terms in a CLIR application 13  , using the InQuery synonym operator to implement what he called " structured queries. " In general  , language modeling approaches to retrieval rely on collection frequency CF in place of DF: Corpus-based approaches to CLIR have generally developed within a framework based on language modeling rather than vector space models  , at least in part because modern statistical translation frameworks offer a natural way of integrating translation and language models 19. Inclusion of rare translations in a CLIR application was shown to be problematic for all three methods  , however. Both Kwok's method and MDF were found to achieve retrieval effectiveness values similar to that obtained with Pirkola's structured query method  , so Kwok's method seems to be a good basis from which to build probabilistic structured query methods. Use of only the most likely of those translations turned out to be an effective expedient  , but only when an appropriate threshold on cumulative probability was selected. The combination of our approach with the MT system leads to a high effectiveness of 105% of that of monolingual IR. Experience has shown that several factors make it hard to obtain statistically significant results in CLIR evaluations . In order to avoid these limitations   , we chose to use a monolingual test collection for which translated queries are available  , and to base our evaluation on the largest possible number of topics. That will establish a lower bound on the performance of our system if it had direct access to the linguistic knowledge in the MT system. On the other hand  , the test set has only 25 queries and the difference between our system and the combined MT run is very small. Figure  1shows the results. To test whether CLIR systems that perform well in the news stories domain are robust enough to simply be used in a different domain  , we have compared SYSTRAN easiest  , most convenient choice that worked extremely well in past evaluation forums and two corpus-based methods trained on the Springer corpus. For retrieving newspaper articles  , we used <DESCRIPTION> and a combination of <DESCRIPTION> and <NARRATIVE>  , extracted from all 42 topics in the NTCIR-3 CLIR collection. For the purpose of formulating queries in patent retrieval  , we used the following combinations of topic fields independently: <DESCRIPTION>  , <DESCRIPTION>+<NARRATIVE>  , and <ARTICLE>+<SUPPLEMENT>. While NEs have been worked on extensively in IR and CLIR  , transliterated queries where the text  , in addition to NE  , is represented in the script of another language  , typically English  , have not received adequate attention. use a technique based on mapping term statistics before computing term weights 8  , 2  to establish a strong context-independent baseline . In future work we plan to try this approach for document translation where we would expect greater benefit from context  , although with higher computational cost  , at least in experimental settings. We evaluated our approach on the English-Chinese CLIR task of TREC-5/6: although we did not observe significant improvements  , we feel that this approach is nevertheless promising. Thus a person interested in the pedigree of the proverb many hands make light work will be able to find a broad range of variants on this theme  , from a range of historical periods using a single query. Two approaches can be distinguished: 1. translation-based systems either translate queries into the document language or languages  , or they translate documents into the query language 2. We further use the alignments to extend the classical CLIR problem to include the merging of mono-and cross-language retrieval results  , presenting the user with one multilingual result list. Results for the strategies just described on the TREC-6 CLIR collection are presented in the following: Figure 2shows a comparison of using alignments alone  , using a dictionary pseudo-translation and then using both methods combined  , i.e. This French-German run outperforms all of the few TREC-6 runs reported for this language combination by a wide margin. doing initial retrieval using a dictionary translation  , and then improving this translation using the alignments  , as outlined above. Cross-language retrieval supports the users of multilingual document collections by allowing them to submit queries in one language  , and retrieve documents in any of the languages covered by the retrieval system. In the following subsections  , we will present the results obtained with the different configurations adopter for evaluating the proposed CLIR system. The columns of each table show the Mean Average Precision  , the Precisions at 5  , 10  , 20  , and 30  , the Average Recall  , the Average R-Precision  , and the number of queries that have been performed. Indeed  , in all experiments performed on our document collection  , the usage sole or combined of the two described ontologies outperformed our baseline. Summarizing what we observed in our experiments  , we may state that the use of domain-specific multilingual resources for enriching basic CLIR systems leads to effective results. The implemented approach has been applied to a document collection built in the context of the Organic. Lingua EU-funded project where documents are domain-specific and where they have been annotated with concepts coming from domain-specific ontologies. We opt for ADD-BASIC as the composition model unless noted otherwise. This provides ground truth to evaluate the effectiveness of the two translation approaches discussed above: machine translation in this case  , we used Google Translate 1  and direct vector projection using the CLIR approach. In this manner  , we sampled 505 document pairs that are mutual translations of each other and therefore semantically similar by construction. Thus  , the collections in two languages are converted into a single collection of document vectors in the target language . Document vectors of the foreign language i.e. , German are projected into the target language English by the CLIR approach explained in Section 3. Thus  , the computation cost of the maximum coherence model is modest for real CLIR practice  , if not overestimated. Hence  , the number of non-zero translation probabilities in q is no more than the total number of translations provided by the bilingual dictionary for the query words  , which is usually much smaller than the product m s m t . Intrinsic to the problem is a need to transform the query  , document  , or both  , into a common terminological representation  , using available translation resources. Cross-Language Information Retrieval CLIR systems seek to identify pertinent information in a collection of documents containing material in languages other than the one in which the user articulated her query. The effectiveness of both corpus and dictionary-based resources was artificially lowered by randomly translating different proportions of query terms  , simulating variability in the coverage of resources. Since the main purpose of these experiments was to examine if the proposed approach can help conventional approaches for CLIR  , we simply used some basic techniques of query expansion and phrase translation in our experiments. Note the achieved MAP values can be further improved. The task of Cross-Language Information Retrieval CLIR addresses a situation when a query is posed in one language but the system is expected to return the documents written in another language. In Section 5 we test the performance of our model on the cross-language retrieval task of TREC9  , and compare our performance with results reported by other researchers. The goals of our fellowship are to raise awareness of the need for proper data management and preservation as well as to promote data curation as a professional activity. For CLIR  , the requirements are much less: It only requires the model to provide a list of the most probable translation words without taking into account syntactic aspects. For MT purposes  , the training corpus should be tightly controlled; otherwise  , wrong or poor-quality translations will be produced. The CLIR experiments on TREC collections show that the decaying co-occurrence method performs better than the basic cooccurrence method  , and the triple translation model brings additional improvements. Our evaluation results show that the triple translation is more precise than the word-by-word translation with the co-occurrence model. The remainder of this paper is organized as follows: Section 2 provides a brief description on the related work. As the problem of translation selection in CLIR is similar to this expansion task  , we can expect a similar effect with the decaying factor. These experiments show that the decaying factor allows us to better distinguish strong and weak term relationships. The research cited viewed pivots as an unfortunate necessity: their use allowed retrieval to take place  , but at the cost of much introduced error. In this paper we describe English-Japanese CLIR experiments using the standard BMIR-J2 Japanese text collection 4. SIGIR '99 6/99 Berkley  , CA  , USA 0 1999 ACM l-5611%096-1/99/0007. ,i5.00 able resources are available  , we do not consider them further in our current investigation. Our approach to CLIR takes advantage of machine translation MT to prepare a source-language query for use in a target-language retrieval task. Our TREC-8 results show that post-retrieval merging of retrieval results can outperform preretrieval merging of multilingual data collections. The success of dictionary-based CLIR depends on the coverage of the dictionary  , tools for conflating morphological variants  , phrase and proper name recognition  , as well as word sense disam- biguation 13 . Finally  , rather than acquiring bilateral word translations  , our focus lies on assigning subwords to interlingual semantic identifiers. We consider automatic lexicon acquisition techniques to be a key issue for any sort of dictionary-based efforts in IR  , CLIR in particular . In this paper we present a system for cross-lingual information retrieval CLIR working over the multilingual corpora of European Legislation Acquis Communautaire 1. Finally  , during the retrieval time  , EuroVoc thesaurus is used to let the user visually extend the query and rerank the results in real-time. For what concerns the query-document model  , this is often referred to as language model approach and has been already applied for monolingual IR see the extensive review in 19 and CLIR 5. Each Chinese query was segmented into words using the segmenters as described above  , the Chinese stop words were then removed from each Chinese query. These problems explain why CLIR effectiveness is usually lower than the monolingual runs  , even with the best translation tools of the world. However  , it is to be noted that the same problem also occurs for query translation with any tool MT or bilingual dictionary. On the other hand  , if we compare the probabilistic translation models with other translations means in particular  , with MT systems  , their performances are very close Nie99. We hope  , however  , that this will encourage these people to participate in the future  , thus increasing the size of the pool. These interfaces provide query translation from the source language into the target languages using bilingual dictionaries . However  , the involvement of the user in CLIR systems by reviewing and amending the query had been studied  , e.g. , using Keizai 4  , Mulinex 1 and recently MIRACLE 3. The document collection used in the TREC-2001 CLIR track consisted of 383 ,872 newswire stories that appeared on the Agence France Press AFP Arabic Newswire between 1994 and 2000. Many participating research teams reported results for word-only indexing  , making that condition useful as a baseline. The documents were represented in Unicode and encoded in UTF-8  , resulting in a 896 MB collection. In this paper we have provided an overview of that work in a way that will help readers recognize similarities and differences in the approaches taken by the Cross-language Information Retrieval CLIR is the task of finding documents that are written in one language e.g. , English using queries that are expressed in another e.g. , Chinese. We present a technique that transforms an unstructured bilingual dictionary into a structured one  , and experimental results obtained using that technique. 2Sakhr's Arabic/English CLIR system is one example an automated technique for converting an unstructured term-to-term translation dictionary into a structured dictionary. Dictionaries with such a structure may be available  , 2 and Section 3.2 presents 1In monolingual retrieval  , automatic query expansion techniques seek to achieve a similar effect. It is intuitive that the LM-UNI model will lead to much better results in the monolingual setting  , as the amount of shared words between different languages is typically very limited  , and therefore other representations for CLIR are sought 41 see next. In the actual implementation  , we operate with log probabilities . Each of the approaches has shown promise  , but also has disadvantages associated with it. Automatic dictionarytranslationsareattractivebecause they are cost effective and easy to perform  , resources are ily available  , and performance is similar to that of other CLIR methods. A better phrase translator should not alter our conclusion that query expansion can ameliorate the errors that occur in word-by-word or phrase   , 1996. Table 13shows the performance of each method as measured by average precision and percentage of monolingual performance  , LCA  , which typically expands queries with muki-term phrases  , is more sensitive to translation effects when pm-translation expansion is performed. LCA expansion gives higher precision at low recall levels  , which is important in a CLIR environ- ment. Usually it is simpler and more efficient to translate queries than to translate documents because queries are generally much shorter than documents. We shall demonstrate that linguistic units such as NP and dependency triples are beneficial to query translation if they can be detected and used properly. This is consistent with the observations on general reasoning: when more information is available and is used in reasoning  , we usually obtain better results. Our experiments of CLIR on TREC Chinese collections show that models using larger and more specific unit of translation are always better  , if the models can be well trained  , because more specific models could model more information. A comparison between the two approaches will show the advantages and disadvantages of using probabilistic term translation for CLIR. The major difference between our approach and structural query translation is that ours uses translation probabilities while the other treats all translations as equals. The results show that dialect similarity can also affect retrieval performance. Retrieval results using individual lexicons are significantly worse than those using the combination of the three lexical resources  , confirming findings by other researchers that lexicon coverage is critical for CLIR performance Levow and Oard  , 1999. SYSTRAN is generally accepted as one of the best commercial MT systems for English-Spanish translation. Therefore in the University of Tampere we have adopted the dictionary-based method for our CLIR studies. Dictionary-based translation is often easier way to implement query translation than the methods based on the comparable documents or the parallel corpora  , as these are not readily available. It is possible to address automatically the domain specific terms of queries to the correct dictionaries  , because different domains have different terminologies. In CLIR translation systems  , it is possible to use many dictionaries   , each of which have limited content  , but which together cover general language issues and many specific domains. Therefore  , as the study attacked the translation polysemy and the dictionary coverage problems  , the results are applicable to most languages  , even though phrases can lower the relative performance of CLIR in some languages. The third problem  , the coverage of dictionaries is not a linguistic problem and is in principle the same for all languages. Experimental results on a real clickthrough data show that the method can not only cover 413 the OOV queries out of 500 queries  , but also achieve 62.2% in top-1 to 80.0% in top-5 precision. We are interested in realizing: whether this nice characteristic makes it possible for the bilingual translations of a large number of unknown query terms to be automatically extracted; and whether the extracted bilingual translations if any can effectively improve CLIR performance. Many of them contain bilingual translations of proper nouns  , such as company names and personal names. The proposed approach was found to be effective in extracting correct translations of unknown query terms contained in the NTCIR-2 title queries and real-world Web queries. To determine the performance of the proposed approach when applied to CLIR  , we have conducted extensive experiments including the experiments with the NTCIR-2 English-Chinese IR task. A model-based approach usually utilizes the existing statistical machine translation models that were developed by the IBM group 3. Our empirical study with documents from ImageCLEF has shown that this approach is more effective than the translation-based approach that directly applies the online translation system to translate queries. In the study  , we examine the CLIR approach that learns a statistical translation model from an automatically generated parallel corpus by an online translation system. Figure 1shows that if one of the query terms is not translated x-axis  , how the corresponding AP y-axis changes using the correct translations of the rest of terms as a query. Moreover  , within each corpus setting  , we go into details to inspect the effectiveness using different features. Specifically  , leaving si untranslated could be a wise choice if its semantics could be recovered by pre-or post-translation expansion. Groups such as ETH 15  , and a collaboration between the University of Colorado  , Duke University and Microsoft 21 investigated corpus based methods. Multilingual thesauri can be built quite effectively by merging existing monolingual thesauri 27 ; the UMLS Metathesaurus is an excellent current example. As anticipated  , performance is still behind dictionary independent methods using parallel corpora lo. Computing DO and HSA on the PLTM model we achieve a relative speed improvement of 5.12 times over MAP. On the patent retrieval task  , following the experimental setup of 10  , model performance was evaluated using MAP computed over 372 queries and a test collection of 70k patents. While on the CLIR task PLTMs were configured with T=100  , 200  , 300  , 400  , 500  , 700 and 1k. However  , specific non-dictionary nouns and proper names often supply key evidence on the relevance of documents with respect to a query. In this case  , the distribution figures suggest that the TRT based fuzzy translation technique is viable in operational CLIR systems  , the noise being acceptable. precision 72.0%  , As shown  , 80% of the correct equivalents are within the set of four highest ranked words. A novel method for CLIR which exploits the structural similarity among MDS-based monolingual projections of a multilingual collection was proposed. Further experiments with larger datasets and more realistic queries are required to evaluate the practical implications of this theoretical advantage. In terms of computation  , the two methods are equally efficient since the joint and marginal probabilities used in computing PMI can be easily derived from the counts of A  , B  , C and D defined in 4.2. Comparative evaluation of PMI and CHI or IG in CLIR was not reported before. Another thread of research has focused on translating multiword expressions in order to deal with ambiguity 2  , 28. For example  , the industry standard leverages state-of-theart statistical machine translation SMT to translate the query into the target language  , in which standard retrieval is performed 4 . Cross-Language Information Retrieval CLIR needs to jointly optimize the tasks of translation and retrieval  , however   , it is standardly approached with a focus on one aspect. Instead  , we use specialized domains such as patents or Wikipedia where relevance information can be induced from the citation or link structure. We introduced a novel way to learn term translation probabilities from the top scoring " readings " of alternative query translations  , as generated by the decoder. Current methods of solving this problem have difficulty in tuning parameters and handling terms that are not registered in a dictionary  , when applied to large-scale and/or distributed digital libraries. In CLIR  , using the query translation approach  , the semantic ambiguity of a query can degrade the performance of retrieval. The findings can inform librarians  , information scientists  , and IR system designers of the needs  , requirements  , and approaches to enhance cross-language controlled vocabularies  , and improve search engines to provide users with more relevant results. Such records are also found in the Mainichi newspaper collection but they are excluded from the NTCIR-3 CLIR-J-J evaluation. Some MEDLINE records are extremely short and no abstract is provided  , although some of them are assessed as relevant to some topics. Despite such biases  , the MEDLINE collection seems to close to the Japanese newspaper collections see Table  5 rather than the Patent collections. The discussed approach uses domain-specific ontologies for increasing the effectiveness of already-available machine translation services like Microsoft Bing 1 and Google Translate 2  by expanding the queries with concepts coming from the ontologies. In this paper we consider a specific bi-language DL—the Niupepa 1 collection—and examine how the default language setting of the DL interface affects usage. Recent research on multi-language digital libraries has focused on cross-language information retrieval CLIR—retrieving documents written in one language through a query in a different language 1. Results and performances of different models and combinations are described in The proposed two-stages model using comparable corpora '4' showed a better improvement in average precision compared to '3'  , the simple model one stage and approached the performance of the dictionary-based model '2' with 79.02%. Because statistical wordto-word translation models were available for use in our CLIR experiments  , we elected to find candidate synonyms by looking for words in the same language that were linked by a common translation. We therefore explored one of the several possible sources of statistical evidence for synonymy. The importance of the technique and the study lies in it introduces a novel and effective way of using statistical translation knowledge for searching information across language boundaries. Based on the above consideration  , we apply example-based query phrase translation in our Chinese-English CLIR system  , and the experiments achieve good results. This might be the case when a query is very short  , or when specific domain terminology e.g. , medicine  , engineering is used. Ballesteros and Croft explored query expansion methods for CLIR and reported " combining pre-and post-translation expansion is most effective and improves precision and recall. " Their work only examined a single language pair English to Spanish  , and relied on the Collins's English-Spanish electronic dictionary. Some of the earliest work in CLIR was done by Salton 17 and Pevzner 13 who used thesauri to index and retrieve documents written in multiple languages. They found that posttranslation query expansion  , i.e. , query expansion on the translated queries  , and the combination-translation query expansion  , i.e. , query expansion on both the original and the translated queries  , are effective in improving CLIR performance. Ballesteros & Croft 3 proposed pre-translation  , post-translation and a combination of post and pre-translation query expansion techniques based on term co-occurrence. Our CLIR method uses an off-the-shelf IR system for indexing and retrieving the documents. We measured the effectiveness of our techniques in terms of average retrieval precision which was computed using the standard 11 recall-point measurement for TREC. The collection being searched is a combination of both German SDA and NZZ  , and therefore a superset of the one that was aligned to English AP or French SDA. A variety of research has also examined the multilingual mapping of different knowledge organization systems such as thesauri or subject headings in order to support CLIR in multilingual library collections. Clinchant8 expands the standard language modeling approach by representing more than one language in the document model and then using a meta-dictionary in order to build a matching multi-language query model. In this approach  , we investigated the following three problems: 1 word/term disambiguation using co-occurrence  , 2 phrase detecting using a statistical language model  , and In section 2  , we introduce briefly our work on finding the best indexing unit for Chinese IR. For the Cross-Lingual Arabic Information retrieval  , our automatic effort concentrated on the two categories; English-Arabic Cross-Language Information Retrieval CLIR and monolingual information retrieval. The First- Match FM technique is used for term selection from a given entry in the MRD 8. We plan to use 50 new topics in the same languages and to ask participating teams to also rerun the 25 topics from this year with their improved systems as a way of further enriching the existing pools of documents that have been judged for relevance. The TREC-2002 CLIR track will continue to focus on searching Arabic. While the libraries are focusing on the customization of existing tools  , such as the The CLIR/DLF fellow at Indiana University has been placed within the D2I Center as a liaison to the libraries. The development of data services at Indiana University is approached as an opportunity to engage multiple units within the university  , particularly the libraries  , IT services  , and computational centers. One of the projects that build upon the library-D2I partnership is the NSFfunded DataNet project  , called Sustainable Environment- Actionable Data SEAD. We have looked in detail at the OOV problem as it applies to Chinese-English and English-Chinese CLIR. Interestingly  , although the Web is constantly changing  , we were able to find most OOV terms  , many of which related to news events up to 10 years ago. The tracks consist of 33 and 47 topics  , respectively  , which are provided both in extended Title+Description+Narrative and synthetic Title+Description forms. The simplest approach is to retain the same formulae  , but to suppress the contribution of unlikely translations. Last year  , in TREC7  , we compared three possible approaches to CLIR for French and English  , namely  , the approach based on a bilingual dictionary  , the approach based on a machine translation MT system  , and the approach based on a probabilistic translation model using parallel texts. Finally we will give a description of some experimental results. We adopted MT-based query translation as our way of bridging the language gap between the source language SL and the target language TL. Retrieval effectiveness is commonly measured using either average precision across a series of recall values or at a fixed rank. -As we will see below  , it is relatively easy to obtain a suitable degree of query expansion based on translational ambiguity. For this  , a parallel corpus of lower quality still can provide reasonably good query translations. This paper proposed two statistical models for dealing with the problem of query translation ambiguity. Our work strongly suggests that a lexical triangulation approach to transitive translation can have a beneficial effect on retrieval. One approach to achieving this is to defer merging until after retrieval has taken place and fuse document rankings instead. Our results suggest that FMT can perform substantially better than DTL methods and is generally robust to a lack of linguistic structure in queries. Pirkola appears to have been the first to try separately estimating TF and DF for query terms in a CLIR application 13  , using the InQuery synonym operator to implement what he called " structured queries. " In general  , high TF and low DF are preferred  , with the optimal combination of those factors typically being determined through experimentation c.f. , 15. The paper then concludes with some notes on limitations of the new techniques and opportunities for future work on this problem. The effectiveness and efficiency of this strategy relative to comparable baselines is then shown in subsequent sections for two applications: CLIR  , and retrieval of scanned documents using OCR. Second  , it offers a principled way of tuning the degree of dictionary coverage to optimize the retrieval effectiveness. Among these  , WTF/DF achieved the greatest improvement 9.7% relative  , and exhibited the greatest range of threshold values over which the improvement was statistically significant 0.6 to 1.0. BWESG induces a shared cross-lingual embedding vector space in which words  , queries  , and documents may be presented in a uniform way as dense real-valued vectors. end  , we rely on two key modeling assumptions: 1 We treat documents and queries as bags of words and do not impose any syntactic information to the document structure. In summary  , we have created a unified framework for MoIR and CLIR which relies solely on word embeddings induced in an unsupervised fashion from document-aligned comparable data. 5 However  , for the clarity of presentation  , we have decided to stress the complete modeling analogy between the monolingual and cross-lingual approach to IR. We address this problem by discriminative training techniques which are widely used in the SMT community  , and use automatically constructed relevance judgments from linked data. In this work we argue that one should not only " look inside " the black box of the SMT system 16   , but directly optimize SMT for the CLIR task at hand. The main problems observed are: 1 the dictionary may have a poor coverage; and 2 it is difficult to select the correct translation of a word among all the translations provided by the dictionary. We first carried out a set of preliminary experiments to investigate the impact of lexicon sources  , phrase  , and ambiguity on query translation. We observe that the queries may be classified into three categories: 1 5 queries that have both monolingual and CLIR result of average precision lower than 0.1 #58  , #61  , #67  , #69  , and #77. One common approach  , known as "query translation ," is to translate each query term and then perform monolingnal retrieval in the language of the document 11. The vertical axis is the location of passages in the book with page 1 at the top. To understand the fingerprinting analogy  , imagine the documents of one language stacked on a pile  , next to a pile that has the translations in the same order as the original. Suppose we are interested in using the projections of figure 1 for performing CLIR of new documents  , any of the three monolingual maps can be actually used for the retrieval task. The idea behind the proposed methodology is to exploit structural similarities observed among the different monolingual projections computed with MDS to identify possible correspondences among new multilingual documents. Thus  , though there has been some interest in the past especially with respect to handling variation and normalization of transliterated text  , on the whole the challenge of IR in the mixed-script space is largely neglected. The results show that this new " translation " method is more effective than the traditional query translation method. Besides being benchmarked as an independent module  , the resulting CLQS system is tested as a new means of query " translation " in CLIR task on TREC collections. The remainder of this paper is organized as follows: Section 2 introduces the related work; Section 3 describes in detail the discriminative model for estimating cross-lingual query similarity; Section 4 presents a new CLIR approach using cross-lingual query suggestion as a bridge across language boundaries. The new CLIR performance in terms of average precision is shown in Table 3. In our experiments  , the top 10 terms are selected to expand the original query  , and the new query is used to search the collection for the second time. We now present our overall approach called SemanticTyper combining the approaches to textual and numeric data. This clearly illustrates the strength of our approach in handling noisy data. First  , we used the data extracted from DBpedia consisting of the 52 numeric & textual data properties of the City class to test our proposed overall approach SemanticTyper. We present the maximum MRR achieved by the approaches in each domain in Table 1we observe it occurs when training on all labelled data sources apart from the test source. It is evident from experimental results that our approach has much higher label prediction accuracy and is much more scalable in terms of training time than existing systems. Our approach called SemanticTyper is significantly different from approaches in past work in that we attempt to capture the distribution and hence characteristic properties of the data corresponding to a semantic label as a whole rather than extracting features from individual data values. This property makes the numerical model more reliable for future wing kinematics optimization studies. Whereas the quasi-steady model requires fitting coefficients   , this numerical model is rigorously derived from Navier Stokes equations and does not require fitting pa-rameters. If the model fitting has increased significantly  , then the predictor is kept. After adding each predictor  , a likelihood test is conducted to check whether the new predictor has increased the model fitting 6. The fitting constraint keeps the model parameters fit to the training data whereas the regularizers avoid overfitting  , making the model generalize better 7. The first summand is the fitting constraint  , while the rest constitutes the regularization. The fitting with this extended model is considerably better Fig. 14 As our model fitting procedure is greedy  , it can get trapped into local maxima. 7. Model fitting. where µt and Σt are prior mean and prior covariance matrix respectively. 5: Quantification of the fitting of oriented-Gabor model RMSE as defined in eq.   , βn be coefficients that are estimated by fitting the model to an existing " model building " data set  , where β0 is termed the model " intercept. " Our aspect model combines both collaborative and content information in model fitting. We have proposed the aspect model latent variable method for cold-start recommending. Our scope of machine learning is limited to the fitting of parameter values in previously prescribed models  , using prescribed model-fitting procedures. One type of cognitive tasks is machine learning. There can also be something specific to the examples added that adds confusion . Rather than over fitting to the limited number of examples  , users might be fitting a more general but less accurate model. Figure 3 gives the variance proportions for the sampled accounts . distributions amounts to fitting a model with squared loss. Table lsummerizes the results. By fitting a model to the generated time-series the AR coefficients were estimated. Our second challenge lies in fitting the models to our target graphs  , i.e. Model modifications are described in Section 3. By limiting the complexity of the model  , we discourage over-fitting. where λi's are the model parameters we need to estimate from the training data. The heuristic fitting provides matching of intuitive a priori assumptions on the system and determines the system model structure. The efforts are based on heuristic fitting the system model in order to obtain the required properties of the model to be used 27- 311. This has been observed in some early studies 8. 6 analyzed the potential of page authority by fitting an exponential model of page authority. Berberich et al. Dropout is used to prevent over-fitting. The sparsity parameter value has been adjusted to tune the model. Using deviance measures  , e.g. Iterative computation methods for fitting such a model to a table are described in Christensen 2 . The complete optimization objective used by this model is given in Table 1 . To avoid problems of over-fitting  , we regularize the model weights using L2 regularization. The mixed-effects model in Eq. The last line is explicitly fitting a mixedeffects model using the function lme in the nlme package. Model performance is demonstrated by emprical data. Section 4 concerns the data collection and fitting procedures for computation of leg model. In order to realize the personal fitting functions  , a surface model is adopted. Therefore  , in order to construct the model based pressure distribution image  , it is much easier to use the hollow model than the solid model. Computing the dK-2 distributions is also a factor  , but rarely contributes more than 1 hour to the total fitting time. In all cases  , model fitting runtime is dominated by the time required to generate candidate graphs as we search through the model parameter space. Nonetheless  , the scope of the Model involves one more fitting activity that  , in the outlying areas of interest of this universe  , complicates a fitting challenge per se. A deep redesign implementing the DELOS Reference Model2 must cover this lack  , as it is intended to be a common framework for the broad coverage of the digital library universe. Within the model selection  , each operation of reduction of topic terms results in a different model. By the language of model selection  , it is to select a model best fitting the given corpus and having good capability of generality. This could imply that with more examples to learn from  , users are more focused on a general model and less able to keep in mind particular cases. After estimating model parameters   , we have to determine the best fitting model from a set of candidate models. It allows us to estimate the models easily because model parameter inference can be done without evaluating the likelihood function. The model can be directly used to derive quantitative predictions about term and link occurrences. We have presented a predictive model of the Web based on a probabilistic decomposition  , along with a statistical model fitting procedure. For large graphs like ours  , there are no efficient solutions to determine if two graphs are physically identical . Applying MLE to graph model fitting  , however  , is very difficult. Existing model-fitting methods are typically batchbased i.e. , do not allow online update of parameters. Furthermore  , these methods have a number of other limitations. We deal with this problem by starting from multiple starting points. p~ ~  ,. ,  , m 10The computational strategy adopted for understanding a document consists of a hierarchical model fitting  , which limits the range of labelling possibilities. ~. Fitting an individiral skeleton model to its motion data is the routine identification task rary non-ridd pose with sparse featme points. to any application. Tanaka 1986 6 proposed the first macroscopic constitutive model. These models are based on basic thermodynamic theory and curve fitting of data from experiments. We generated AR 1 time-series of length 256. Figure 2billustrates the highest and second highest bid in the test set  , items that we did not observe when fitting the model. Figure 2awas taken from these data. Tuning λ ≥0 is theoretically justified for reducing model complexity  " the effective degree of freedom "  and avoiding over-fitting on training data 5. is the identity matrix. The shapes of the bodies are various for each person. By using the imported surface model  , the personal fitting function is thought to be realized. Although on a large scale the fitting is rather accurate  , the smaller and faster phenomena are not given enough attention in this model. IW is a simple way to deal with tensor windows by fitting the model independently. However  , the number of iterations until convergence can be large. A formal model: More specifically  , let the distribution associated with node w be Θw. This is a standard trade-off in fitting multiple models to data 8. Our own source code for fitting the two-way aspect model is available online 28. Recommendations to person p are made using: Pm|p ∝ Pp  , m. The RegularizerRole is played by a regularization function used to keep model complexity low and prevent over-fitting. quasi-Newton method. 4due to the unsuitable profile model. Figure 5a shows a failure in fitting the profile to the sensor data around P1 in Fig. Large η vales may lead to serious over-fitting. Small η values may cause the learning model over-sensitive to the training samples. We compared ECOWEB-FIT with the standard LV model. Next  , we discuss the quality of our approach in terms of fitting accuracy. The replicated examples were used both when fitting model parameters and when tuning the threshold. The weights tried were: w = 1 no upweighting  , w = 5  , and w = 6. There are two deficiencies in the fixed focal length model. Conduct curve fitting for sampled distance and zoom level as in Line segment primitives are efficient in modelling a collection of observations of the environment. The model is built by fitting primitives to sensory data. The next section will discuss the classification method. This labeling and model fitting is performed off-line and only once for each sensor. 1633-2008 for a fitting software reliability growth model. To calculate the failure probabilities of the subsystems  , we searched the IEEE Std. semi-supervised of the label observations by fitting the latent factor model BRI on the above three sources of evidences. unsupervised or only a fraction i.e. Model fitting and selection takes on average 7 ms  , and thus can be easily computed in real-time on a mobile robot. 12bottom. He had to use special hardware for real-time performance. He used residual functions for fitting projected model and features in the image. λU   , λI are the regularization parameters. Established methods for determining model structure are at best computationally intensive  , besides not easily automated. After fitting this model  , we use the parameters associated with each article to estimate it's quality. We use a model that separates observed voting data into confounding factors  , such as position and social influence bias  , and article-specific factors. To overcome the disadvantage some efforts have been taken. Dudek and Zhang 3 used a vision system to model the environment and extract positioning information. The maps were used to determine robot pose by fitting new sensor data to the model. In order to perform localization  , a model is constructed of how sensory data varies as a function of the robots position . One study built on the Wing-Kristofferson model to propose various model-fitting techniques for synchronization cases 16. In particular  , many researchers have focused on isolating synchronization behaviors in response to timing changes. The αinvesting rule can guarantee no model over-fitting and thus the accuracy of the final fitted model. In the searching step  , we test the variables using an α-investing rule and in a sequential manner. In this section we study the recommendation performance of ExpoMF by fitting the model to several datasets. Furthermore  , ExpoMF with content covariates outperforms a state-of-the-art document recommendation model 30. We provide further insights into ExpoMF's performance by exploring the resulting model fits. This stage aims to estimate the position of a model in the image plane  , calculating the distance between the image centre and the model position. The line fitting error can be approximated by circular Other work found that abrupt tempo changes and gradual tempo changes seem to engage different methods of phase correction 17. the likelihood ratio or χ 2 measure  , as a measure of the goodness-offit for a model  , the best-fitting  , parsimonious least number of dependencies model for the table is determined. However  , we found that the 4-parameter gravity model: By fitting the model to observed flows  , we might mask the very signal we hope to uncover  , that is  , the error. Using a curve fitting technique  , the impedance model was established in such a way that the model can simulate the expert behavior. The impedance with which a human expert manipulates a tool was identified by measuring the expert motion. Then  , by using a line fitting procedure  , a fitted line segment is used to model each clus- ter. Such a model is described in terms of the marginals it fits and the dependencies that are assumed to be present in the data. For different parameters  , it calculates the maximum probability that a parameterized model generates the data exactly matching the original  , and chooses the parameters that maximizes such probability. Model fitting on AE features was performed using WEKA 3.7 30  , and the response model was calculated in MATLAB. The contact event sets for the classifier are modeled as multinomial distributions 29 with nominal labels assigned to each event class. A data structure for organizing model features has been set up to facilitate model-based tracking. The location of the actual edge is then determined by fitting a line over all " peak " pixels associated with each visible edge. For a particular scene vertex the fitting test would then be triggered a number of times equal to the number of model LFSs  , in the worst case. Note that the plane fitting test could be as well used as a verification method in the event that no compatible scene vertices were detected. From the results  , it is evident that interactive fitting was far superior to manual fitting in task time and slightly better in accuracy. Pose orientation error was determined by measuring Ihe angular deviation of an axis of the model from the known ground truth axis direction. The purpose is to support the tasks of monitoring  , control  , prognostics  , preventive maintenance  , diagnostics  , corrective maintenance  , and enhancement or engineering improvements. Note the should be set to a number no smaller than in order to have enough fitting models for the model generation in a higher level. A summary hierarchy  As shown in the procedure  , to achieve the space limitation in the streaming environment  , the number of fitting models maintained at each level is limited to be the maximum number of . In particular  , if there are many non-informative attributes or if complex models are used  , the problem of over-fitting will be alleviated by reducing dimensions. Because the number of model parameters to be learned grows in accordance with K  , the acquired functions might not perform well when sorting unseen objects due to over-fitting. Although our plane fitting test is fast  , the time overhead that such an approach would introduce made us avoid its usage in such cases. We first fit the general model by fitting it to the general distribution of the minutes between a retweet and the original tweet. While there may be statistical issues with fitting and evaluating the models on the same data if one wanted to use the models for predictive behavior   , we are here particularly interested in which models best fit and hence explain the data. The goodness of fit test of the model was not significant p=0.64 meaning that predicted and observed data matrixes did resemble each other. Model fitting information was significant p=0.000 indicating that the final model predicts significantly better the odds of interest levels compared to the model with only the intercept. Statistical model selection tries to find the right balance between the complexity of a model corresponding to the number of parameters  , and the fitness of the data to the selected model  , which corresponds to the likelihood of the data being generated by the given model. However  , to provide a better data fitting  , more expensive models including more parameters are needed. Nagelkerke pseudo R 2 was 0.35  , which hints that the model explains about 35 % of the variation in interest scores. Since the LV model cannot capture seasonal patterns  , it was strongly affected by multiple spikes and failed to capture co-evolving dynamics. As shown in the figure  , our approach achieved high fitting accuracy. Fitting the Rated Clicks Model to predict click probabilities on the original lower results yields similar results. We observe a slightly positive effect from abstract bolding  , although the effect is not significant with 95% confidence. σ is used for penalizing large parameter values. Λ is the vector of model parameters  , the second term is the regularization term to avoid over fitting  , which imposes a zero prior on all the parameter values. It is clear that this particular view selection may not be optimal . A 980-node surface model is then computed by fitting a deformable surface as shown in Figure 12b. The tyre-dependent parameters were experimentally adjusted fitting the measured responses of the army vehicle off-road tyre 13. A detailed model of the tyre friction forces was incorporated in the simulation. Rehg 4 implemented a system called DigitEyes which tracked an unadorned hand using line and point features . There are something good and something bad. This fitting method makes the edge of the model more smooth and more approximate to that of the part than the zero-order-hold  , and makes using thicker material possible. Hence  , by leveraging the objective function  , we can address the sparsity problem of check-in data  , without directly fitting zero check-ins. Therefore  , the unvisited POIs also contribute to learning the model  , while they are ignored in conventional MF. This requires segmenting the data into groups and selecting the model most appropriate for each group. By fitting data to parameterized models  , surface or boundary-based representations impose strong geometric assumptions on the sensor data. An alternative to template based matching is fitting of a motion model to a gradient field the motion field. are quasi-static and the object is supposed to be planar or at least convex. As will be shown  , this results in a simple highly generalisable model fitting the majority of the data. Although we require the original target variable to do this  , an important property is demonstrated. Figure 2gives an example of the summary hierarchy. 2In the real-time walk of a legged robot  , a ground model should first be established during the previous gait period. Note that the fitting curve and the average error are shown in Fig. The uneven surface of the vermiculite does not lend itself to primitive fitting without a severe reduction in surface location accuracy. The model image shows the results of surfacing from range data. The success with which web pages attract in-links from others in a given period becomes an indicator of the page authority in the future. Addi-tionally  , we use a regularization parameter κ set to 0.01; this step has been found to provide better model fitting and faster convergence. We note that this results in faster convergence for the already computed dimensions. One of our contributions is that we propose to use hierarchical regularization to avoid overfiting. In principle  , the optimal K should provide the best trade-off between fitting bias and model complexity. The SRS was placed in hallways within the model. For these tests  , the ceiling was left off to aid in viewing  , but would in practice provide information for the fitting routine. Image curves are represented by invariant shape descriptors  , which allow direct indexing into a model library. An invariant fitting theorem which works for algebraic curves of any degree was introduced. Figure 6 : One wave length error detection using the reflection model. Calculate angle and distance to the reflecting point by fitting TOFs of the same objects with Formula 3  , and finding L and 60 Fig.4. To fit a tag ti's language model we analyze the set of tweets containing ti  , fitting a multinomial over the vocabulary words  , with probability vector Θi. We induce m language models  , one per hashtag. In our experiments we randomly split the movies into a training set and a test set. We speed up model fitting by considering only actors billed in the top ten and eliminating any actors who appear in only one movie. Finally  , we obtained the following model for λ: We started with all possibly relevant variables: After fitting to the data we found that the number of children had little influence. Log-likelihood LL is widely used to measure model fitness . Fitting different models for navigational and informational queries leads to 2.5% better LL for DCM compared with a previous implementation in 7 on the same data set average LL = -1.327. A hierarchical structure to the data alone does not completely motivate hierarchical modeling. Once one moves to the campaign level the number of terms starts to be large enough to support model fitting. The funding model to support this evolution  , however  , is not yet established. Still  , these repositories need to keep evolving in order to avoid techniques over-fitting the body of artifacts available and to better represent the universe of artifacts. adjusting for more usage characteristics resulted in less accurate predictions  , discussed further in Section 8. Second  , we wanted to prevent over-fitting of the field defect prediction adjustment model i.e. Given their small size  , we were forced to use a relatively simple model with a small number of features to avoid over-fitting. We used the TREC 2009 web track ad hoc queries and judgments as our training data. For example  , the performance with K = 30 is worse than the that with K = 20. We also consider its stochastic counterpart SGBDT  , by fitting trees considering a random subset of training data thus reducing the variance of the final model. Since our task is classification  , we optimize for the deviance loss function 9. It should be noted that a steady-state friction model can also be obtained using any other curve fitting technique such as those using polynomial models. These values are listed in Table II. This difference allows us to avoid the complexities of rigid motion manipulations while we are fitting the image. We also express the model constraints in a coordinate invariant form as pairwise relations between primitives. For a given temperature rise  , free strain recovery of SMA wire can be calculated using Brinson's one dimensional constitutive model With reduced dimensions  , the generalization ability can be improved. All estimates are made using 500 bootstrap samples on the human rated data. Table 2shows the results of fitting the Rated Clicks Model using human rated Fair Pairs data. To avoid over-fitting  , we constrain the gis by imposing an L2 penalty term. Such a model generalize to new campaigns if we can estimate the unknown coefficients gi for each user feature i from the training data. 1 is to maximize the log-likelihood of the training data. In this paper  , the primary purpose of fitting a model is not prediction  , but to provide a quantitative means to identify sub-populations. The choice is motivated bytheshape of the observed reliability growth curve. The reward is a repository that offers the powerful extensibility of COMZActiveX  , without requiring many new extensibility features of its own. Fitting an OODB or repository into an existing object model is a delicate activity  , which we explain in detail. These landmarks are found for both the reference map and the current map. Corner landmarks in the map are found with a least-squares model fitting approach that fits corner models to the edge data in the map. The surface geometry of a patch is determined by fitting the data points in the patch to a quadric surface and solving an eigensystem. The procedural model is fast  , robust  , and easy to maintain. Hence the cross-axis effect of y-acceleration on the x-axis may be modeled by the least-squares fitting of a secondarder polynomial to the data  , The result of this model is shown in Fig. 3 3 is the planestress model with these parameters  , not an arbitrary best fitting curve. An approximate line load was applied normally at 0.6 mm steps along 2 while recording one tactel dots in Fig. This set contains all consistent values of the model parameters  , so it is a quantitative description of the fitting error. A bounded sensor observation  , instead of lending statistical weight to some parameter vector  , constrains the parameters to a set. Although there are many formats  , which describe surface models  , in this paper Object file of Wavefront's Advanced Visualizer is adopted. Traditionally  , motion fields have been very noise sensitive as minimization over small regions results in noisy estimates. 1  , I measured the between-within variance for the 10 blogs in the dataset on estimated values for the trust  , liking  , involvement and benevolence latent variables. Prior to fitting the 19 measurements in the model in Fig. the current model—support incompatibility and non-convexity— and developed new models that address them. Since all retrieval runs tend to be truncated for practical reasons  , truncation is an important factor for fitting any distribution. The regularizer with coefficient λ > 0 is used to prevent model over-fitting. where Iij is an indicator whose value is 1 when consumer i purchased good j in the dataset  , and 0 otherwise. By varying the value of T we can control the trade-off between data likelihood and over-fitting. Then the model chooses T template configurations from the candidate pool  , θ  , to best explain the generation of queries. For each target graph  , we apply the fitting mechanism described in Section 4 to compute the best parameters for each model. We evaluate the six graph models using the Facebook graphs listed in Table 1 . The most common approach is directly fitting Ut to the actual query execution time of the ranking model 7. Several previous studies have proposed strategies for estimating retrieval costs 7  , 25. It provides additional flexibility in fitting either of these models to the realities of retrieval. Obviously  , this type of distortion can also be applied to the ellipsoidal model of Chavarria Garza. The LossRole is played by a loss function that defines the penalty of miss-prediction  , e.g. Another possible direction for this work is fitting the features onto a global object model. For example  , if a fingertip encounters a ridge  , some specific strategies may be used to determine the size and extent length of the feature . The model also includes computation of the aligning torque M z on each steered wheel. Our approach is attractive for the marketing field  , because the unobserved baseline sales  , marketing promotion effects and other specific effects are estimated by simultaneously. Fitting the proposed model to POS data  , interesting and practically important results are obtained. Our proposal for step 6 is inspired on the PAC 10 method to evaluate learning performance. Step 5 is improved using a model selection criterium to mitigate the over-fitting problem. We have tested the effectiveness of the proposed model using real data. By fitting the output of our proposed model to the real bid change logs obtained from commercial search engines   , we will be able to learn these parameters  , and then use the learned model to predict the bid behavior change in the future. In other words  , the learning trajectories significantly differ among the three initial conditions  , thus supporting Hypothesis 5. For all the projects there is a significant difference between the simpler model in Equation 4 and the model in Equation 3  , showing that fitting curves separately for different initial conditions significantly improves the model fit. The Adjusted-R 2 measure denotes the percentage of variance explained by the model and  , for both collections  , the obtained model explains 99% of such variance. The last one was the model that best fitted D δ   , and its parameters are presented in Table 2  , along with the goodness of fitting measure Adjusted-R 2 . Our model construction approach was similar to the so-called growth modelling 6  , in which first null models without predictors are fitted and then both random and fixed factors are progressively introduced to the model. The results of fitting the heteroscedastic model in the data can be viewed below  , > summarylme2 Apart from the random and fixed effects section  , there is a Variance function section. The very small p-value of the likelihood ratio statistic confirms that the heteroscedastic model explains the data significantly better than the homoscedastic model. Columns show project  , model 1 -the full model in Equation 3 and 2 -the simplified model from Equation 4  , degrees of freedom  , log-Likelihood  , likelihood ratio  , and p-value for the test comparing the full and the simplified models. The results could he dismissed as merely another example of over-fitting  , except that the type of over-fitting is highly specific  , and occurs due to confounding controllable mechanisms with the uncontrollable environment. This illustrates a flaw in the model-free learning system paradigm: failing to separate controllable mechanisms from uncontrollable environment can lead to learning a controller that is fragile with respect to the behaviour of the environ- ment. We conclude with literature review in Section 8 and discussion. The model used to compose a project from software changes is introduced in Section 4; Section 5 describes the result of fitting such models to actual projects; Section 6 considers ways to validate these empirical results  , and Section 7 outlines steps needed to model other software projects. In this paper  , in order to cope with a personal variety of the shape of the body  , a surface model  , which fits the bedridden person  , is imported to the tracking system . This type of approach includes techniques such as least squares fitting 19 and Iterative Closest Point ICP 1 allowing the determination of the six degree of freedom transformation between the observed points and the model. We take a different approach of matching a model to the observed points  , commonly used in the robotics community. Next we model the O2 concentration signal based on all inputs  , but WIA2 fuel mass and SIC2 feeding screw rpm measurements were replaced by the estimated mass flow signal see Fig. Finally  , our model can be used to provide a measure of the triadic closure strength differentially between graph collections  , investigating the difference in opt for the subgraph frequencies of different graph collections. In this way  , the procedure is in fact fitting the 'mean curve' of the model distribution to the empirical subgraph frequencies. Second  , single-point estimates do not help inference of model parameters  , and may in fact hurt if the ensuing model-fitting stage uses them as its input. On the other hand  , BaySail is able to provide full distributional information  , which avoids these problems. For this reason   , the model LFSs are placed in the LFS list of the model database in descending order of the area of the surface to which they correspond. It is desirable to use the simplest friction model in order to avoid computational complexity. As might have been predicted by the fitting results in Section 3.1  , it was found that use of a Hertz contact model to predict subsurface strains resulted in a biased estimate of the indenter radius. This indicates that the information about curvature is contained in the data  , however the model used to estimate curvature is not quite correct. The method of estimating the lots delively cycle time can help fab managers for more precisely lots management and AMHS control. Good curve fitting results are achieved with R square of 0.869 in the priority job model and with R squire of 0.889 in the regular job model. The good fitting between the experimental results and the model indicates that the model is quite accurate  , and may allow to make extrapolations to predict the actuator performance when it is scaled down to the target size for the arthroscope. There is a very good fit between the sequence of actual positions of the instrument tip and the theoretical values. For simplicity  , we consider only the angular constraints imposed by the model on the local optima; only the orientations of the local fits are affected. To find the total fit error over all segments for a collection of arbitrary planes  , we add a Lagrange term constraining the angles between pairs of fitting planes to equal the angles between corresponding planes in the model. The data that was used in the experimental results can be obtained at https: //sourceforge.net/p/jhu-axxb/ In the AX = XB case  , for each point  , we found its closest point on the model and computed the sum squared difference between them. We quantify the reconstruction by fitting the model to the new computed point set and finding a normalized metric. In order to perform accurate positioning  , Dudek and Mackenzie 2 composed sonar based maps where explicit model objects were constructed out of sonar reading distribution in space.  Curvature: In log-log space our data is curved as indicated by the fact that the best fitting distribution  , Zipf-Mandelbrot  , by theory has a curved form in loglog space. ZAZM: The particular model form with best BIC fit is the ZAZM Zero-Adjusted Zipf-Mandelbrot model for both datasets. Please note that the willingness  , capability  , and constraint functions are all parametric. Running experiments on a Dell 2900 server w/ 32GB of RAM  , most models can be fit to the largest of our graphs New York  , 3.6M edges within 48 hours. We generate 20 randomly seeded synthetic graphs from each model for each target graph  , and measure the differences between them using several popular graph metrics. In contrast to C++ or Smalltalk based OODBs  , its object model is a binary standard  , not a language API  , and is very strongly interface-based  , rather than class-based. By fitting two of the constants in the impact model which consist of various mass and geometric terms  , we obtained a usable model of impact which predicted average initial translation velocities to within 5 to 15 percent  , initial rotational velocities to within 30 percent. The translationall velocil.ies matched well  , but the measured rotational velocities were much larger than predicted. This can be done by computing B i X −1 p i where p i are the segmented model points in the first case  , and the segmented bead in the second case. If we assume a too complex model  , where each data point essentially has to be considered on its own  , we run the risk of over fitting the model so that all variables always look highly correlated. First  , we need a basic assumption of what the distributions will look like. Furthermore  , we evaluate the reliability of our models  , since AUC can be too optimistic if the model is overfit to the dataset. Assess models and reliability: After fitting our defect models   , we measure how well a model can discriminate between the potential response using the Area Under the receiver operating characteristic Curve AUC 17. Commonly made assumptions  , though reasonable in the context of workflow mining  , do clearly not hold for a dependency model of a distributed system  , nor do they seem fitting for a single user session. Second  , there is a difference in the model to be discovered. For robust verification with the fitting test  , we have to be sure that the hypotheses corresponding to surfaces with bigger area are tested before those corresponding to surfaces with smaller area. The technique works by augmenting the existing observational data with unobserved  , latent variables that can be used to incrementally improve the model estimate. EM addresses the problem of fitting a model to data in cases where the solution cannot be easily determined analytically. Thus we propose to solve this problem by an iterative method  , conceptually similar to the one described by Besl 5  , which combines data classification and model fitting. Conversely  , knowing the parameters of the model  , a search for compatible image points can be accomplished by pattern classification methods. Thus it cannot be said that this model would work for any soft tissue  , but rather  , soft tissues that exhibit similar characteristics to agar gel. This empirical model has been derived by fitting trends to experimental data conducted in agar gel as a tissue phantom. Hence non-uniform weights could easily incur over-fitting  , and relying on a particular model should be avoided. As shown in the following experiments  , the best model on current data may have bad performances on future data  , in other words  , P M  is changing and we could never estimate the true P M  and when and how it would change. Using the model  , we can then translate that probability into a statistically founded threshold of clicks and remove all " users " that exceed that threshold. Outlier removal using distributional methods proceeds by fitting a model to the observed distribution and then selecting a tail probability say 0.1% to use as a definition of an outlier. This result indicates that most queries are noisy and strongly influenced by external events that tend to interfere with model fitting. Comparing the temporal SSM models versus the baselines  , we observe that for the General class of queries the model that smooths surprises performs the best. Overall  , the models were trained with a combination of different parameter settings: 1 ,5  , 0 ,10 ,100 ,1000  , and with and without the indicator attributes. The reason for fitting the less restrictive " sliding-window " model is to test whether the " full " model captures the full extent of temporal change in weights. The robot control system has been synthesized in order to realize the identified expert impedance and to replicate the expert behavior. Note that our model is different from the copying models introduced by Simon 17  in that the choice of items in our model is determined by a combination of frequency and recency. Further  , fitting w using a power law with exponential cutoff as described above results in a model requiring only three parameters that provides explanations nearly identical in quality to the model produced by pointwise inference of w at all possible lo- cations. The " full " model is trained on all observed names across all 129 years whereas the " slidingwindow " models consist of a series of submodels each of which is trained using observations for a given year +/-a window of 4 years: 1880 1888  , 1889 1897  ,   , 2000 20088. Despite its complexity  , the LuGre dynamic friction model has been chosen in this activity to further improve the fitting between simulation and experimental results. In 13 the different behaviors shown by static and dynamic friction models Dahl model in the rendering of the friction phenomena acting on the tendon-based driving system have been evaluated  , and the better physical resemblance of the Dahl friction model has been reported. As the number of ratings given by most users is relatively small compared with the total number of items in a typical system  , data sparsity usually decreases prediction accuracy and may even lead to over-fitting problems. However  , MF approaches have also encountered a number of problems in real-world recommender systems  , such as data sparsity  , frequent model retraining and system scalability . One method of removing robots is to identify them with outliers and remove outliers. One might speculate whether embedding the IDEAL model in a less fitting strategy would have lead to the same positive results. Due to the characteristics of the organization  , in the case of NP  , the application of the humanistic change strategy seemed most adequate. The derivation is done by fitting 20 evenly spaced points  , each point being the number of total words versus the number of unique words seen in a collection. Heaps Law requires extra model parameters  , α and β  , that are derived from the input collection. Given a topic relevance score  , for each query  , the score of each retrieved document in the baseline is given by the above exponential function f rank with the parameter values obtained in the fitting procedure. The adjusted R-square  , on the other hand  , penalises R-square for the addition of regressors  , which do not contribute to the explanatory power of the model. Regularization via ℓ 2 norm  , on the other hand  , uses the sum of squares of parameters and thus can make a smooth regularization and effectively deal with over-fitting. Regularization via ℓ 1 norm uses the sum of absolute values of parameters and thus has the effect of causing many parameters to be zero and selecting a sparse model as solution 14  , 26. To further analyze the effect of covariates  , we compare the perplexity of all models in the repurchase data and the new purchase data in Table 2. related covariates in addition to fitting parameters of a conditional opportunity model for each category m. It shows the importance of considering covariates when modeling the purchase time of a follow-up purchase. If the general shape of the object is fit to some simple surface  , it should be possible to add the details of fine surface features using a simple data structure. We thus segment the color image with different resolutions see Section IV-A. Since we are dealing with sparse depth data  , it is further desirable to have as large segments as possible -otherwise model fitting becomes impracticable due to lack of data inside segments. As an example of what not to do  , we could take our relevant-document distribution to be a uniform distribution on the set of labeled relevant documents. Two questions must be answered to use this approach: i what family of distributions is used a modeling question  , and ii which distribution to choose from the family given the data a model-fitting question. The equation of each 3D line is computed by fitting a vertical line to the selected model points. The selected edges represent discontinuities in color and lie inside of a planar surface to avoid errors caused by edges at the boundary between two surfaces. To fit the three-way DEDICOM model  , one must solve the following minimization problem With a unique solution  , given appropriate data and adequately distinct factors the best fitting axis orientation is somewhat more likely to have explanatory meaning than one determined by  , e.g. , VARI- MAX 22 rotation. It may be possible that one or more chunks in that window have been outdated  , resulting in a less accurate classification model. Horizon fitting selects a horizon of training data from the stream that corresponds to a variablelength window of the most recent contiguous data chunks. 1 measurement of respondents' sensations  , feelings or impressions Dimension reduction techniques are one obvious solution to the problems caused by high dimensionality. The main reason for this is that the number of model parameters to be learned grows in accordance with the increase of dimensionality; thus  , the acquired functions might not perform well when sorting unseen objects due to over-fitting. For the Dynamic class  , temporal models that only take into account the trend or learn to decay historical data correctly perform the best. However  , a slight drop of performance can be observed for high θ values  , because it produces a large number of pattern clusters i.e. , increased model complexity  , which results in over fitting the data. This is because higher values of θ result in highly similar pattern clusters that represent specific semantic relations. Model Parameters. In our experiments  , after computing the metrics per user  , we averaged the results over all users and reported the results for Mean Average Precision@k MAP@k and Mean NDCG@k. We varied k from 1 to 10 as this is usually the size of a recommendation list fitting a device's screen.  Extensive experiments on real-world datasets convincingly demonstrate the accuracy of our models. We describe a fast method for fitting the parameters of these models  , and prescriptions for picking the right model given the dataset size and runtime execution constraints. Based on the rationale of curve-fitting models  , various alternatives to the DPM approach have been proposed and investigated 14  , 15  , 181  , but so far no superior model was reported. Applying this rule to the functions defining a 95% confidence band for the DPM-curve yields a 95% confidence interval for the total number of defects e.g. , 64 to 85 in figure 1. We thus avoid training and testing on the same dataset. To help mitigate the danger of over-fitting i.e. , of constructing a model that fits only because it is very complex in comparison to the amount of data  , we always evaluate on one benchmark at a time  , using the other benchmarks as training data. We can do model selection and combination—technical details are in Appendix C. This can be performed using only data gathered online and time complexity is independent of the stream size. We want to a avoid over-fitting and b present to the user those patterns that are important. Words best fitting this cumulative model of user interest are used as links in documents selected by the user. The weighted average of the user's last few link selections is passed to the search engine; results are then dynamically combined into a hypertext document. From this we can also expect that the image feature extraction error is within the range 5 to 15 pixels. This shows that the image-based techniques are more flexible to data fitting and local inaccuracies of the model than the geometric-based approaches  , which impose a rigid transformation . A reconstructed 3D model of the object is computed by fitting superquadrics to the data which provides us with the underlying shape and pose. Our proposed approach uses a low latency multi-scale voxelization strategy that is capable of accurately estimating the shape and pose parameters of relevant objects in a scene. We obtain results comparable to the state of the art and do so in significantly less time. Using the MATLAB profiler 5000 executions  , 1ms clock precision  , 2 GHz clock speed on standard Windows 7 OS without any code optimization  , our classifier executes in 1ms per AE hit on average. In addition to high accuracy and robustness  , the classifier demonstrates the potential for realtime implementation with offline model parameter fitting. At this time  , the side edge is joined slopes in stead of steps  , so zigzag is reduced obviously. The first line runs a paired t-test; in the second one the response variable y is explicitly written as a function of a fixed effect system and a random effect Errortopic. A model fitting the re-centered data then shows the effect of the varying IV on the DV with respect to the different levels of the re-centered IVs. We take the 25% and 75% quantile of the values of a variable as its low and high level  , respectively . However  , since this increases the dimensionality of the feature space—which makes it sparser—it also makes the classification problem harder and increases the risk of over-fitting the data. The objective of feature fusion is to combine multiple features at an early stage to construct a single model. In such a scenario  , it is not sufficient to have either one single model or several completely independent models for each placing setting that tend to suffer from over-fitting. While some attributes may be shared across different objects and placing areas  , there are some attributes that are specific to the particular setting. In this context  , we have modeled skills by adopting an explicitly different model fitting strategy that is based on the entropies obtained from multiple demonstrations. This learning method focuses on those portions in which a long-time is spent  , even though the movement slightly changes because those portions are of great importance in achieving the task. Because it is easier to express the metric error for the branch fitting than for the sub-branch finding  , 30 trials were first run on simulated branches with no sub-branches. Figure 3shows the endpoints of the rays superimposed on the ground truth model for one of the simulated models. This first segmentation may contain some errors  , e.g. , several superimposed leaves may fall in the same region  , and regions including few points may lead to a relatively large fitting error. The surface model provides the position and orientation of each leaf. The core idea of our method is based on the notion that surface boundaries are in most cases represented by an edge in the color image. This allows us to write the local error for segment k as: Solving the problem requires using knowledge about the system  , which enable one to handle the factors being omitted under conventional formal procedures. A modified scale space approach  , based on a line model mask with weights calculated from the line fitting mors  , is presented. For demonstration purposes here  , a method of smoothing only line segments within a laser scan  , while leaving alI other parts of the scan in tact can successfully meet our requirements to segment laser data and extract lines. Indeed  , the computational strategy adopted consists of a hierarchical model fitting  , which limits the range of labeling possibilities. In particular   , the experiments concerned the induction and performance evaluation of rules for the identification of the class of a document  , according to its logical components organized in a logical structure. One typical tree model has 10 layers and 16 terminal nodes. In the training stage  , the proper decreasing ratio is set to grow the tree; then the tree is pruned to achieve the best performance by avoiding over fitting with the training set. However  , when the attribute vectors that describe objects are in very high dimensional space  , these supervised ordering methods are degraded in prediction performance . By controlling for quality and position  , statistically significant positive estimates of wT and wA would imply that click behavior is biased towards more attractive titles and abstracts  , respectively   , beyond their correlation with relevance. Netflix Ratings: Within the Netflix dataset  , the results were not nearly as simple. This explains why our model has such an improved predictive probability than BPMF as shown above and demonstrates the importance of fitting the variance as well as the mean. In this sense  , the general reliability serves as a prior to reduce the over-fitting risk of estimating object-specific reliability in the MSS model. This suggests that a generally more reliable group is more likely to be reliable on a particular object. Recall that the mean of a set of points in R n is the point that minimizes the sum of squared residuals . We have experimented with hill climbing in our model fitting problem  , and confirmed that it produces suboptimal results because the similarity metric dK or others is not strictly convex. Hill climbing does not work well for nonconvex spaces  , however  , since it will terminate when it finds a local maxima. On the other hand  , we are a priori not interested in an entire flow of execution and such tricky issues as mutual exclusion or repetition. The resulting transliteration model is used subsequently for that specific language pair. At each re-training step  , a test set is used to compute the transliteration accuracy  , and the training is continued till the point when transliteration accuracy starts decreasing  , due to over-fitting. The more general the model  , the more effort it will expend on fitting to specific features of the training documents that will generalize to the full relevant population. However  , the improved performance is only guaranteed for the training data  , which is simply a sample from the underlying population of relevant documents which may not adequately characterize its true distribution. Specifically  , given a user's query  , SSL sends the query to the centralized sample database and retrieves the sample ranked list with relevance score of each document. Semi-Supervised Learning Merging SSL 27 uses curve fitting model to calculate comparable document scores from different sources for result merging. Therefore  , we propose to use a shared sparsity structure in our learning. The pixeUfeature error is about 5 pixels for the image-based techniques and about 15 pixels for point based techniques. Adding more constraints to the system reduces the size of this set and permits more precise or detailed knowledge about the world. The system was developed using the Silicon Graphics software package called " Open Inventor "   , which provides high level C++ class libraries to create  , display  , and manipulate 3-D models. This section describes the implementation of the model fitting system and informal evaluations performed with volunteer operators. Table 3gives the mean estimate of r   , over 40 degrees for 9 different indenters. In the following a general expression will be given  , and then will be described how to specialize it for the two cases. Quantitative results in terms of segment magnification obtained in the second view  , fitting errors  , and surfaces types are summarized in Table I. In the second view  , however  , the surfaces can be distinguished  , and  , using the segmentation procedure  , separated  , and fitted by a surface model. Normalization of certain AE sensor features such as amplitude and ASL was found to improve classification accuracy over non-normalized features  , primarily due to numerical precision when calculating feature weights β j . We use information entropy as the uncertainty measurement of the B-spline model. With our approach  , an object surface is divided into a set of cross section curves  , with closed B-spline curve used to reconstruct each of them by fitting to partial data points. It should be obvious  , without going through a complex matching procedure  , that the points on the adjacent flat sueaces cannot belong to the model  , which is curved at all points. For example  , consider the task of recognizing the U-shaped pipe fitting in the left scene of Figure2. Once we have mined all frequent itemsets or  , e.g. , closed itemsets  , we seek to select k itemsets whose segments cover the numerical data with as well-fitting models as possible. If the birds occur close together and in areas with similar rainfall  , this model is a good fit to the segment. A mathematical model was established and validated both deductively based on its geometric structure and inductively through empirical findings. Empirical modelling  , which focuses on the concepts of observation and data fitting from real experiments was used to characterize the behaviour of the PMA. As evident in Figure 5a  , the residual plot based on the confidential data reveals an obvious fanshaped pattern  , reflecting non-constant variance. We start by fitting the OLS model of income on main effects only for each variable  , using indicator variable coding for the categorical variables. It is fascinating that the typical ρ i for the individuals of seven of our eight datasets is approximately 1  , the same slope generated by the SFP model. In Figure 4we showed the slopes ρ of the OR fitting for the IEDs of all individuals of our datasets. To address this possibility of over-fitting  , we consider a second heterogeneous attrition model  , in which attrition probabilities Ri are randomly generated from the distribution of estimated attrition rates shown in Figure 1. Indeed  , the average estimated attrition for individuals in completed chains is 3% lower than the average estimated attrition for individuals in incomplete chains. There is large variability in the bids as well as in the potential for profit in the different auctions. 2 j we see a fairly wide range of variances across the beers. The play is divided into acts in such a way that each act has a fixed set of actors participating objects fitting conveniently on the scene scenario diagram. The basic idea is to model the event sequence as a play  , with objects as actors. This is our estimate for the runtime frequency of the path. The proportion of positive examples in the annotation hierarchy subtask was low  , and for that subtask we experimented with upweighting positive training examples relative to negative ones. Second  , it is reasonable to assume that the error in each variable is independent of the error in other variables. This is in contrast to the more widely adopted fitting approach of ordinary least squares where only one variable in the model is assumed to contain error. Many robotic manipulation tasks  , including grasping   , packing  , and part fitting require geometric information on objects. Furthermore  , in contrast to reported analytic techniques based on differential geometry 3 ,4  , 10 ,121  , our method does not require an edge correspondence problem to be solved or a smoothness assumption to be made about the object's surface  , and it produces an integrated  , consistent model from the data. -Any geometric model representation should be capable of generating the error vectors required. It is applicable to arbitrary shapes: -It does not require curve fitting or matrix inversion -It does not require a Jacobian or silhouette image to generate error/correction terms. We have simulated the same VSA-II model under exactly the same design and operative conditions: encoder quantization  , white noise on motor torques  , torque input profiles  , polynomials used for the fitting  , etc. The first one is the residual-based stiffness estimator in 14. In order to obtain actor and director information  , we downloaded relevant pages from the Internet Movie Database http://www.imdb.com. Since the subjects were instructed to favor accuracy over task time  , each trial was completed when the subject deemed that the closest fit hacl been attained. We would also have to consider 6DOF poses  , complicating the approach considerably. A method of voting for object centroids followed by a model fitting step was described in 20  , but we assume having no CAD models for test objects in this paper. One of the crucial problems is where to find the initial estimates seeds in an image since their selection has a major effect on the success or failure of the overall procedure. There is a certain advantage to the use of such an entropy-based skill learning method. Taking advantage of the theorem of separated axis lo  , real-time accurate and fast collision detection among moving geometrical models can be achieved. In the 3D graphics system  , a layered oriented tight-fitting bounding box tree has been established to approximate to each geometrical model of fingers and objects for grasping. Furthermore  , if the screw length were to be halved  , the maximum curvature would increase by more than a factor of two. To overcome this shortcoming  , we propose to use a multi-stage model. Due to this dynamics of degree distribution  , SLIM and ELIM which assume static degree distribution  , will not do well in fitting the observed diffusion data particularly in the later time steps. Formally this corresponds to minimizing the error when each tuple is modeled by the best itemset model from the solution set. Nonetheless  , the accuracy remains stable for a wide range of k 1 values  , indicating the insensitivity of the model with respect to the choice of k 1 values. This might be due to over-fitting the training data with more RBFs. Based on the intuitions above  , we propose to do one-way ANOVA sequentially on each feature and obtain the p-value pk for F k based on the fixed e↵ect model: More importantly  , for achieving interpretability and reducing the risks of over-fitting  , we also hope that output worker subgroups are not too many. In particular  , in Figure 7awe see that for MG-LRM  , the peak appears at a higher number of iterations than the other models. For most models  , the performance increases as the model learns good weights and then stabilizes at a slightly lower value  , which can be attributed to the opposing effects of over-fitting and of the stabilizing effect of the regularization coefficients.  We describe a fast method for fitting the parameters of these models  , and prescriptions for picking the right model given the dataset size and runtime execution constraints. Thus  , they can be immediately used for efficient ad selection from a very large corpus of ads. They are ultimately interested in learning the parameters controlling the model  , as well as the uncertainty associated with an incomplete raw dataset. " The modeler wants " all the data  , " but only for purposes of fitting and comparing models that help to explain the data. In other cases  , the LIWC categories were different enough from the dataset that model chose not to use topics with ill-fitting priors  , e.g. On the other hand  , some discovered topics do not have a clear relationship with the initial LIWC categories  , such as the abbreviations and acronyms in Discrepancy category. In fact  , although using small batch sizes allows the online models to update more frequently to respond to the fast-changing pattern of the fraudulent sellers   , large batch sizes often provide better model fitting than small batch sizes in online learning. It is interesting to observe that batch size equal to 1/2 day gives the best performance. Ribeiro also outlines a framework for fitting these parameters given a window of time series activity levels  , and then uses them to extrapolate and make a long term prediction of future activity levels. The model is specified by a set of parameters  , including the estimate of the susceptible population  , and the transition probabilities between different states. We also tried several other  , more complex models  , without achieving significantly better model fitting. As independent input variables  , we provided single-vote averages and covered range  , both appearing as first-order and second-order polynomials  , i.e. , SVA and CR  , and SVA 2 and CR 2   , respectively. This suggests that ad groups are very homogeneous   , and we would expect clicks from different terms in an ad group to have similar values to the advertiser. Moreover  , spline and polynomial curve fitting or energy minimization techniques such as active contours and snake 4 fail to give precise baselines and there is always an inclination towards descenders in the above methods. Given the fact that arbitrary baselines can take any form  , it is thus impossible to model them with polynomials or splines. Rank-GeoFM/G denotes our model without considering the geographical influence. The reason is that GeoMF addresses the data sparsity problem by fitting both nonzero and zero check-ins with different weights  , which is less reasonable than our ranking methodology because zero check-ins may be missing values and should not be fitted directly. An important characteristic of query logs is that the long tail does not match well the power law model  , because the tail is much longer than the one that corresponds to the power law fitting the head distribution. An example is given in Figure 1where α is 0.88 if we force f1 to be the actual value. Our selected procedure to predict future retweet activity is summarized in resolution Δ pred   , we proceed as follows: First  , we identify the infectious rate of a tweet pt by fitting the proposed oscillatory model. Finally  , the retweet activity in a bin A k is calculated from the estimated retweet rate , One problem with using R-square as a measure of goodness of fitting is that it never decreases in that it adds more regressors. For our sequence of models  , the cross-validated correlation and overall correlation are about the same  , giving us some assurance that the models are not over-fitting. The cross-validated correlation is the correlation between the model prediction and the leave-one-out predic- tions. In contrast   , we have specified in advance a single hypothesis h *   , i.e. , the interaction model motivated in Section 3  , and the values of ⃗ x is determined by specific types of user behavior. In a typical machine learning scenario h· would be selected from a pool of possible hypotheses by fitting example pairs of y and ⃗ x. We then fit model and frame nuisance parameters and found convergence over a wide range of initial values to B = 3.98  , nuisance angle = 36.93    , and nuisance distance = 1.11 mm. Applying the same fitting procedures described in Section VI-D to the torsion free case  , we first determined a tip error of 24.78 mm 54.32 mm maximum. The constants σ i of the final model are intended to be universal constants that should be applicable to a wider range of parameters not explicitly tested in our experiment. The constants K i in 6–9 were fitting parameters for the specific nondimensional data sets; they are implied functions of the dimensionless groups  , and would be different for other combinations of values. A classification technique is said to suffer from overjitting when it improves performance over the training documents but reduces performance when applied to new documents  , when compared to another method. The values of this section give the ratio of the standard error of each system/topic group to the standard error of the first system/topic group. When two robots are within the same " node " of the map  , they can localize with the same landmarks and operate in a common coordinate system. For example  , measurements made by the Polhemus sensor are transmitted as an electromagnetic signal  , and so can have errors introduced by metallic objects or stray magnetic fields existing in the vicinity of the sensor contain error. We start with a brief introduction to the 4-bar legs in Section 2 followed by a modeling discussion in Section 3 that introduces a polynomial representation of the empirical funct ,ion relating strain nieasurement to leg configuration . Hence  , the quasi-steady model we compare with only contains the translational term. While there are quasi-steady models based on 2D inviscid flow that address added mass and rotational circulation effects  , they usually involve extra fitting parameters and are not robust for large operating range. All of these computations are subject t o error. Thus  , t o compute a stick model of an object  , we first thin the range image of the object  , and then compute a stick description in a manner analogous t o that for fitting superquadrics. In the context of variable selection  , this implies that we may line up the variables in a sequence and include them into the model in a streamwise manner without over-fitting. Employing an α-investing rule allows us to test an infinite stream of hypotheses  , while at the same time control mFDR. It is important to note  , however  , that residuals only can reveal problematic models; a random pattern only indicates lack of evidence the model is mis-specified  , not proof that it is correctly specified. With bad fitting models  , it is often the case that multiple assumptions fail simultaneously  , and the plots exhibit non-random patterns. We seek to predict household income from age in years  , education 16 levels  , marital status 7 levels  , and sex 2 levels. In Figures 9-a and 9-b we compare  , respectively  , the histogram and the OR of the inter-event times generated by the SFP model  , all values rounded up  , with the inter-event times of the individual of Figure 1. This type of model is not new in the literature 41  , 10  but they have not been extensively studied   , perhaps due to the lack of empirical data fitting the implied distribution. In opposition to traditional methods aiming at fitting and sometimes forcing the content of the resources into a prefabricated model  , grounded theory aims at having the underlying model emerge " naturally " from the systematic collection  , rephrasing  , reorganisation and interpretations of the actual sentences and terms of the resources . Grounded theory 27 is a method often used in Social Science to extract relevant concepts from unstructured corpora of natural language resources e.g. , texts  , interviews  , or questionnaires. There is considerable variation within each run -the standard deviation is as much as 15 percent in initial rotational velocity and 5 percent in initial translational velocity. On the other hand  , the green curve quasi-steady model is symmetric with respect to its local maxima so the quasi-steady model does not distinguish between the stroke acceleration phase and the stroke deceleration phase. This distribution seem to follow a powerlaw distribution as we see in Figure 4and when we fit our general Figure 4: General Model: y-axis is the ratio of retweets  , and the x-axis is the number of minutes between a retweet and the original tweet. The final model is believed to be a plausible representation that will aid in further experimental studies  , physical modeling  , and numerical simulation to ultimately result in an improved model with a high degree of confidence for magnetic-screw path planning in soft tissue. Notice that our fit is even visually very good  , and it detects seasonalities and up-or down-trends: For example   , our model fitted the success of " Wii " which launched in 2006 and apparently drew attention from the competing " Xbox " . ECOWEB discovered the following important patterns:  Long-term fitting: Figure 1a shows the original volume of the four activities/keywords as circles  , and our fitted model as solid lines. Section 3 describes ways to obtain data on software changes and describes a method to estimate effort for a software change. The model without training is accurate for sufficiently large values of T   , but it cannot be applied for short observations because the quality of parameter fitting deteriorates  , as we showed in Sec. We also observe that training can improve the prediction performance for short observation windows T < 24 hours  , and that the model with training provides accurate predictions  , even for very short observation windows   , such as T = 1 hour. We now discuss how to address two practical challenges in employing our model as a prediction tool. In practice  , we can estimate {a  , b  , c  , d  , f  , PIQ} by fitting the data collected in a short initial monitoring period into Equation 1-3 the time window for data collection in all of our experiments is 20 weeks  , and input the fitted parameters into our model to forecast the number of active users in the future. The striking agreement between the fit model and the mean of each collection is achieved at the corresponding edge density by fitting only . The subgraph frequency of Gn ,p at the edge density corresponding to the data is shown as a black dashed line in each plot — with poor agreement — and gray dashed lines illustrate an incremental transition in   , starting from zero when it corresponds to Gn ,p and ending at opt . Previous work 20  , 57 showed that the use of different measures can impact both the fitting and the predictive performance of the models built by GA: relative measures e.g. , MMRE  , MEMRE often affect negatively the overall model accuracy  , while absolute measures e.g. , SAE seem to not have any detrimental effect. Many different indicators can be used to evaluate the accuracy of the estimates see Section 2. At close distances less than 10 cm  , the sonar sensors cannot be used for range measurement however  , with model fitting  , IR can provide precise distances  , enabling the robot to follow the wall and not having t o rely on error-prone dead-reckoning  11. Comparing figure 10with figure 7b shows that the accuracy is similar to our previous experiments where the exact robot distance t o the obstacle was measured. While the empirical data can be readily fitted to many known parsimonious models such as power laws  , log-normal  , or exponential  , there is no guarantee that the fitted model can be used to predict the tail of the distribution or how the distribution changes with the observation window . Since we predict cascade statistics  , our work also relates to research on fitting empirical data to parsimonious statistical models 1  , 5 . The part µ/e has to be higher than 0 to avoid ∆ k to converge to 0 and has to be divided by the Euler's number e to make the median of the generated IED around the target median µ more details in the Appendix A. We were successful in selecting similar developers: the ratio between the largest and smallest developer coefficients was 2.2  , which would mean that the least efficient developer would require 120% additional effort to make a change compared to the most efficient developer  , but Table 2: Results from model fitting. At the same time  , changes performed using VE were of the same difficulty requiring a statistically insignificant 7% increase in effort as changes with no #version lines at all &E versus @NONE. Comparing this with the errors in Table 1  , we see that in the best case this limit is nearly achieved while on average the error is twice the noise level indicating that model error does exist and it is on the same order of magnitude as the noise. The averaged tactile sensor data  , which is independent of the force data  , has a standard deviation of 0.4 % peak strain so we expect a fitting error of 0.9 % peak strain. In addition to the exploitation of the entire eigensystem of the segment fits and the expression of the model in a view-invariant form  , there are several other differences between our approach and that of Bolle and Cooper.2 We use general quadrics instead of restricting the form of the fitting functions to cylinders and spheres. We prefer to consider the problem in terms of sum square error  , but each view affords its own useful insight. Their additional restriction gives tighter fits to segments that are of fixed " optimal " size. We therefore evaluate the temporal correlation and the two derivative models by comparing 1 the quality of the summaries generated from these models and 2 their utility towards finding additional tweets from the tweet sample that are related to the event and yet do not contain the keywords from the original queries. A model that optimizes for the log-likelihood or perplexity score risks over-fitting the parameters to these noisy tweets. Importantly  , the evidence does show that document encoders are evaluating the advantages of the XML standard e.g. They considered that there were other ways of representing the same texts using different markup languages and that limitations in the Consortium's view needed to be evaluated: Fit for purpose as it emerges here is not about fitting a model or matching a markup language to the requirements of specific projects  , it is a general quality of fitness to the strategic objectives for documentation over time. The Bernoulli parameter pr ,u in our model  , however  , is specific to a rank r and a user u  , thus leaving more flexibility for setting different hypothesized values for simulation or fitting empirical parameters from log data. That is  , in 28  , a single persistence probability p is shared by results at all ranks; and the probability that a user examines the result at rank r is p r−1 . To be able to rank a document we needed to specify both the relevant and irrelevant probability distributions for a term  , so we need priors for both. As was discussed earlier  , in order use the model to generalize from labeled to unlabeled date e.g. , to assign relevance ranking values to unlabeled documents based on some relevance judgments we must incorporate a prior so as to avoid over-fitting the labeled data. Third  , using the position and orientation of the best leaf candidate  , the robot moves the camera system closer to it to obtain a more detailed view  , which is used to obtain a better model and eventually separate different leaves. Using the above mapping  , the remaining parameter of the amplifier model eq 4a  , internal resistance  , was determined by fitting estimated terminal voltage during an experiment to actual  , using the MATLAB" To calculate the estimated motor current  , the output of eq 3 was fit to the real motor current using actual terminal voltage. A value of 1.65 R was found  , as compared to the datasheet value of 1.33 Then  , the actual existence of the contour feature is verified by determining disparity between F  , and the content of CW. For a given contour feature F and a circular window image CW  , the following method is used to determine whether C W contains an instance of F: First  , a parameter fitting technique based on moments is applied to determine the most accurate model contour F. of F type hypothetically existing in CW. That is  , we assume individuals have attrition rates that are randomly drawn from this estimated population distribution  , and define the probability of observing a completed chain ω of length Lω to be To address this possibility of over-fitting  , we consider a second heterogeneous attrition model  , in which attrition probabilities Ri are randomly generated from the distribution of estimated attrition rates shown in Figure 1. the jackknife standard errors indicated that a difference of this size was not large enough to be distinguishable from random fluctuations i.e. It is evident that natural language texts are highly noisy and redundant as training data for statistical classification  , and that applying a complete mathematical model to such noisy and redundant data often results in over-fitting and wasteful computation in LLSF. To conclude the study in this paper  , noise and redundancy reduction is proposed and evaluated in the LLSF approach to documentto-categones mapping  , at the levels of words  , word combinations  , and word-category associations. Specifically   , even after being learned on a wealth of training data for a user  , the system could suffer from over-fitting and " cold-start " problem for new visitors the Web site. This is the primary reason why a straightforward approach to personalization  , that consists of learning the model for each user only from that user's past transactions  , fails for the personalization task with the Web data. In our case  , we use global topics and background topics to factor out common words. For 7 and 6  , they used a topic-variation matrix per region  , which might be too expensive to be applied over a large number of regions while the authors in those papers found that their model peaks at around 50 regions and 10 topics and the predictive performance deteriorates otherwise for excessive number of parameters   , resulting in over-fitting. Figure 1shows our discoveries related to the video game industry consisting of d = 4 activities  , namely  , the search volumes for " Xbox " x1  , " PS2  , PS3 " x2  , " Wii " x3  , and " Android " x4  , taken from Google  , 2 and spanning over a decade 2004-2014  , with weekly measurements. To estimate the desired distributions   , we assume that the correct distribution is one member of some specific family of distributions and  , based on the query-related information provided  , we attempt to choose a plausible distribution from that family. The concluding ' Section 5 is briefly concluding the method and presents its prospective applications including comments on feasibility of the hardware implementation. The steps consist of 1 express the change in the metric in terms of a function of the means and variance of a probability density function over the metric 2 mapping the estimates from the click-based model to judgments for the metric by fitting a distribution to data in the intersection 3 computing estimates for the remaining missing values using query and position based smoothing. We would like to evaluate a new ranking model by comparing with a baseline  , and looking at the difference in the chosen metric. Theoretical calculation shows that by reducing the diameter of the disks to 4 mm and adopting the same 150 pm SMA wires  , the bending angle is still in the range f 90 " and the maximum force exertable remains substantially unchanged About 1 N vs. the 4 N generated by the multi-wire configuration proposed by Grant and Hayward ~ 5 1  . Three participants spoke about the importance of commencing assessment of text encoding requirements at a higher level of abstraction than the TEI's model of a text as important. The intersection is the portion of the query-URL pairs that we have both editorial judgments and the user browsing model estimates . One explanation for these features not helping in our experiments may have been due to over-fitting the model on the relatively small data set. This was somewhat surprising  , since i one of our assumptions was that " fact " classifications were being triggered by stories having a higher than normal density of numbers and names versus " opinions " that might have higher than normal densities of adjectives and common nouns; ii at first glance  , fact based sentences seem shorter compared to opinion sentences  , but this does not make a difference in the classifier accuracy  , and does not carry over to article length either. Defining the I-space and a continuous mapping from I-space onto W-space. A mapping from capability space to resource space expresses the fidelity profiles of available applications. A mapping from capability space to utility space expresses the user's needs and preferences. As described by Heck- bert Hec86   , the traditional graphical texturing problem comprises mapping a defined texture from some convenient space called the texture-space   , to the screen-space. Texture generation and mapping has received considerable attention in graphics. Based on the mapping provided for Medium- Clone in section 2  , Space populates the mapping relations as follows: Example. Let R be the orientation mapping from the surface-space to the world-space The object's surface-space can thus be mapped to world-space. This is done by mapping the original joint space polytope in the intermediate space with matrix Jq. T ?iEW.flT J  , . For homogeneous robots  , it is the mapping From a global perspective  , in multi-robot coordination   , action selection is based on the mapping from the combined robot state space to the combined robot action space. We call this new space the reduced information space and the mapping from the information space onto it the aggregation map. Among the many possible ways of choosing a partition   , one solution is to choose a particular function mapping the information space onto a smaller tractable space. The radial distance between the camera and target  , as measured along the optical axis  , factors into this mapping. Tracking by camera pan requires mapping pixel positions in the image space to target bearing angles in the task space. This fixed mapping gives more flexibility to the k-mer feature space  , but only increases the size of the feature space by a constant factor of 2. Thus  , the fixed 3  , 1 wildcard mapping of abc is {abc  , a*c}. The tangential space mapping where V s 7 is tlie gradient function for 7. and Veep is tlie tangential space mapping of the kinematic function' . because it is com- Differentiating tlie where D denotes the differential operator. the arm is in constant contact with the obstacle . Mapping transforms the problem of hashing keys into a different problem  , in a different space. The overall Mapping- Ordering-Searching MOS scheme is illustrated in Figure   2. Mapping all users and items into a shared lowdimensional space. Stage 1. The directory space. , id-r for some mapping function G. yet to be defined. Reverse mapping is indicated by dotted arrows  , where the mapping of force flows in the opposite direction as velocity. The " directions " of these matrices show the forward mapping of velocity from one space to another. The mapping can include time variant contact conditions and also timely past and/or future steps during manipulation. The skill mapping SM gives the relation between the desired object trajectory This skill mapping SM maps from the 6-dimensional object position and orientation space to the 3n- dimensional contact point space. The texture properties are defined relative to an object's surface. Let R be the orientation mapping from the surface-space to the world-space The relationship between the topic space and the term space cannot be shown by a simple expression. The mapping is given by the matrix shown in equation 5. Of course  , this mapping concurs with inaccuracy. Similar patterns in the input space lie in a geographical near position in the output space. It admits infinite number of joint-space solutions for a given task-space trajectory. For a kinematically redundant system  , the mapping between task-space trajectory and the join-space trajectory is not unique. the Jacobian mapping from task space to sensor space  , is also a critical component of our visual servoing control strategy. A key component of this measure. J is the Jacobian matrix of linkage kinematics in leg space. These parameters are used to derive a mapping from each camera's image space to the occupancy map space. and is described by the following equations: v  , = v&+ B; denotes the stiffness mapping matrix relating the operational space to the fingertip space. In 2  , Koo and K ,  , denote the independent stiffness elements of the operational space and the fingertip space  , respectively. There is a continuous many-to-one mapping from I-space t o W-space determined by the forward kinematics of the arm. For any point in I-space  , there is a unique corresponding arm endpoint position in W-space. A singular value decomposition of this mapping provides the six-dimensional resolvabilify measure  , which can be interpreted as the system's ability to resolve task space positions and orientations on the sensor's image plane. The object centered Jacobian mapping from task space to sensor space is an essential component of the sensor placement measure . The key idea in mapping to a higher space is that  , in a sufficiently high dimension  , data from two categories can always be separated by a hyper-plane. The mapping is done through kernel functions that allow us to operate in the input feature-space while providing us the ability to compute inner products in the kernel space. The mapping  , termed the planar kinematic mapping in Bottema and Roth 1979  , is a special case of dual quaternion representation of object position in a three dimensional space. The Image Space is a three dimensional projective space with four homogeneous coordinates . For the defined model the phase space is 6-dimensional. So the mapping Eunction is 5-dimensional. It requires  , first  , mapping a world description into a configuration space  , i.e. , generating the configuration space obstacles Lozano-Perez 811. The configuration space approach  , for example  , is computationally very expensive. In the case of our mobile robot we chose four particular variables for the reduced information vector. This kernel trick makes the computation of dot product in feature space available without ever explicitly knowing the mapping. The Hilbert curve is a continuous fractal which maps each region of the space to an integer. We employ two well-known space-mapping techniques: the Hilbert space-filling curve 15 and iDistance 23. As a result  , collision checking is also performed directly in the work space. The robot links and obstacles are represented directly in the work space  , thus avoiding the complex mapping of obstacles onto the C-space. Although the mapping is diffeomorphic  , the transformed path to the joint space possibly does not coincide with the optimal path in the joint space. Suppose that one path is planned in z space by a certain optimization scheme. This slicing was developed in 6 for use in teleoperation of robot arm manipulators. To alleviate this problem  , we propose a second mapping which transforms the 3D C-space into a discontinuous 2D space of " sliced " C-space obstacles. Available resource levels are provided by the system  , and constrain the configuration space to a feasible region. The resulting dynamical model is described by fewer equations in the u-space. The redundancy allows one to obtain a low-order model for the manipulator dynamics by mapping the joint velocity q- space to a pseudovelocity U- space. First  , a conventional automobile is underactuated non-holonomic  , so the mapping from C-space to action space is under-determined . An action space approach is attractive for the purposes of cross-country navigation for several reasons. But unlike the mapping on a basis  , a mapping to a dictionary does not allow the reconstruction of the data element. Similar to the mapping on a basis the mapping on a dictionary takes as input a data space element and outputs a coordinate vector. Experiments in 1  , 5 show that the LegoDB mapping engine is very effective in practice and can lead to reductions of over 50% in the running times of queries as compared to previous mapping techniques. LegoDB is a cost-based XML storage mapping engine that automatically explores a space of possible XML-torelational mappings and selects the best mapping for a given application. We have proved that the forbidden region of an obstacle can be computed only by mapping the boundary of the obstacle using the derived mapping function. A mapping function has been derived for mapping the obstacles into their corresponding forbidden regions in the work space. Also  , the stiffness mapping matrix B; between the operational space and the fingertip space of each hand can be represented by where i  B ;   denotes the stiffness mapping matrix between the operational space and the fingertip space of the ith hand. In this case  , the stiffness matrix in the operational space can be expressed as where i  K f  and ZG ,f denote the stiffness matrix in the fingertip space of the ith hand and the Jacobian matrix relating the fingertip space of the ith hand to the operational space  , respectively. Due to space limitations  , we cannot present all mapping rules. Where needed an informal explanation of the mapping rule is given and finally a formal definition using first-order predicate logic is given. However  , non-holonomic vehicles have constrained paths of traversal and require a different histogram mapping. The polar histogram is a suitable mapping from grid space to the histogram bins for holonomic vehicles with unconstrained steering directions. We have performed the task that pouring water from a bottle with the power grasp  , which can test the joint space mapping method. Some tasks were performed to evaluate the mapping method. This yields a coefficient vector with as many coordinates as there are dictionary elements. As reasoned above  , HePToX's mapping expressions define the data exchange semantics of heterogeneous data transformation. For space reasons  , here we just informally explain the mapping semantics by examining the two DTDs in Figure 1. The results of the Mapping stage are sufficiently random so that more space-expensive approaches are unnecessary . By using and extending Pearson's method 15   , mapping tables containing only 128 characters are produced . Teleoperation experiments show that the human hand model is sufficient accuracy for teleoperation task. The joint space mapping and modified fingertip position mapping method are exercised in the manipulation of dexterous robot hand. Instead we provide a few examples to illustrate the mapping. Providing the mapping of the entire OWL syntax into the three types of rules considered in this paper is beyond the scope and space limitations of this paper. Given the search space ΩP  covering all possible mappings   , finding a C min mapping boils down to inferring subsumption relationship between a mapping and the source predicate  , and between two mappings. Section 5.2 will discuss this approach in details. The transformation of pDatalog rules into XSLT is done once after the mapping rules are set up  , and can be performed completely automatically. The mapping is straight-forward  , but space precludes us from explaining it in detail. As in the example in Section 2  , the user provides the mapping between application resources and role-based access control objects using a Space-provided embedded domain-specific language. User-provided Mapping. The baseline approach builds a non-clustered index on each selection dimension and the rank mapping approach builds a multi-dimensional index for each ranking fragment. We compare the total space usage with baseline BL and rank mapping RM approaches. Partition nets provide a fast way to learn the scnsorimotor mapping. The robot learns the mapping a.nd categorizations entirely within its seiisorimotor space  , thus avoiding the issue of how to ground a priori internal representations. The parameters of the human hand model are calibrated by the open-loop calibration method based a vision system. Partition nets provide a fast way to learn the sensorimotor mapping. The robot learns the mapping and catego-rizations entirely within its sensorimotor space  , thus avoiding the issue of how to ground a przorz internal representations. In this context a datatype theory T is a partial mapping from URIrefs to datatypes. A RDFSDL vocabulary V is a set of URIrefs a vocabulary composed of the following disjoint sets:  VC is the set of concept class names  VD is the set of datatype names  VRA is the set of object property names  VRD is the set of datatype property names  VI is the set of individual names As in RDF  , a datatype " d " is defined by two sets and one mapping: Ld lexical space  , Vd value space and L2Vd the mapping from the lexical space to the value space. That is  , the cross-modal semantically related data objects should have similar hash codes after mapping. Then we attempt to learn a bridging mapping matrix  , M  , to map the hash codes from mpdimensional hamming space to mq-dimensional hamming space or vice versa  , by utilizing the cross-modal semantic correlation as provided by training data objects. By using this representation  , the robot is shrunk to a point with its position being represented by its end effector and the obstacles are represented as forbidden regions in the work space. If we control the sparsity of projection matrix A  , we could significantly reduce the mapping computation cost and the memory size storing projection matrix. But a large number of latent intents would greatly increase the cost of mapping queries from book space to the latent intent space. The coordinate form representation of the latter is given by tlie n x n manipulator Jacobian matrix DecpO. These solutions realize a one-to-one mapping between the actuated joint velocity space and the operational velocity space. The Jacobian matrix mapping the joint and the operational vector spaces of the fully-isotropic T3R2-type parallel manipulators presented in this paper is the identity 5×5 matrix throughout the entire workspace. For example   , the forward mapping is unique in the case of the serial structured finger  , but in the case of the closedloop structured finger such as the finger with five-bar mechanism described in 8  , the backward mapping is unique. Note that the forward or backward Jacobian mapping between the joint space and the fingertip space may not be unique due to the structure of finger used in robot hands. In this paper  , we treat a robot hand with five-bar finger mechanism and then the stiffness relation between the fingertip space and joint space is described by using the backward Jacobian mapping. The lexical-to-value mapping is the obvious mapping from the documents to their class of equivalent OWL Full ontologies. To make this clear  , consider a datatype where the lexical space is the set of Turtle documents  , and the value space contains the equivalent classes of RDF graphs according to the OWL 2 RDF-based semantics entailment regime a.k.a OWL 2 Full. 1 We learn the mapping Θ by maximizing the likelihood of the observed times τi→j. We formalize this as τi→j ∼ f x; θ = Θai  , where Θ denotes a mapping from the space of actions A to the space of parameters of the probability density function f x; θ. Figure 1 shows the two essential mappings for skillful object manipulation. That is where it hurts in parallel kinematics  , especially when one considers only the actuator positions for sensing: the mapping is neither bijective several solutions to the forward kinematic problem nor differentiable singularities of any type. a differentiable bijective mapping between the sensor-space and the state-space of the system 16. Fullyisotropic PWs presented in this paper give a one-to-one mapping between the actuated joint velocity space and the operational velocity space. The Jacobian matrix mapping the joint and the operational vector spaces of the fully-isotropic PWs presented in this paper is the 3×3 identity matrix throughout the entire workspace. The hyper-plane is in a higher dimensional space called kernel space and is mapped from the feature space. toward the constraint region C are not allowed  , the effective space velocity is unidirectional along vector n. Knowing that the mapping between the effective space and the task velocity space is bijective  , any constraint on the effective space reflects directly into a constraint on the task velocity space. Since joint velocities incident to the constraint boundary aC i.e. A partial function I : S C mapping states to their information content is called an interpretation. Our theory distinguishes between an object state space S and an information content space C. The object state space consists of all the possible states that objects representing information might assume  , and the information space contains the information content representable in the object state space. The result is a task velocity toward the constraint region C are not allowed  , the effective space velocity is unidirectional along vector n. Knowing that the mapping between the effective space and the task velocity space is bijective  , any constraint on the effective space reflects directly into a constraint on the task velocity space. Mapping all the obstacles onto C-space is not computationally efficient for our particular problem; therefore  , collision detection is done in task space. In 19  , collision detection is done in C-space using the pre-determined C-space configuration although the random points are generated in task space. the set of positions and orientations that the robot tool can attain  , will be denoted by W = this section  , we show how the robot's task space can be mapped to the camera's visual feature space and then we will consider the mapping from the robot's configuration space to the visual feature space. The task space of the robot  , i.e. The control space is defined by the degrees of freedom of our haptic device  , the Phantom. However  , it is difficult to work in such a high-dimensional configuration space directly   , so we provide a mapping from a lower-dimensional control space to the configuration space  , and manipulate trajectories in the control space. The mapping from each image space to the map space is only dependent on the camera calibration parameters and the resolution of the map space. Each image space occupancy map is transformed to the map space by applying F equation 2. The 2n + 1 variables of.the access tree model form a 2n + 1 dimensional space R. The access model implies a mapping G: S ---> R from the space of file structures S ontu the space of all the combinations of model variable values  , R. This mapping is usually many-to-one because the variables only represent average characteristics of the file structures  , i.e. The details of these parameters are shown in Table 1. Weston et al 30 propose a joint word-image embedding model to find annotations for images. Then the model tries to learn a mapping from the image feature space to a joint space n R : A robotic system that has more than 6 dof degrees-of-freedom is termed as kinematically redundant system. Further  , addition and scalar multiplication cannot yield results similar to those performed in the data space. This implies that the mapping of a data element in the coordinate space of a dictionary does not allow reconstruction. Intuitively  , a tight connection between two documents should induce similar outputs in the new space. Let the mapping function Φ contain m elementary functions  , and each of them φ : X → R map documents into a onedimensional space. average pointer proportion and average size of filial sets of a level. But this mapping is not one-to-one  , there are infinite number of possible joint-space solutions for the same task-space trajectory. This is one of the most common techniques used for kinematically redundant systems. The tracking of features will be described in Section 3.1. If the axes are aligned as shown in the figure  , the Jacobian mapping from task space to sensor space for a single feature can be written as f Figure 1 . Figure 2shows the resolvability of two different stereo camera configurations. If the axes are aligned as shown in the figure  , the Jacobian mapping from task space to sensor space can be written as Figure 4shows the coordinate frame definitions for this type of camera-lens configuration. Since the mapping from I-space t o W-space is continuous  , and since a sphere is an orientable surface  , so is the cylinder surface. I Figurestead  , it is the surface of a cylinder Figure 5 . An alternative method of dealing with sparsity is by mapping the sparse high-dimensional feature space to a dense low-dimensional space. We describe it in more details next. Instead of mapping documents into a low-dimensional space  , documents are mapped into a high dimensional space  , but one that is well suited to the human visual system. Word clouds and their ilk take an alternative approach. Finally  , Space verifies that each data exposure allowed by the application code is also allowed by the catalog. Second  , Space uses the mapping defined by the user to specialize each exposure's constraints to the objects constrained by the catalog. To achieve the goal of partially automated configuration  , the model separates concerns into three spaces: user utility  , application capability  , and computing resources; and two mappings. 4 showed that the lexical features of the query space and the Web document space are different  , and investigated the mapping between query words and the words in visited search results in order to perform query expansion. Cui et al. In other words  , with longer lifespan  , the partitions at the upper corner of the space rendition contain more tuples  , hence more pages. Graphically  , their mapping points in the space rendition move up wards. The Hough transform 5 was developed as an aid to pattern recognition and is widely used today. Thus the Hough transform provides a one-to-one mapping of lines in the original space to points in the transform space. Ordering paves the way for searching in that new space  , so that locations can be identified in the hash table. In SMART the Jacobian is used for a wide variety of variable mappings. In robotics it typically refers to the velocity mapping between a robot's joint space and its world space motions. Many classical visualization techniques are based on dimensionality reduction  , i.e. , mapping high-dimensional data into a low dimensional space. The first is to visualize high-dimensional data in a high-dimensional space. To explain this mapping from intention space to relevancy space  , let us assume we have a resource R which has been tweeted by some author at time ttweet. Figure 2a This difference becomes larger in the region which is far from the origin. The unique mapping is highly related to the concept of observability. This transformed state space is equivalent to the state space consisting of the deflection angles θ and ψ i with its timederivatives . Figure 2: Mapping between sensor space and mental space based on empirical rules and physical intuition. Subconscious knowledge or techniques often play an important role in human task performance. Therefore  , it is represented by a mapping of the shape space Q into the force-distribution space T*Q. A compliance can be regarded as a conservative force field. Using the learned sensorimotor mapping and body ima.ge  , the robot chooses an action in the sensorimotor space to circumnavigate obstacles and reach goals. sensorimotor space that extends beyond the cmiera's view based on collisions. First  , for an input hyper-plane  , all the cluster boundaries intersect the hyper-plane are selected. More formally  , the forward mapping from the input space to the output space can be accomplished as follows. The proposed method uses a nullspace vector in the velocity mapping between the q-space and the u-space to guarantee the continuity in the joint velocities. Finally  , in Section 6 we describe several simulation experiments. This representation greatly simplifies collision checking and the search for a path. From this perspective  , visual tools can help to better understand and manipulate the mapping into the program space. In general  , programmers use a language to map their ideas into a program space. In fact  , the theoretical condition for the validity of a sensor-based control is that there exists a diffeomorphism i.e. A different approach is to derive a reduced-order dynamical manipulator model 6. A typical trial comprised the mapping of several hundred square metres of trials space  , followed by two or more days testing a wide variety of runs through this space. The sorting office had many impermanent sonar features. Let  , the joint velocity polytope of a n-dof manipulator be described by the 2n bounding inequalities: This is done by mapping the original joint space polytope in the intermediate space with matrix Jq. Tracking in this manner is known as piloting 3 or steering 4. Note that this definition implicitly assumes to be able to generate negative values for the joint variables. These ellipsoids are the mapping froin unitary balls in t ,he velocity/force joint space to the analogous in the task space. If the axes are aligned as shown in the figure  , the Jacobian mapping from task space to sensor space for a single feature can be written as Figure 3shows the coordinate frame definitions for this type of camera-lens configuration . In this paper  , we consider a compliance and damping as impedance elements. On the other hand  , a damping is a mapping of the shape-velocity space TQ into its dual space T*Q. However  , there is a large gap between the problem space and the solution space. Establishing a mapping between domain model and the architecture is the objective of domain engineering 16. To compare the operations allowed by an application to those permitted by our security patterns  , a mapping is required between the objects defined in the RBAC model and the resources defined by the application. Based on the mapping provided for Medium- Clone in section 2  , Space populates the mapping relations as follows: Space asks the user to define this mapping. In many cases  , this mapping is obvious a resource named " User " in the application   , for example  , almost always represents RBAC users  , but in general it is not possible to infer the mapping directly. Formally  , it is a mapping from types of application resources to types of RBAC objects; the mapping is a relation  , since some application resources may represent more than one type of RBAC object. The robot learns a sensorimotor mapping and affordance categorizations or proto-symbols and uses the mapping for primitive navigation to exploit affordances. The robot learns a sensorimotor mapping and affordance categorizations and projects the mapping into the future to exploit affordances . The results of the experiment are summarized in Figure 4. Given the correct user-provided mapping  , the patterns applied by Space were always at least as restrictive We transformed the strings to an integer space by mapping them to their frequency vectors. to transform one string to the other. Extensive fault tests show that mapping reliable memory into the database address space does not significantly hurt reliability. This exposure can be reduced by write protecting buffer pages. These embeddings often capture and/or preserve linguistic properties of words. Word-embeddings are a mapping from words to a vector space. plastic  , metal or glass  , to friction cone angles that define the grasp wrench space. On a basic level  , this is often approached by mapping discrete material properties  , e.g. The XSLT stylesheets are created based on the pDatalog rules. In this section  , we formally define the extension of the database . However  , due to space limitation  , we describe the intension to extension mapping only. So uncertainty can be represented as a sphere in a six dimensional space. Thus the mapping from one we consider the characteristically same configuration of a manipulator. The -mapping model confirms that this gap does exist in the 4-D space. The gap between cluster A and B can be visually perceived. Triplify automatically generates all the resources in the update URI space  , when the mapping µ in the Triplify configuration contains the URL pattern " update " . Invocation. However  , space precludes an explanation here. There are additional details that concern how to preserve the data structure which holds the mapping of disk pages to buffer pages. Another dynamically consistent nullspace mapping  , which fits very well in the framework of operational space control  , was proposed by Khatih 61: by the manipulator's mass matrix. The language model described in 2 falls in this category. This mapping has two main advantages. We then apply the space-filling curve to this future position to obtain the second component of Equation 1. Clearly  , this constraint reduces the size of our search space. Thus  , when we come to mapping the root location  , we only consider configurations meeting the constraint. However  , the efficiency of exhaustion is still intolerable when SqH is large. The introduction of Query-Topic Mapping reduces the search space significantly in Opti-QTM. This mapping can be extended naturally to expressions. The repair space is thus E ∪ S. We recall that a program state σ maps variables to values. Therefore  , we only describe a number of representative examples  , though others can be described in a similar way. Traditional information retrieval systems have focused on mapping a well-articulated query onto an existing information space 4  , 43. Integrating Queries and Browsing. This places reliable memory under complete database control  , eliminates double buffering  , and simplifies recovery. Mapping reliable memory into the database address space allows a persistent database buffer cache. In the EROC architecture this mapping function is captured by the abstraction mapper. Logical expressions are mapped by an optimizer search engine to a space of physical expressions. We also show this in the demo. First artificial space-variant sensors are described in 22. Such a peripherally graded pattern was first expressed as a conformal exponential mapping in 21. This dictionary element is therefore represented twice. After this approach  , C hyperplanes are obtained in the feature space. is a mapping function and b i is a scalar. However  , the lack of this optimization step as of now does not impact the soundness of the approach. This helps to prune the space for conducting containment mapping. When we increase the mean lifespan of tuples  , more tuples have longer lifespan. The exact mapping of topics and posts to vectors depends on the vector space in which we are operating. Vector construction. Tracking of articulated finger motion in 3D space is a highdimensional problem. The corresponding mapping from classified hand postures to Barrett configurations is selected offline in advance. We can understand them as rules providing mapping from input sensor space to motor control. For the sake of clarity  , the parameters listed are also discretized. The mapping of the Expressivity to more than one sub-parameter consequently constrains the space of all possible configurations. ble as to be seen in Figure 3 . The space of word clouds is itself high-dimensional  , and indeed  , might have greater dimension than the original space. Our use of the stress function is slightly unusual  , because instead of projecting the documents onto a low-dimensional space  , such as R 2   , we are mapping documents to the space of word clouds. So  , in a rr@rm space  , in which slope is plotted along one axis and intercept along the other  , every point uniquely determines and is uniquely determined by a line in the regular space. Absolute space comes from the idea that the representation for each space should be independent of all other spaces. I Absolute Space Representation: An Absolute Space Representation or ASR 7   , is a cognitive mapping technique used to build models of rooms or spaces visited. Since an adversary can no longer simulate a one-to-n item mapping by a one-to-one item mapping  , in general  , we can fully utilize the search space of a one-to-n item mapping to increase the cost of attack and prevent the adversary to easily guess the correct mapping. Note that the number of possible transformed transactions is 2 |B S F | which is much larger than the number of possible original transactions 2 |I| . Because it is difficult to build a feature space directly  , instead kernel functions are used to implicitly define the feature space. This mapping is defined as φ : X → F   , where X is the original space  , and F is the feature space. Because the synibol space is continuous space and the dynainics in this space is continuous system  , the continuous change of the vector field in the inotioIi space and the continuous motion transition is realized. By the mapping function F  , the reduced motion zk is extracted t o the joint angles of the robot 9k. U refers to map the query text q from the m-dimensional text space to the kdimensional latent space by a liner mapping  , and V refers to map the retrieved image d from the n-dimensional image space to the k-dimensional latent space. where U ∈ R k×m and V ∈ R k×n . Lewis Lew89 surveys methods based on noise  , while Perlin Per851 Per891 presents noisebased techniques which by-pass texture space. Since the animation and the trajectory are equivalent  , we may alter the trajectory and derive a new animation from the altered trajectory. For example  , the integral and differential equations which map A-space to C-space in a flat 2D world are given below: During the transient portion the steering mechanism is moving to its commanded position at a constant rate. The mapping from A-space to C-space is the well-known Fresnel Integrals which are also the equations of dead reckoning in navigation. To find the stiffness relation between the joint space and the fingertip space  , it is first needed to consider the structure of finger in the hand. The wirtual obstacle is a continuum of points in I-space corresponding t o those arm positions in W-space at which the arm intersects some obstacles. When the hand system grasps the peg for the compliance center 0 1 of Figure 4   , this is identical to combine the two cases of Figures 2If the compliance center is moved to the point 0 2   , the sign of the kinematic influence coefficient y1 in 6 changes into negative  , and the sign of the kinematic influence coefficient y2 in 11 changes into negative . While a tight as possible mapping uses the reach space of the robot hand optimally   , it may nevertheless occur that  , since the human finger's workspace can only be determined approximately   , some grasps may lead to finger tip positions which lie outside reach space of the artificial hand. When considering the mapping of the reach spaces of the human and robot hands we are faced with the following problem. For a more complete description of this mapping from activation level space to force space  , see 25. Extreme points in the space of applied forces are created by limits in activation levels some tendons will be at their maximum force and some will be inactive. Then the two robots exchange roles in order to explore a chain of free-space areas which forms a stripe; a series of stripes are connected together to form a trapezoid. One robot moves and sweeps the line of visual contact across the free space  , thus mapping a single region of free space. LSH is a framework for mapping vectors into Hamming space  , so that the distances in the Hamming hash space reflect those in the input space: similar vectors map to similar hashes. Among the common methods to achieve this is Locality Sensitive Hashing LSH 1. The one-class classification problem is formulated to find a hyperplane that separates a desired fraction of the training patterns from the origin of the feature space F. This hyperplane cannot be always found in the original feature space  , thus a mapping function Φ : F − → F   , from F to a kernel space F   , is used. in 21. In vector-space retrieval  , a document is represented as a vector in t-dimensional space  , where t is the number of terms in the lexicon being used. Documents are retrieved by mapping q into the row document space of the term-document matrix  , A: Therefore  , it can be computed off-line and used as a look-up table  , forming the following pseudo-code: The mapping from each image space to the map space is only dependent on the camera calibration parameters and the resolution of the map space. It is not possible  , in general  , to compute the speed and steering commands which will cause a vehicle to follow an arbitrary C-space curve. The interface allows direct mapping between the interaction space to a 3D physical task space  , such as air space in the case of unmanned aerial vehicles UAVs  , or buildings in the case of urban search and rescue USAR or Explosive Ordnance Disposal EOD robotic tasks. The 3D Tractus was designed with 3D spatial tangible user interfaces TUIs themes in mind. Denote the joint space of an n-joint  , serialdifferentiability of g is necessary because the joint accelerations are bounded  , and therefore the joint velocities must be continuous . In its most abstract form  , the forward kinematics of a serial-link manipulator can be regarded as a mapping from joint space to operational space. The space overhead problem is crucial for Semantic Search  , which involves the: use of a space consuming indexing relation: A weighted mapping between indexing terms and document references. We study the two complcmcntary access methods through a common approach designed to improve time access and space overhead  , the Signature techniques Crh84. The construction of the configuration space  , the control space  , the mapping between them and the haptic forces makes it possible to author and edit animations by manipulating trajectories in the control space. We have provided several techniques for editing existing trajectories  , and as this is done the user can see the effect on the animation in real time. For example  , we can present a current situation and retrieve the next feasible situation through interpolation. With the FSTM partitioned effectively as an union of hyper-ellipsoids  , we can obtain the mapping from an input space of a dimensions to an output space of f3 dimensions in the N-dimensional augmented space  , a+f31N. If our thesis is correct  , physical TUIs such as the 3D Tractus can help reduce the ratio of users per robots in such tasks  , and offer intuitive mapping between the robotic group 3D task space and the user's interaction space. Examples may range from mining tasks  , space exploration  , UAVs or Unmanned Undersea Vehicles UUV. ORDBMSs that execute UDFs outside the server address space could employ careful mapping of address space regions to obtain the same effect. Also  , calls to SAPI functions from the AM extension execute as regular C function calls within the server address space  , so there is no need to " ship " the currently active page to the AM extension; copy overhead is therefore avoided. However  , subsequent research publications report 1 ,13 that a direct mapping from source to target TUs without an intermediate phonetic representation often leads to better results. Pt|s as a series of conversions from the grapheme space spelling of the source language to the phoneme space pronunciation  , and then to the grapheme space of the target language. The manipulator knows some mappings from the problem space to the solution space and estimates the mapping for the goal problem by using them. The solution space is a set of manipulator trajectories or a label representing there is no solution for the problem. We will discuss the haptics in Section 2.3  , but first we give the mathematical model. During learning  , the simple classifier is trained over dataset T producing a hypothesis h mapping points from input space X to the new output space Y . This is necessary during the search over the space of subsets of clusters  , and while estimating final predictive accuracy. FigureObject a has a different geometrical feature than object b  , yet under many grasping configurations  , the relation between the body attached coordinate system of the gripper and the object is the same. Furthermore  , this mapping is naturally a many to many mapping that can be reduced to a many to one mapping in obstacle free environments  , thus reducing the learning space and resulting in a much better generalization. In this figure  , the transformations are defined as: 2 functionfis also relating between gripper and object configurations  , then the relationship between an object geometry  , task requirements and gripper constraints can now be mapped to a generic relation between two coordinate systems. In future it is likely that as we move to a push model of information provision we should provide the means to have local variants of ontologies mapping into our AKT computer science 'standard reference' ontology. In this version of CS AKTive Space we have not included this ontology mapping capability since we have been responsible for engineering the mapping of the heterogeneous information content. The mapping provided by the user translates between the RBAC objects constrained by the pattern catalog and the resource types defined in the application code. Space requires the mapping above and MediumClone's source code—it needs no further input or guidance from the user. Space does not permit entire rules templates are shown or the inclusion of the entire mapping rule set  , but this is not needed to show how the homomorphism constrains the rules. In order to illustrate the interaction between metamodels   , a homomorphism  , and a set of mapping rules  , we examine portions of two rules from the formalization of UML with Promela. If space-filling curves are used  , the mapping is distance-preserving  , i. e. similar values of the original data are mapped on similar index data  , and that for all dimensions. By mapping multi-dimensional data to one-dimensional values  , a one-dimensional indexing method can be applied. The PSOM concept SI can be seen as the generalization of the SOM with the following three main extensions: the index space S in the Kohonen map is generalized to a continuous mapping manifold S E Etm. Unfortunately  , in general the planes do not match at the borders of the Voronoi-cells  , which may leave discontinuities in the overall mapping. Also  , we performed some teleoperation tasks to test modified fingertip position mapping method such as: grasping a litter cube block only with index finger and thumb; grasping a bulb and a table tennis ball with four fingers. Figure 2shows the structure of the global address scheme and an example mapping. To build a global catalogue of a user's personal information space  , each file needs to have a unique and non-ambiguous mapping between a global namespace and its actual location. The basic approach in 9 is to treat the problem as a search for desired functions in a large search space s. In actuality  , preparatory Mapping and Ordering steps are needed so that fast Searching can take place. The extraction of the latent features of users  , tags  , and items and mapping them into a common space requires a special decomposition model that allows a one-to-one mapping of dimension across each mode. Hence  , the recommender system can explain to u3 that " T oy Story " is recommended because he/she likes comedy and " T oy Story " is a comedy. Here  , we adopt the PARAFAC model 4 to carry out further tensor decomposition on the approximate core tensorˆStensorˆ tensorˆS to obtain a set of projection matricesˆPmatricesˆ matricesˆP The extraction of the latent features of users  , tags  , and items and mapping them into a common space requires a special decomposition model that allows a one-to-one mapping of dimension across each mode. The best among the derived configurations is selected using cost estimates obtained by a standard relational optimizer. We represent the design space synthesis function  , c  , as a semantic mapping predicate in our relational logic  , taking expressions in the abstract modeling language to corresponding concrete design spaces. Relation c can be seen as mapping abstract  , intensional models of design spaces to extensional representations   , namely sets of concrete design variants. Example 2.2 select culture painting title : t  , Figure 5: Path-to-path Mappings pings save space by factorizing DTD similarities and allow semi-automatic mapping generation. cultureepaintinggtitle is mapped to WorkOfArtttitle because their leaf nodes are equal and there is a mapping between the context of title cultureepainting and a sub-path of WorkOfArtttitle. This inference is specific to data types– For some types  , it is straightforward  , while others  , it is not. The solutions we obtain through mapping are not optimal; however  , due to the good locality properties of the space mapping techniques  , information loss is low  , as we demonstrate experimentally in Section 6. Recall that both optimal k-anonymity and -diversity are NP-hard 14  , 13  in the multi-dimensional case. It is desired to ensure the mapping functions Φx to be consistent with respect to the structure of G| T V  , E. In the following  , we measure the information loss of each k-anonymous or -diverse group using N CP   , and the information loss over the entire partitioning using GCP see Section 2. For navigation  , the mapping is served as the classifier for the distribution of features in sensor space and the corresponding control commands. The learned lookuptable is the reactive 191 sensorcontrol mapping that explicitly stores the relations between different local environmental features and the corresponding demonstrated control commands. In this method  , the optimal trajectories in the state space are grouped using the data obtained from cell mapping. A cell mapping based method has been developed to systematically generate the rules of a near-optimal fuzzy controller for autonomous car parking. The information bases under the other mappings show the same general trend. Although we ran comparisons under all three mappings  , due to space constraints  , we show only measurements taken under the M-NC mapping  , because M-NC was the superior mapping in Section 5.2. Space uses this mapping to specialize the constraints derived from the checks present in the code to the set of RBAC objects  , so that the two sets of security checks can be compared. If the handles were clustered  , the strength of Btrees and direct mapping was exhibited. If the handles were clustered randomly  , direct mapping performed a little better than both hashing and the B+-tree because it used significantly less disk space about 30 ,000 pages. When a robot link moves around an obstacle  , the link-obstacle contact conditions vary between vertex-edge and edge-vertex contacts . In this paper  , we investigate the collision-free path planning problem for a robot with two aims cooperating in the robot's work space. However  , despite the importance of vision as a localization sensor  , there has been limited work on creating such a mapping for a vision sensor. Having a mapping of sensor performance across the configuration space has been argued to be beneficial and important. Particular mapping functions have to be defined  , which makes the problem more complex but in turn only meaningful configurations might be created. Experimental results on a Pentium 4 with an average load of 0.15 have shown an average query time of 0.03 seconds for the mapping and 0.35 seconds for the ranking when mapping to 300 terms. These are compared to Ouδ for the vector space method. The user can interact in the 3D domain by physically sliding the 3D Tractus surface up and down in space. Within the RDS we can treat elements of X as if they were vectorial and  , depending on the approximative quality of the mapping  , we can expect the results to be similar to those performed if they were defined in the original space. The RDS R – a quotient space given by the equivalence class of coefficient vectors resulting in the same dictionary element over the vector space R n – and the RDIP ·  , ·· R form a vector space with inner product. Queries belonging to this URL pattern have to return at least two columns. Figure 4 shows that the first two latent dimensions cluster the outlets in interpretable ways. We start by looking at the mapping of the labeled outlets  , as listed in Table 3  , in the space spanned by the latent dimensions. We emphasize that these features cannot be calculated before the result page is formed  , thus do not participate in the ranking model. Namely  , let W be the function mapping the space of Yfeatures to the weights: To our knowledge  , this is the first work that measures how often data is corrupted by database crashes. Our main conclusion is that mapping reliable memory into the database address space does not significantly decrease reliability. Our second software design Section 5.2 addresses this problem by mapping the Rio file cache into the database address space. Second  , databases can manage memory more optimally than a file system can  , because databases know more about their access patterns. This exposes reliable memory to database crashes  , and we quantify the increased risk posed by this design. This is consistent with the estimates given in Sullivan9la  , Sullivan93J. Our main conclusion is that mapping reliable memory directly into the database address space has only a small effect on the overall reliability of the system. Then any multi-dimensional indexing method can be used to organize  , cluster and efficiently search the resulting points. The idea is to extract n numerical features from the objects of int ,erest  , mapping them into points in n-dimensional space. First  , we generated a dictionary that has a mapping between terms and their integer ids. In this section  , we describe how we transformed the candidate documents in each sub-collection into its representation in the Vector Space Model VSM. Documents are retrieved by mapping q into the row document space of the term-document matrix  , A: Like the documents  , queries are represented as tdimensional vectors  , and the same weighting is applied to them. We address these two issues by mapping the answer and question to a shared latent space and measure their similarity there. Therefore  , surface level similarity measures such as Cosine or Jaccard will fail to identify relevant propositions. TermWatch maps domain terms onto a 2D space using a domain mapping methodology described in SanJuan & Ibekwe-SanJuan 2006. For the second period 2006-2008  , 1938 records were obtained. In this paper we introduce one way of tackling this problem. Mapping navigable space is important for mobile robots and can also he a product in its own right  , e.g. , in the case of reconnaissance . IJsing this mapping reactive obstacle avoidance can be achieved. This effectively maps the low-dimensional force vector F from the workspace into the high-dimensional joint space of the manipulator. This could be done by mapping the object parameters into the feature space and thus writing them as a geometric constraint. In the case that a model of the environment is given  , one might also wish to incorporate obstacle constraints . We also plan to apply this method to general C-space mapping for convex polyhedra. We hope to extend this method in the future to work with non-convex polyhedra. Due to space limitation  , the detailed results are ignored. The results are beyond our expectations: the learned lexical mapping did not help for all the three ranking methods CS  , QL and KL. Finally  , an implementation of concurrent control as a mapping of constraints between individual controllers is demonstrated. Fourth  , a general framework for concurrent control borrowing from priority-based null-space control of redundant manipulators is described. Nevertheless it's possible that with different kernels one could improve on our results. It appears that the data does form a consistent mapping in high dimensional space  , and therefore we were able to get good results. This paper explores the utility of MVERT for exploration and observing multiple dynamic targets. These approaches build maps of an unknown space by selecting longterm goal points for each robot Other approaches focus more mapping I81 19. In semi-autonomous navigation  , omnidirectional translational motion is used for mapping desired user velocities to the configuration space. The robot is driven by selecting commands on the ASPICE GUIs; a mouse is used as input device. We then calculate the mean of its column-wise Pearson correlation coefficients with Y . The slice held out is then mapped to the 3-D latent space with mapping matrix and appended to the learned embeddings of the other slices. We c m directly transfer the calibrated joints value measured by the CyberGlove@ to the robot hand. After h e calibration and knowing accurate joint angles of human hand fingers  , the joint space mapping is easy to fulfill. If the automated system could function well in this space  , then it will also function well in the retirement community. The automated behavioral mapping surveillance system was setup to replicate the installation area  , as well as the ambient lighting conditions. These include scaling  , rotation  , and synchronization of observations from several tours of a space. Beck and Wood 2 include several common operations involved in map-making in their model of urban mapping. The time series are further standardized to have mean zero and standard deviation one. The space V now consists of all time series extracted from shapes with the above mapping . Let¨be Let¨Let¨be a feature mapping and be the centroid matrix of¨´µ of¨´µ  , where the input data matrix is represented as in the feature mappingörmappingör the feature space explicitly. At this time  , it might be effective to subtract the explained component in the target ordering from sample orders. After that  , by mapping attribute vectors to the new sub-space  , components in attributes related to this vector are subtracted. An intermediate future work would be to incorporate the XQuery logical optimization technique in 9  in our normalization step to reduce the possible navigation redundancies in the VarTree representation. For discrete QoS dimensions  , for instance audio fidelity   , whose values are high  , medium and low  , we simply use a discrete mapping table to the utility space. latency by flipping the order of the good and bad values . Since the target predicate has a pre-defined domain of values  , each representing a range  , our search space is restricted to disjunctions of those ranges. Consider mapping between the price predicates in Example 1. triples that represent specific points in the geometric space. Mappings model both the descriptive characteristics of an object  ,  Relationships among objects are modeled by " domainobject   , mapping-object  , range-object. Thus  , mapping reliable memory directly into the database address space does not significantly lower reliability. These uncommitted buffers are vulnerable to the same degree in all three systems Section 5.2. But it does not become a subject of this paper so far as an n-a imensional space. We use this mapping to parameterize the grasp controller described in Section 3. The opposition space is important to this discussion because it links specific contact regions on the hand surface with the role they play in the grasp. The particular minimum of 3 in which the robot finds itself is dependent on the path traversed through through joint space to reach current joint angles. Thus the forward kinematics  , given the actuator states  , is not necessarily a unique mapping. For example  , a typical mapping approach  , called approximate cell decomposition 7  , maps an environment into cells of predefined shapes. There is usually a trade-off between low cost in time and space and high map fidelity and path quality. Second  , the inverse model  , the mapping from a desired state to the next action is not straightforward. First  , since soil is not rigid  , a C-space representation of natural terrain has very high dimensionality. The above results represent the first approach to a perception mapping system; it involves all sensors and all space around the robot. A crucial issue is naturally the sensor overlapping configuration. The global exploration st ,rategy provides the order in which these areas are explored. The local exploration strategy guides the path traveled for the mapping of a convex area of free space a triangle  , or a trapezoid. Section 2 extends Elfes' 2-D probabilistic mapping scheme to 3-D space and describes a framework for workspace modeling using probabilistic octrees. Finally  , simulation results and performance considerations are presented for the power line maintenance application. -procedures for mapping sensory errors into positional/rotational errors e.g. -providing the a-priori knowledge on the C-space configuration and the type of shared control active compliance or using nominal sensory pat- terns. This property can be viewed as the contraction of the phase space around the limit cycle. The mapping F is stable if the first return map of a perturbed state is closer to the fixed point. This is because we excluded the coupling terms iKfxyi=1 ,2 ,3 in the fingertip space for independent finger control. Note that the elements of the second row of the mapping matrix are calculated as zero. The sensory-motor elements are distributed and can be reused for building other sequences of actions. This will build a mapping of the sensory-motor space to reach this goal. In particularly  , by allowing random collisions and applying hash mapping to the latent factors i.e. We address this problem by implementing feature hashing 27 on the space of matrix elements. we can both reduce the search space and avoid many erroneous mappings between homonyms in different parts of speech. We assume that by mapping only nouns to nouns  , verbs to verbs  , etc. Imitation of hand trajectories of a skilled agent could be done through a mapping of the proprioceptive and external data. The collected data could be used for generating unexplored movement and for reaching unexplored positions in the action space. A mapping is defined by specifying an implementation component in the requires section of an abstract package definition. Abstract components from the problem space are distinguished from implementation components by having an empty location field in their package definition. The kernel function implicitly maps data into a highdimensional reproducing kernel Hilbert space RKHS 7  and computes their dot product there without actually mapping the data. is a kernel function  , and C > 0 is the cost parameter . Clearly  , this plot does not reveal structures or patterns embedded in the data because data dojects spread across the visual space. The right view of Figure 5 shows the result of a random mapping of host names. two different paths in the interpretation space can lead to the same program. If the mapping from problem descriptions to programs is to be rich enough to generate a sufficiently wide variety of programs  , ambiguity is an unavoidable consequence  , i.e. An architectural style specification  , omitted due to space limitation  , defines the co-domain of an architectural map. 10 } Listing 2: The elided mapping predicate for the SCC application type and REST architectural style Section 2 presents object-relational mapping ORM as a concrete driving problem. This paper provides one solution to this problem  , particularly for design space models expressible within a relational logic 20 . Space  , in contrast  , requires only that the programmer provide a simple object mapping. Boci´cBoci´c and Bultan 3 and Near and Jackson 24 check Rails code  , but require the user to write a specification. Later  , we generalized this idea to map the strings to their local frequencies for different resolutions by using a wavelet transform. 7  , 8  presented techniques for representing text documents and their associated term frequencies in relational tables  , as well as for mapping boolean and vector-space queries into standard SQL queries. Grossman et al. The acquired parameter values can then be used to predict probability of future co-occurrences. Figure 1: Mapping entities in folksonmies to conceptual space rameters by maximizing log-likelihood on the existing data set. Indeed  , mapping technology itself—including the prior technology of the printed map— privileges a particular cognitive perspective 9. Geographers and historians emphasize that a map advocates a way of thinking about space  , rather than transmitting the single correct representation. We address this problem by implementing feature hashing 28 on the space of matrix elements. We built an earlier Java-based prototype in order to rapidly explore the design space for visual mapping of organizations. Both the faces and the displayed information are obtained from a centralized corporate directory. Our choice of visual design builds upon one of the simplest hierarchical layouts  , the icicle plot 1. The classifier was trained to be conservative in handling the Non-Relevant categorization. After examining the relevancy of the datasets using our developed relevancy classifier  , we now use our TIRM mapping scheme in transforming the results into the intention space. Second  , suboptimal mappings have a larger impact in the two-dimensional space than in the unidimensional one. Thus  , mapping an entity to a suboptimal random coordinate affects the spatial deviation of more blocks in DBPedia than in BTC09. The access interface need only maintain a relatively simple mapping between object identifiers and storage locations. b Large holdings can be moved to wherever space is available  , without having to rewrite the corresponding catalog database. The attribute for each sample point object occupanjcy or free space was determined by the solid interference function "SOLINTERF" in AME. The sample points for RCE mapping were randomly selected in the CAD environment. Higher map resolution and better path usually mean more cells thus more space and longer planning time. This design offers more protection than the first two designs  , but manipulating protections may slow perfor- mance. Keeping an I/O interface to reliable memory requires the fewest modifications to an existing database but wastes memory capacity and bandwidth with double buffering. maximum heap space  , and the numbers of MultiExprs and ExprXlasses in the logical and physical expression spaces at the end of optimization. The columns in the tables show enumeration  , mapping  , and total optimization times  , estimated execution co&! This narrows down the search space of potential objects on the image significantly. Based on the mapping  , the FMA is used to retrieve a list of anatomical entities that could possibly be detected in this body region. Second  , consider the mapping of textual words into the latent space in LSCMR. But we find something interesting that though some topics overlap  , some smaller but more precise topics are discovered see the two " Biology " topics in Table 5. The mapping of feasible initial-state perturbations around a nominal initial state x 0 to sensor-observation perturbations is given by the observability matrix Let the columns of the matrix N span the null-space of B. We apply a. liyclrodynamic potential field in the sensorimotor spa.ce to choose an action cf. For an environment depicted in Fig. This histogram was established from a mapping from a 3D space to 2D ZXplane using the depth inforniation to represent the obstacles in the environment. The fuzzy rules and membership functions are then generated using the statistical properties of the individual trajectory groups. Figure 11shows another mapping. In a computer implementation  , if the available storage space is scarce  , it is straightforward to devise other mappings from hexagonal to quadractic not necessarily rectangular grids that do not leave empty cells. In computer graphics  , for cxample  , an object model is defined with respect to a world coordinate system. Many problems in computer vision and graphics require mapping points in space to corresponding points in an image. Fundamentally  , thc dccomposition in 12 rcprcscnts a. mapping from the space of infinitc-dimcnsiona.1 rcalvalucd functions to thc finitc-dimcnsiona.1 spa.cc  ?P. Thus we would wa.nt to decompose  ,BTs into 8 cocfficients , Employing this demonstration technique saves from the burden of mapping the human kinematics as in other approaches 7  , 14. Moreover  , kinaesthetic teaching intrinsically solves the correspondence problem  , as the robot learns in its own joints space. A phase space represents the predicted sensory effects of chains of actions. Projection heuristics provide an efficient method of projecting a learned sensorimotor mapping into the future to exploit affordances. We will develop a polygonal line method to avoid the poor solutions by fitting the line segments without any mapping or length constraints. This is due to their fixed topology on the latent data space or to bad initialization 8. Additionally  , potential clusters are maximally S-connected  , i.e. We represent these more compactly by mapping regions from the original space to descriptor nodes that record the object count for these regions. In the aforementioned methods it is assumed that the dataset is embedded into a higher-dimensional space by some smooth mapping. This number of components can be viewed as the number of effective dimensions in the data. Measure the relativity between the semantics of a tag t k and the chosen dimension according to the The intent of any input query is identified through mapping the query into the Wikipedia representation space  , spanned by Wikipedia articles and categories. 14 leveraged Wikipedia for the intent classification task. According to the objective function 6  , we think that the optimal r-dimensional embedding X *   , which preserves the user-item preference information  , could be got by solving the following problem: Mapping all users and items into a shared lowdimensional space. During the final phase of resolution i.e. , relation mapping  , the remaining relationships between concepts are mapped into the viewpoint model space. If types conflict  , HyDRA assists in the conflict's resolution. These relations may include temporal relations  , meronymic relations  , causal relations  , and producer/consumer relations. In practice  , we can often encode the same probability distribution much more concisely. The size of a probabilistic mapping may be quite large  , since it essentially enumerates a probability distribution by listing every combination of events in the probability space. The mapping from the system state to the Java code we implemented is straightforward. Space limitations do not allow us to concentrate on the implementation  , which is thoroughly described in 19. In this section  , we discuss our development of predicate mapper  , which realizes the type-based search-driven mapping machinery. Due to space limitation   , please refer to 12 for more details. Both problems are NP-hard in the multidimensional space. In this paper  , we developed a framework for solving the k-anonymity and -diversity problems  , by mapping the multidimensional quasi-identifiers to one dimension. The relationship between database intension and extension then is an injective mapping between two topological spaces. That is  , the extension of a database can be seen as a topological space built out of entities rather than entity types. The state of the art in multimedia indexing is based on feature extraction 30  , 161. In the following  , lower-case bold Roman letters denote column vectors  , and upper-case ones denote matrices. We aim to derive a mapping Ψ : X → V that projects the input features into a K-dimensional latent space. The use of these techniques for document space representation has not been reported In the literature. Therefore  , transformation methods must be considered which are more efficient than the mapping techniques In the generation of the data point  ,. ,... ,.uon. This solution is one of five Pareto-optimal solutions in the design space for our customer-order object model. Figure 6presents a graphical depiction of an Alloy object encoding a synthesized OR mapping solution. The second component of the visual mapping is brightness . In particular  , the brightness of a statement  , s  , is computed by the following equation: 5In color space models  , a pigment with zero brightness appears as black. Indeed  , there is no theoretical basis for mapping documents into a Euclidean space at all. While this framework  , like many others  , has no theoretical basis  , it is an intuitive extension of a vector based approach. Apart from the limited number of discontinuities  , the mapping from pose-space to eigenspace is conformal: that is  , continuous but curved. In the experiments described below we used a fix sample grid of Ax=Ay = 50cm and A0 = 0.5 degrees. The tip of the bucket position and its orientation relative to the horizontal are the task space variables being controlled. Cylin-der extensions are determined from the joint angles using a polynomial mapping  Selective usage of these elements may be more suited for specific situations of navigation. The output is well-defined  , closed under the operation  , and is unique. Taking this function as weighting for the individual behaviours from the input space  , a mapping is defmed between the input and output spaces. These are highly desirable properties for an unsupervised feature mapping which facilitate learning with very few instances. Similar poses of the same object remain close in the feature-space  , expressing a low-dimensional manifold. The camera-totarget distance remains constant when the target horizontally translates in a plane parallel to the camera's image plane and simple perspective is used for the image-to-task space mapping. Tracking by camera translation is much simplier. uncertainty in the kinematics mapping which is dynamic dependent. The required joint trajectory cannot be generated by the given trajectory in inertia space due t o the dynamic parametel. Most approaches increase efficiency by dividing large multi-robot problems into several smaller single-robot tasks. Since the PCM contains only obstacles in a fixed vicinity of the vehicle  , obstacles "enter" and "leave" the map gradually as the robot moves. Based on this mapping each cell of the grid is marked either "obstacle" or "free-space". We have shown an efficient and robust method for recomputing 3-d Minkowski sums of convex polyhedra under rotation. Bottema and Roth 1979 introduce this mapping directly and study the image curves which represent the coupler motion of a planar four bar linkage. They went on to characterize the geometry of their projective image space. Mapping motion data is a common problem in applying motion capture data to a real robot or to a virtual character . Our accuracy requirements are much less because the mari0nette.k gesturing in free space rather than precisely positioning an object. Mapping with only stationary objects  , and localization using entire observations in which the dual sensor model of occupancy grids is applied for range readings from moving objects. OGSD Occupancy grids presuming free space is crossable. They obtain an affordance map mapping locations at which activities take place from learned data encoding human activity probabilities. An example of work on shared space of humans and robots is given by Tipaldi and Arras 15. What follows is a sequence of strings that define the traversal path through the output space of the selected extractor. The mapping expression starts by specifiying the " extractor key "   , a unique identifier of the extractor to be used. The second data set contains 2 ,000 data items in 3- dimensional space with 2 clusters the middle one in Fig.3. can compare the resultant mapping with the original data set directly. To calculate the document score for document d i   , the vector space method applies the following equation: We will now show how LSA is as an extension to the VSM  , by using this query mapping. We also consider transforming the NED mapping scores into normalized confidence values. For assessing the confidence  , we devise several techniques  , based on perturbing the mention-entity space of the NED method. The other primitives are less crucial with respect to the YQL implementation  , and therefore we skip their discussions due to space limitations. A short discussion of the mapping of each Remote Query Interaction primitive follows. Since the adversary only has information about the large itemsets  , he can only find the mappings for items that appear in the background knowledge. So  , the adversary can reduce the search space for each mapping of item. However  , mapping an inherently high-dimension data set into a low-dimension space tends to lose the information that distinguishes the data items. To address the " dimensionality curse " problem  , the index subsystem must use as few dimensions as possible . The SOM defines a mapping from the input data space onto a usually two-dimensional array of nodes. The vector size of the subject feature vector was 1 ,674 and the vector size of the description feature vector was 1 ,871. This is because wild stores rarely touch dirty  , committed pages written by previous transactions. This provides the means to study alternative physical representations and to analyse the consequences of changes made in the conceptual schema. We also test a number of other standard similarity measures  , including the Vector Space Similarity VSS 3 and others. We employ a mapping function f x = x+1/2 to bound the range of PCC similarities into 0  , 1. When decoding the relative strength of active signals in a complex 3d world with different densities of matter – i.e. When stock is reorganized  , the system must reconfigure its mapping of library space onto the subject headings. The mapping  can not be achieved by the system without breaking contact constraints. If the number of columns of the blocks C11 and Caa equals the dimension of the task space  , the cooperating system is " minimal " . For the purposes of synthesizing a compliance mapping   , it is assumed that the robotic manipulator and the gripper holding the object can move freely in space without colliding with the environment. The above equation does not include joint friction. In this paper  , we investigate a novel approach to detect sentence level content reuse by mapping sentence to a signature space. Thus  , it is essential that content reuse detection methods should be efficient and scalable. the terms or concepts in question. We choose a setup of P such that it provides a mapping into the space of all possible superconcepts of the input instances  , i.e. The stress term of the objective function is inspired by multidimensional scaling MDS  , a classical method for dimensionality reduction 2. Consider a naive indexing approach where a sentence-file stores keyword vectors for the sentences in the collection. In particular  , we propose a sentencesignature based mechanism for mapping from the sentence domain to a multi-dimensional space such that word-overlap searches can be re-posed as range searches in this space. Hashing then involves mapping from keys into the new space  , and using the results of Searching to find the proper hash table location. According to the preceding calculations  , both procedures will yield exactly the same ranking. Instead of mapping both queries and documents to the kdimensional concept space via U T k and computing the cosine similarity there  , we may therefore as well transform the documents via the m × m matrix U k U T k   , and compute cosine similarities in the original term space. Therefore  , the knowledge of inverse kinematics mapping is of great interest since it allows the path planing to be independent of the geometry of the robot. the inverse kinematics maps the world coordinate space onto the joint coordinate space  X E R " -+ q ~ R ~   l    ,  1 3  . Currently  , a 7:l position amplification permits comfortable mapping of RALF's full workspace into the workspace of the human operator. To facilitate the teleoperation tasks  , the controller for KURBIRT computes its tip position and scales the position from the space of the master robot to the space of the slave  , RALF. The control law is provided by mapping these two spaces as an open-loop schema. The sensor and the manipulation spaces are partitioned by considering the features of the images and the space of the DOF of the manipulator that is called the configuration space. Errors in the estimated and actual generalized force were used to drive the system to minimize the external loads projected into the configuration space. The method employs a mapping of the unknown interaction forces into a generalized force in the configuration space of a continuum segment. As discussed in t ,he Introductioii  , well known concepts for manipulability mea.sures of robotic structure are the so-called velocity and force maiiipulability el- lipsoids  , 12. The geometric configuration of robot manipulability includes two wellknown types: manipulability ellipsoidl  and manipulability polytope2  , 3 ,4. The concept of robot manipulability means that constraints on joint space are transformed to that of task space through the mapping zk = J q   , or in general the transformation P = A&. Hence  , in order to obtain more specific latent query intents  , we often need to obtain rather a large number of latent query intents. Most tasks  , for example welding  , insertions  , and grasping   , require a higher precision than can be achieved by using artificial forces. If the axes are aligned as shown in the figure  , the Jacobian mapping from task space to sensor space for a single feature can be written as The fuzzy logic is used to select the elements of the transformation matrix 1T which indirectly determine the contribution of each joint to the total motion. Figure 7shows the trajectory taken by the wheelchair green when the user attempts to follow a leader blue. Trajectories and maps were produced via Hector mapping 17; map regions are as follows: light grey represents known vacant space  , black represents known surfaces and dark grey represents unknown space; the grid cells are 1 metre square. Practically  , the document space is randomly sampled such that a finite number of samples   , which are called training data R ⊆ R  , are employed to build the model. Let us suppose there is a classifier such as h  , which is defined as h : R → C  , where h is a many-to-one mapping of the documents to the binary class space. Bound the marginal distributions in latent space In the previous section  , we have discussed how the marginal distribution difference can be bounded in the space W . The following theorem concludes that we can further bound the marginal distributions of two domains by the mapping T . Thus  , we develop a mechanism for efficient wordoverlap based reuse 33  by mapping sentence domain context to a multi-dimensional signature space and leveraging range searches in this space. In this paper  , our focus is not on developing better reuse metrics  , but on the efficient identification of reuse in large collections. Index schemes: There have been a number of proposals for finding near-duplicate documents in the database and web-search communities 21  , 37  , 10. To an abstract model  , m ∈ Design abst   , we apply a design space synthesis concretization function  , c  , to compute cm ⊂ Designconc  , the space of concrete design variants from which we want to choose a design to achieve desirable tradeoffs. The inputs of the system are assembly quality ternis  , i.e. , the elements of assenibly quality space U1  , while the outputs are the assembly operation strategies ant1 quality control strategies  , i.e. , the elements of assembly cx~ntrol strategy space U ,. The NFEPN niodel is also used to implement and optimize the mapping f 1 3 . In their original formulation  , these manipulability measures or ellipsoids considered only single-chain manipulators  , and were based on the mapping in task space trough the Jacobian matrix of the joint space unit ,a.ry balls qTq 5 1 and T ~ T 5 1. A kinematic mapping f has a singularity at q when the rank of its Jacobian matrix Jf q drops below its maximum possible value  , which is the smaller of the dimensions k of the joint-space and n of the configuration space. The exponential commutes with its defining twist and its derivative is therefore: In computational biology  , it has been found that k-mers alone are not expressive enough to give optimal classification performance on genomic data. But what happens if the grasping configuration doesn't follow any of the simple built-in action models ? By dividing the mapping space into simple mappings  , more complex mappings could be learned over the whole object configuration space with a minimum number of experiments. In order to discuss and motivate the inverse kinematic function approach  , we must first describe the forward kinematics of a manipulator. A unique mapping will need additional constraints  , such as in the form of desired hand or foot position. In order to kinematically transform an RMP back to a humanoid robot  , one needs to generate a map from the 11– dimensional RMP space to the much larger robot kinematics space. Thus  , each fuzzy-behavior is similar to a conventional fuzzy logic controller in that it performs an inference mapping from some input space to some output space. Each behavior is encoded as a fuzzy rule-base with a distinct mobile robot control policy governed by fuzzy inference. Resolvability provides a shared ontology  , that is a scheme allowing us to understand the relationships among various visual sensor configurations used for visual control. A key component of this measure  , the Jacobian mapping from task space to sensor space  , is also a critical component of our visual servoing control strategy. is the Jacobian matrix and is a function of the extrinsic and intrinsic parameters of the visual sensor as well as the number of features tracked and their locations on the image plane. If the axes are aligned as shown in the figure  , the Jacobian mapping from task space to sensor space can be written as This set is called The above theorem states that points in the workspace close to obstacles  , relate to points in the configuration space with even less clearance. The mapping between workspace and configuration space is straightforward: A point p in the workspace corresponds to the set of configurations in C which have p as their position. News articles are also projected onto the Wikipedia topic space in the same way. Then  , the final mapping Φl of a location l into the Wikipedia topic space is the multiplication of the product vector and the local topic distribution. The motion strategy can be represented as a function mapping the information space onto the control space. motion commands corresponding to current knowledge of the system  , whose execution gives the robot the maximum probability of reaching a goal configuration from any initial configuration. In contrast to this direction of research  , relatively little research e.g. ,2 ,4 has involved the inverse kinematics -the direct mapping from the workspace to the joint space -for kinematically redundant manipulators. This resolved motion technique first determines the joint velocity using the pseudoinverse matrix  , and then incrementally determines the joint displacement; it thus transforms from workspace to joint space via joint velocity. These mapping methods are not widely used because they are not as efficient as the VSM. If the mappings to the topic space are performed correctly we are able to retrieve document at a higher precision than the vector space method. This fact is especially interesting if the data space is non-vectorial. The derivation of t from a induces a mapping  , cl  , from concrete designs to concrete loads parameterized by a choice of abstract load. As long as cm preserves a representation of a in its output  , then from any single design space model  , m  , we can synthesize a concrete design space  , and both abstract and concretized loads. Space is otherwise completely automatic: it analyzes the target application's source code and returns a list of bugs. Space Security Pattern Checker finds security bugs in Ruby on Rails 2 web applications  , and requires only that the user provide a mapping from application-defined resource types to the object types of the standard role-based model of access control RBAC 30  , 15 . A load/store using out of bounds values will immediately result in a hardware trap and we can safely abort the program . Note that we can reuse the high address space for different pools and so we have a gigabyte of address space on 32 bit linux systems for each pool for mapping the OOB objects. In the above definition  , it is equivalent to compute the traditional skyline  , having transformed all points in the new data space where point q is the origin and the absolute distances to q are used as mapping functions. Each point p = p 1   , p 2  in the original 2-dimensional space is transformed to a point Given a source logical expression space  , a target physical expression space  , and a goal an instance of Goal  , a Mapper instance will return a physical expression that meets whatever constraint is specified by the goal. The condition number and the determinant of the Jacobian matrix being equal to one  , the manipulator performs very well with regard to force and motion transmission. As opposed t o mapping < to new active joint space velocities through a given shape matrix Jcp   , this approach introduces additional joint space velocities using a new shape matrix . A more involved approach to redundant actuation is the introduction of entirely new actuators to the mechanism. Basically  , defuzzification is a mapping from a space of fuzzy control action defined over an universe of discourse into a space of non-fuzzy control actions. Since we use the height defuzzification method  , we can specify a rule directly by assigning a real number instead of a linguistic value to pj which is to be optimized by EP. Although inany strategies can be used for performing the defuzzifi- cation 8  , we use the height defuzzification method given by where CF is a scale factor. As discussed in 21  , the measure is easily extendable to other visual sensors including multi-baseline stereo and laser rangefinders. The set of all possible twists at a given position and orientation of a rigid body is the tangent space at that point; it is represented by the tangent space at the origin of a chosen reference frame. Such a path is  , mathematically speaking  , a mapping from the real line  " time "  into the manifold. Among the collision-free paths that connect the initial and goal configurations  , some may be preferable because they will make more information available to the robot  , hence improving the knowledge of its current state. If the axes are aligned as shown in the figure  , the Jacobian mapping from task space to sensor space for a single feature can be written as Figure 2F shows the coordinate frame definitions for this type of camera-lens configuration. Before planning the vision-based motion  , the set of image features must be chosen. We also can define image features as a mapping from C. This means that a robot trajectory in configuration space will yield a trajectory in the image feature space. Fingerprint-based descriptors  , due to the hashing approach that they use  , lead to imprecise representations  , whereas the other three schemes are precise in the sense that there is a one-to-one mapping between fragments and dimensions of the descriptor space. The third dimension is associated with whether or not the fragments are being precisely represented in the descriptor space. Dimension reduction is the task of mapping points originally in high dimensional space to a lower dimensional sub-space  , while limiting the amount of lost information. Therefore  , the text query and the retrieved image are mapped to a common k-dimensional latent aspect space  , and then their similarity is measured by a dot product of the two vectors in the kdimensional space  , which is commonly used to measure the matching between textual vectors 1. Instead  , the map is created with consideration to where the ASRs are with respect to each other and the robot. Similar to a  we project these unreachable positions back to the closest reachable position in the workspace. In this section  , the results of numerical simulation of the Stiffness mapping between 2-dof cylindrical space and 2-dof joint space using both direct and indirect CCT are presented. Kc  , =  0 The initial values of joint stiffness matrix and joint torque in Figure 6are The former problem may be solved by the use of perfect hash functions  , such as those proposed in 1 ,2 ,3 ,5 ,6 ,7 ,9 ,10 ,26 ,28 ,301  , where a perfect hash function is defined as a oneto-one mapping frcxn the key set into the address space. Secondly  , the address space cannot easily be changed dynamically. As a result of COSA  , they resolve a synonym problem and introduce more general concepts in the vector space to easily identify related topics 10. The authors apply an ontology during the construction of a vector space representation by mapping terms in documents to ontology concepts and then aggregating concepts based on the concept hierarchy  , which is called concept selection and aggregation COSA. , where each column of Wp and Wq generates one bit of hash code for the p th and q th modal. The indexing relation is of the kind defined in IOTA Ker84In this chapter we present  , first  , the query language structure. If X and Y are input and output universes of discourse of a behavior with a rule-base of size n  , the usual fuzzy if-then rule takes the following form Thus  , each fuzzy-behavior is similar to a conventional fuzzy logic controller in that it performs an inference mapping from some input space to some output space. where a k are comers of the n-dimensional unit activation hypercube  , or the set of all combinations of minimally and maximally activated muscles. Dehzzification is a mapping from a space of fuzzy control actions defined over an output universe of discourse into a space of nonfuzzy control actions. Ail and A12 are the membership function in the antecedent part  , B  , is the membership function in the consequent part. A specific form of the ho­ mography is derived and decomposed to interpolate a unique path. A homography is a mapping from 2-D projective space to 2-D projective space  , which is used here to define the 2-D displacement transformation between two ob­ ject poses in the image. Daumé and Brill 5 extracted suggestions based on document clusters that have common top-ranked documents. Examples are presented to demonstrate the computational and the corresponding regional transformation: The resolvability ellip- soid 5 illustrates the directional nature of resolvability  , and can be used to direct camera motion and adjust camera intrinsic parameters in real-time so that the servoing accuracy of the visual servoing system improves with camera-lens motion.   , it is very tlifficidt to implement and optimize the mapping f l : l iising the mathematical or numeric approaches. In other words  , it is sufficient Remarkably  , in this case the optimization problem corresponds to finding the flattest function in the feature space  , not in the input space. Hence  , the key idea to overcome the problem of dimerisionality is the use of kernel functions for establishing an implicit mapping between the input and the feature spaces. Consequently several projections or maps of the hyperbolic space were developed  , four are especially well examined: i the Minkowski  , ii the upperhalf plane  , iii the Klein-Beltrami  , and iv the Poincaré or disk mapping. But it lays in the nature of a curvated space to resist the attempt to simultaneously achieve these goals. One advantage of this is that the high dimensional representation  , e.g. , the word cloud  , can convey some information about the document on its own. A fundamental assumption for multimodal retrieval is that by mapping objects in a modalityconsistent latent space  , the latent space representations of semantically relevant inter-modal pairs should be consistent. The most desirable value of multimodal retrieval is to enable transfer of knowledge across different modalities so that cross-modal retrieval performance can be improved. Tradeoffs   , Pareto-optimal solutions  , and other critical information can then be read from the results. In that case  , mapping this vector of functions or  , equivalently  , this vector-valued function across the points in the space yields a multi-dimensional  , non-functional property image of the design space. The use of a solid arrow to make this connection denotes that this mapping from the problem level to the solution level facilitates two goals  , in this case both the generation of new variants and also expedited navigation. Hence in Figure 1 we connect the Functional variation dimension in the problem space to the Nominal flow change dimension in the solution space. Scans from a triangle of points in pose-space will project to a non-Euclidean triangle of points in eigenspace. This is generated during mapping; as the robot moves into unvisited areas  , it drops nodes at regular intervals  , and when it moves between existing nodes it connects them. It is also given a set of nodes in 2D-space with edges between them  , constituting a navigation graph which represents known robot-navigable space 6. Interpolating a viable object path for a given object displacement requires knowledge of the initial and fi­ nal poses as well as how the object is to be displaced. A good example of the use of geometry within this application is the mapping of two dimensional views of the roadway into a three dimensional representation which can be used for navigation. It is clear that a robust solution to this problem must involve as much generic information as possible about space and the relationship between objects in space. Repeatability is guaranteed in the augmented Jacobian method because repeated task-space motion is carried out with repeated joint-space motion  , whereas in the resolved motion method repeatability is not guaranteed. It is widely stated 3 ,that the difference between the two inverse mapping techniques lies in the repeatability. Valuable prior research has been conducted in this direction for learning hashing codes and mapping function with techniques such as unsupervised learning and supervised learning. Semantic hashing has been proposed for the problem to map data examples like documents in a high-dimensional space e.g. , a vector space of keywords in the vocabulary into a low-dimensional binary vector space  , which at the same time preserves the semantic relationship of the data examples as much as possible. Attempting a strategy which would require the user to lead the point " inside " such structures  , with no knowledge of which entrance leads to the target and which to a dead-end  , is likely to negate the human ability to see " the big picture " and degenerate into an exhaustive search of the insides of Cspace obstacles. For each data item in the compressed data  , a backward mapping is necessary to discover the coordinates of the original space  , so that a new position can be computed corresponding to the new requdsted space. These operators  , however  , rely heavily on the ability to dis cover efficiently  , given an arbitrary position in the compressed data  , the corresponding logical position in the original dntabase   , in order to reposition the data items in the new transposed space. The unique mapping maps the energies of each DoF V θ ,ψi with the appropriate phases to the force trajectory F p ,x t by neglecting the influence of handle motion ˙ r. The energies V θ ,ψi and phases ϕ θ ,ψi span a transformed state space. It is clear that the most difficult phase of object recognition is making the pointwise mapping from model to scene. The considerable computation and space requirements such an approach would usually entail are avoided by using a sparse  , minimal feature that is easily extracted to reduce the number of features that can exist in a given scene  , and by decomposing the dimensions of transform space  , and by eliminating empty regions of transform space early in the search. Second  , since it is not known initially how many steps are required for the solution  , we start with one step transition and gradually increase the number of steps as required. Formally  , any density matrix ρ assigns a quantum probability for each quantum event in vector space R n   , thereby uniquely determining a quantum probability distribution over the vector space. The Gleason's Theorem 2 can prove the existence of a mapping function µρ|vv| = trρ|vv| for any vector v given a density matrix ρ ∈ S n S n is the density matrix space containing all n-by-n positive semi-definite matrices with trace 1  , i.e. , trρ = 1. We map the human hand motion to control the dexterous robot hand when performing power grasps  , the system adopts the joint space mapping method that motions of human hand joints are directly transferred to the robot hand and the operator can adjust the posture interactively; when performing the precise tasks  , the system adopts the modified fingertip position mapping method. 7. In the teleoperation system  , we use the space mouse as the 3D input device  , which has six DOFs and can control the end point position and pose of the Staubli RX60 robot. A recent work has shown that a finger or manipulator should have at least the same number of active joints as the number of independent elements of the desired operational compliance matrix to modulate the desired compliance characteristic in the operational space 5. To find the stiffness in the joint space of each finger  , first we have to compute the unique Jacobian relation; particularly  , the forward mapping is unique in the case of the serial structured finger  , but in the case of the closed-loop structured finger  , the backward mapping is unique 5. Force sensors are built into HITDLR hand. The procedure of computing the fingertip stiffness for the given object stiffness can be consequently summarized as below. Performing this mapping also provides a means to model the relationship between question semantics and existing question-answer semantics which will be discussed further in Sect. This information is augmented with that derived from the set of answer terms  , thus by mapping a query question to the space of question-answers it is possible to calculate its similarity using words that do not exist in the question vocabulary and therefore are not represented in the topic distribution T Q . Mapping the distribution of question topics to the distribution of question-answer topics avoids problems that occur when limited vocabularies are used in a question . Relation a  , an abstraction relation  , explains how any given concrete design  , d ∈ cm  , instantiates i.e. , is a logical model of its abstract model  , m. Function c is specified once for any given abstract modeling language  , as a semantic mapping predicate in our relational logic. This mapping is described by As in 2  , see also 3  , 4  , 5  , 7  , 8  , we assume that the image features are the projection into the 2D image plane of 3D poims in the scene space  , hence we model the action of the camera as a static mapping from the joint robot positions q E JR 2 to the position in pixels of the robot tip in the image out­ put  , denoted y E JR2. This way of sharing parameters allows the domains that do not have enough information to learn good mapping through other domains which have more data. The intuition for having this objective function is to try to find a single mapping for user's features  , namely Wu  , that can transform users features into a space that matches all different items the user liked in different views/domains. A pointer in each entry of the mapping table would lead to what is essentially an overflow chain stored on the magnetic disc of records that are assigned to the hash bucket but which have not yet been archived on the optical disc. To improve efficiency  , and in particular space utilization   , implementing hashing for a file stored on a WORM disc will involve some degree of buffering on a magnetic disc for both the mapping table and the contents of hash buckets. These mapping matrices are calculated for a given coil arrangement by treating the coils as magnetic dipoles in space and are calibrated through workspace measurements as outlined in 11  , 10. where each element of I is current through each of the c coils  , B is a 3 × c matrix mapping these coil currents to the magnetic field vector B and B x   , B y   , B z are the 3 × c matrices mapping the coil currents to the magnetic field spatial gradients in the x  , y and z directions  , respectively. However  , since the thumb and the ATX are coupled by the position constraints at the attachment points  , a unique mapping can be achieved between the degrees of freedom of the thumb and the ATX leading to the redundancy of the coupled system the same as that of the thumb alone. Thus  , for a given task-space trajectory  , there will be an infinite number of possible joint-space trajectories for both the thumb and the ATX. These internal points are hidden within the polytope P and they do not contribute to manipulability information. Determining manipulability polytope requires the mapping of an n-dimensional polytope Q in joint space to an m-dimensional polytope P in task space by the transformation P = AQ with n > m. It is known that one part of the hypercube vertices becomes final zonotope vertices5  while the remainder become internal points of P . Using a known object model the interpolation of thi  , desired path can then be represented in the task space by a 3-D reconstruc­ tion or mapped directly to the image space. A desired path can be uniquely defined by chOOSing a particular decomposition of the 2-D homography or collineation mapping the projec­ tive displacement of the object features between the initial and final image poses. By performing a singular value decomposition 8 on the task space to sensor space Jacobian  , and analyzing the singular values of J and the eigenvectors of JTJ which result from the decomposition  , the directional properties of the ability of the sensor to resolve positions and orientations becomes apparent. The following sections briefly describe the derivation of the Jacobian mapping and analyze the Jacobian for various vision and force sensor configurations. From a global perspective  , in multi-robot coordination   , action selection is based on the mapping from the combined robot state space to the combined robot action space. In the context of multi-robot coordination  , dynamic task allocation can be viewed as the selection of appropriate actions lo for each robot at each point in time so as to achieve the completion of the global task by the team as a whole. The 3D Tractus was designed to support direct mapping between its physical space to the task virtual space  , and can be viewed as a minimal and inexpensive sketch-based variant of the Boom Chameleon 14. The 3D Tractus height is being tracked using a simple sensor and the stylus surface position is tracked through a tablet PC or any other touch sensitive surface interface 5. Assuming that spatial and temporal facets of concepts are potentially useful not only in human understanding but also in computing applications  , we introduce a technique for automatically associating time and space to all concepts found in Wikipedia  , providing what we believe to be the largest scale spatiotemporal mapping of concepts yet attempted. When we read a story  , we place naturally characters in time and space that provide us with further context to understand. In addition to the object-oriented description of a perspective we define a navigation path where the navigation space is restricted depending on the selected perspective. The navigation space is defined by the semantic distance between the initial concept and other related concepts. While she uses salience values to describe a metric of object similarity  , we have chosen a fuzzy set approach for mapping user terminology to the represented domain knowledge  , described in more detail in Kracke@ 1. The manipulability polytope is also more practical when the maximum velocity and/or torque of each joint is given. In order to incorporate the curiosity information   , we create a user-item curiousness matrix C with the same size as R  , and each entry cu ,i denotes u's curiousness about item i. Specifically  , MFCF maps both users and items to a latent space  , denoted as R ≈ U T V   , where U ∈ R l×m and V ∈ R l×n with l < minm  , n  , represent the users' and items' mapping to the latent space  , respectively. When users ask for a particular region  , a small cube within the data space  , we can map all the points in the query to their index and evaluate the query conditions over the resulting rows. For example  , we could map the x  , y  , and z coordinates of a data point to a single integer by using a well-known mapping function or a space-filling curve and physically order the points by three attributes at the same time. For each document in X represented as one row in X  , the corresponding row in V explicitly gives its projection in V. A is sometimes called factor loadings and gives the mapping from latent space V to input space X . Each column of V corresponds to one latent variable or latent semantic  , and by V T V = I we constrain that they are uncorrelated and each has unit variance 1 . If intervals are represented more naturally   , as line segments in a two-dimensional value-interval space  , Guttman's R-tree 15  or one of its variants including R+-tree 29 and R*-tree 1  could be used. If only multidimensional points are supported  , as in the k-d-B-tree 27  , mapping an interval  , value pair to a triplet consisting of lower bound  , upper bound  , and value allows the intervals to be represented by points in threedimensional space. A sufficient condition is that the mapping defined by the task function between the sensor space and the configuration space is onto for each t within O ,T. We recall that the feasibility of a task defined by a task function and an initial condition lies in the existence of a solution F *  t  to the equation e@  , t  = 0 for each t within O  , TI. According to the Jordan Curve Theorem  , any closed curve homeomorphic t o a circle drawn around and in the vicinity of a given point on an orientable surface divides the surface into two separate domains for which the curve is their common boundaryll. Then  , Space uses the  Alloy Analyzer—an automatic bounded verifier for the Alloy language—to compare the specialized constraints to our pattern catalog which is also specified in Alloy. Space extracts the data exposures from an application using symbolic execution  , specializes the constraints on those exposures to the types of role-based access control using the mapping provided by the user  , and exports the specialized constraints to an Alloy specification. As this technique offers conceptual simplicity   , it will be pursued. As a request must search the Q buckets contained in the fraction of the volume of the address space as defined by the request  , one method of mapping to these buckets would be to generate all possible combinations of attribute sets containing the request attributes and map to the address space one to one for each possible combina- tion. Successively  , this germinal idea was further developed  , considering the dynamics a  , multiple arms 35  , defective systems and different motion capabilities of the robotic devices 6  , 83  , wire-based manipulators  , 9  , 101. So the joint-space trajectories of the thumb can be determined by the joint-space trajectories of the ATX and vice versa. In this paper we describe the 3D Tractus-based robotic interface  , with its current use for controlling a group of robots composed of independent AIBO robot dogs and virtual software entities. This is just one method of generating a query map  , if we look further at types of mappings  , we will realise that the possibilities are endless. Instead of calculating the document scores in the latent topic space  , we can use the mapping to extract related query terms from the topic space and use an inverted index to calculate the document scores in a faster time. Spatial databases have numerous applications  , including geographic information systems  , medical image databases ACF+94   , multimedia databases after extracting n features from each object  , and mapping it into a point in n-d space Jaggl  , FRM94  , as well as traditional databases  , where each record with n attributes can be considered as a point in n-dimensional space Giit94. fractional values for the dimensionality  , which are called fractal dimensions. In order to guarantee the fast retrieval of the data stored in these databases  , spatial access methods are typically used. HiSbase combines these techniques with histograms for preserving data locality  , spatial data structures such as the quad- tree 8 for efficient access to histogram buckets  , and space filling curves 6 for mapping histogram buckets to the DHT key space. HiSbase realizes a scalable information economy 1 by building on advances in proven DHT-based P2P systems such as Chord 10 and Pastry 7   , as well as on achievements in P2P-based query pro- cessing 4. L is the number of attributes in a request i~ L~ M . In this section  , we describe an example open-source application MediumClone and demonstrate how we used Space to find security bugs in its implementation. However they are not adequate to accurately estimate the actual performance achievable at the End Effector EE for two main reasons: the ellipsoids  , or 'hyperellipsoids' in R m   , derive from the mapping to the task space of hyperspheres in the normalized joint space  , while the set of joint performances is typically characterized by hypercubes  , i.e. respectively: closeness to singularity  , isotropicity of performances and maximum performance irrespectively of the direction mentioned above. Since a continuous state s ∈ S specifies the placement of objects  , one can determine whether or not the predicate holds at s. This interpretation of which predicates actually hold at a continuous state provides a mapping from the continuous space to the discrete space  , denoted as a function map S →Q : S → Q. As an example  , Onbook  , table holds iff the book is actually on the table. Moreover  , trajectories over S give meaning to the actions in the discrete specification. For the single stance motion  , we modify the animation motion to be suitable for the robot by 1 keeping the stance foot flat on the ground  , and 2 mapping the motion in the Euclidean space into the robot's configuration space. We do not generate target motions for the double support phase  , since it is relatively short and there is not much freedom in the motion since both feet remains at their positions. Using our fully decoupled tracker and mapper design and fast image space tracking  , we are able to compute the pose estimates on the MAV in constant time at 4.39 ms while building the growing global map on the ground station. We have divided the full SLAM problem into a fast monocular image space tracking MIST on the MAV and a keyframe-based smoothing and mapping on the ground station. The approach we take is to use an online optimization of one-step lmkahead  , choosing trajectories that maximize the space explored while minimizing the likelihood we will become lost on re-entering the map. If we choose trajectories that can explore the space rapidly but allow us to return to the mapped regions sufficiently often to avoid tracking errors or mapping errors  , then we can avoid such problems. each joint performance is bounded by +/-a maximum value; the ellipsoids are formulated using task space vectors that are not homogeneous from a dimensional viewpoint  , to take into account both translational and rotational performances; the weight matrices used to normalize do not provide unique results this problem had already been identified in 5. In such a case there is one dominant direction  , which is reflected in one slot  , see figure 3 -d. The advising orientation depends on the pq-histogram quadrant where the peak is found. The position of this peak will give us a rough estimate of the free space; that is  , there is a direct mapping between the location of peak in the histogram and the angle of the free space in the image  , see figure 3-d. A single pq-histogram returns only one orientation for the free space  , which is appropriate if we are observing a wall. Overall  , the mapping of linguistic properties of the quotes in the latent bias space is surprisingly consistent  , and suggest that out-an longer  , variable period of time 32. On the negative end of the spectrum  , corresponding to international outlets  , we find words such as countries  , international  , relationship  , alliance and country names such as Iran  , China  , Pakistan  , and Afghanistan. We do not describe the mechanism of such automation due to the scope and the space limitation of this paper. the mapping from the stereotyped association to ModelElements that can reify the association can be defined formally with OCL 23 and thus allow automatically checking whether a given UML model is an instance of a given pattern. The proliferation of generated components is the main limitation of the naive method-to-component mapping. The component taxonomy can come to the rescue here-if we use it to produce a convenient number of reasonably efficient generic components that is  , a suitably parameterized component for judiciously chosen points in the space. The results 812 were encouraging but mixed and revealed some shortcomings of the AspectJ design with respect to its usability in this context. To ease the design and evolution of integrated systems  , mapping of the mediator approach into the design space of AspectJ 1 was attempted. Notice that it is possible for two distinct search keys to be mapped to the same point in the k-dimensional space under this mapping. We shall refer to the resultant multi-dimensional index structure as the bitstring-augmented multi-dimensional index. Schema mappings are inserted at the key space corresponding to the source schema at the overlay layer – or at the key spaces corresponding to both schemas if the mapping is bidirectional: U pdateSchema M apping ≡ U pdateSource Schema Key  , Schema M apping. Queries are then reformulated by replacing the predicates with the definition of their equivalent or subsumed predicates view unfolding. They use minimal space  , providing that the size is known in advance or that growth is not a problem e.g. , that one can somehow use the underlying mapping hardware of virtual memory to make the array grow gracefully. Existing Index Structures Arrays are used as index structures in IBM's OBE project Amma85. By mapping one-dimensional intervals to a two-dimensional space  , we illustrate that the problem of indexing uncertainty with probabilities is significantly harder than interval indexing  , which is considered a well-studied problem. We then change our focus to study the theoretical complexity of indexing uncertainty  , and argue that there is no formerly known optimal solution that is applicable to this problem. In the information visualization field  , mapping of data variables on the display space is often performed by means of visual attributes like color  , transparency  , object size  , or object position. A solution for visualizing icon-based cluster content summaries combined with graph layouts can be found in 8 from the information visualization research field. The local internal schema consists of a logical schema  , storage schema  , level schema. The physical schema describes the mapping of data to the memory stora e space managed by the operating system The hlg 3 level schema is a description of an application data view and it describes the next local conceptual schema in detail. The error involved in such an assignment will increase as the difference in effective table sizes between the new query and the leader increases. It is only if the cluster's space is covered by more than one plan  , that there will be an error in prediction because all the queries mapping to this cluster will be assigned the plan associated with the query leader. Within these triangles  , users were asked to compare the three systems by plotting a point closest to the best performing system  , and furthest from the worst. Space does not permit a detailed description of the experiment  , but Figure 6provides a summary by mapping out participants' responses to two questions: which system made tasks easiest to complete  , and which system they preferred overall. Similar in spirit  , PSI first chooses a low dimensional feature representation space for query and image  , and then a polynomial model is discriminatively learned for mapping the query-image pair to a relevance score. Polynomial Semantic Indexing 232 PSI. For example  , the question string " Where is the Hudson River located ? " In order to generate queries providing high precision coverage of the answer space for a given question  , custom rules were developed providing a mapping from a given question type to a set of paraphrasing patterns which would generate alternative queries. That mapping is probably the most direct  , but it leaves a number of Figure 8: Grah representation for a tetrahedral truss structure with 102 struts shown in Figure 1 empty cells. However  , the large number of cells necessary for precise mapping results in time-consuming grid update procedures. In certainty grids space is represented by a grid with each cell holding a value corresponding to the probability that an obstacle is located in that region. The master workspace was transformed into a cylindrical shaped space to assist the operator in maintaining smooth motion along a curved surface. The method of variable mapping of master t o slave motion was successfully applied to manipulation assistance in a cylindrical environment. Finally  , we introduce two applications of ILM that bring out its potential: first  , Diffusion Mapping is an approach where a highly redundant team of simple robots is used to map out a previously unknown environment  , simply by virtue of recording the localization and line-of-sight traces  , which provide a detailed picture of the navigable space. in the solution. This trajectory  , moreover  , is generate in advance. In case of the NEC PC-9821Bp 486DX2-66MHz  , the mapping of the obstacles and the possible motion area from the workspace to the posture space totally takes about 20 minutes  , however  , the generation of the obstacle avoidance trajectory only takes 0.36 seconds. However  , our experience with doing this using an optimal control approach is that the computational cost of adding many obstacles can be significant. The RRC manipulator used in this task is equipped with a Multibus-based servo control unit located in a separate cabinet. This extender allows a high-speed bidirectional shared memory interface between the two buses by mapping the memory locations used by the Multibus directly into the memory space of the PC. Since there is no natural mapping of documents to vectors in this setting  , the procedure for posts is similar. To create the topic vectors in this word-centric vector space  , we compute a weighted sum of words from the previously computed sensitive topic distributions . However  , most existing research on semantic hashing is only based on content similarity computed in the original keyword feature space. Most importantly  , the manipulability definitions are independent of the choice of parametrization for these two spaces  , as well as the kinematic mapping. For example  , the actuator characteristics are reflected in the choice of a Riemannian metric for the joint and tool frame configuration space manifolds  , or one can even include inertial parameters in the Riemannian metric to obtain a formulation for dynamic manipulablilit-y. Due to the geometrical structure of the state space and the nature of the Jacobian mapping between joint velocities and rates of change of a behavioral variable see eq. The forcelet erected over the control variables for each behavioral goal accelerates the joint angles in a direction that changes the behavioral variable in the desired way. Having a single groundstation supporting multiple low-cost MAVs while building a single globally consistent map may be a trivial solution to creating a centralized multi-robot system. Tightening the bounds in the same figure by more frequent archiving will lead to a large improvement in our model. Higher primates  , including humans  , exhibit a space-variant pattern in which the highest resolution is concentrated in the center of the field of view  , called the fovea  , with uniformly decreasing resolution to the periphery of the field of view. On the other hand  , the inverse kinematic method has symbolic solutions only in types of manipulator kinematics 7. However  , this method -be it symbolic or numerical -is attractive because of the direct mapping from the workspace to joint space  , fixing most of the aforementioned problems of the resolved motion method. The outer radius rout is defined by the smallest circumscribed sphere with the reference point of the robot as its center. On-line control command is calculated mapped from the learned lookup table with the on-line sampled new sensor signals. Summarizing  , in this paper we present a framework for solving efficiently the k-anonymity and -diversity problems  , by mapping the multi-dimensional quasi-identifiers to 1-D space. 21 are worse in terms of information loss and they are considerably slower. In the context of a search engine  , inverted index compression encoding is usually infrequent compared to decompression decoding   , which must be performed for every uncached query. Although we have to store a mapping table for fast block locating  , the extra space occupied by it is much smaller than that used by the inverted index itself. During the mapping of FMSVs  , the most effective heuristic feature sets are selected to ensure reasonable prediction accuracy. 3 and to map text information into DVs for social information related music dimensions 13  , a supervised learning based scheme  , called CompositeMap  , is developed to generate a new feature space. In order to establish a representation of the environment configuration  , we transformed the calculated depth to a safety distribution histogram. By a random exploration which is limited  , according to the low mobility  , the system will associate perceptive sktes and sequences of action that pennit to reach its goal particular context. This is another issue that has seen a great deal of exploratory research  , including studies of offices and real desks 6. In our system  , tags provide an additional basis for mapping the document space  , reflecting our focus on the organization of a local workspace. For example  , pattern matching classes that encode multi- DoF motions 22 or force functions for each joint 9; or direct control within a reduced dimensionality space 14. Recent academic work within the field of simultaneous control thus has emphasized alternative mapping paradigms. For example  , a mapping in the coordinate space of a dictionary which contains two identical elements would result in two identical coefficients  , each corresponding to the contribution of one of the identical dictionary elements. In the current work we adopt a centroid-based representation  , where every dimension v i ,j corresponds to the distance between the contour point s i ,j and the contour's mass center. However  , our study shows that fractal dimensions have promising properties and we believe that these dimensions are important as such. This means that the methods in this paper do not provide a mapping to a lower-dimensional space  , and hence traditional applications  , such as feature reduction  , are not directly possible. They also use a query-pruning technique  , based on word frequencies  , to speed up query execution. Iceberg queries 7 uments and their associated term frequencies in relational tables  , as well as for mapping boolean and vector-space queries into standard SQL queries. For example  , outlets on the conservative side of the latent ideological spectrum are more likely to select Obama's quotes that contain more negations and negative sentiment  , portraying an overly negative character. By mapping the quotes onto the same latent space  , our method also reveals how the systematic patterns of the media operate at a linguistic level. The constraints associated with these exposures and the user-provided mapping are passed through a constraint specializer  , which re-casts the constraints in terms of the types in our pattern catalog. Space uses symbolic execution to extract the set of data exposures 25 from the source code of a Ruby on Rails application. Then  , Space uses the Alloy Analyzer to perform automatic bounded verification that each data exposure allowed by the application is also allowed by our catalog. Given the correct user-provided mapping  , the patterns applied by Space were always at least as restrictive We examined the code of the applications in our experiment for precisely this situation—security policies intended based on evidence in the code itself to be more restrictive than the corresponding patterns in our catalog—and found none. Q4 no results presented due to lack of space features the 'BEFORE' predicate which may be expensive to evaluate. We remark that System C also uses a data mapping in the spirit of 23  that results in comparatively simple and efficient execution plans and thus outperforms all other systems for Q2 and Q3. This makes it very difficult for GA to identify the correct mapping for an item. This happens because the space of possible one-to-n mappings is huge and it is possible to find many candidate mappings having similar i.e. , slightly lower fitness value. Figure 2shows a simple example of query reformulation. Thus  , LSH can be employed to group highly similar blocks in buckets  , so that it suffices it compare blocks contained in the same bucket. In our case  , blocks are the items that are represented in the high-dimensional space of E or E 1 and E 2  through Block Mapping. To avoid epoch numbers from growing without bound and consuming extra space  , we plan to " reclaim " epochs that are no longer needed. To allow users to refer to a particular realworld time when their query should start  , we maintain a table mapping epoch numbers to times  , and start the query as of the epoch nearest to the user-specified time. To handle this sort of problem  , space-filling curves as Z-order or Hilbert curves  , for instance  , have been successfully engaged for multi-dimensional indexing in recent years 24 . Based on the findings from our evaluations  , we propose a hybrid approach that benefits from the strength of the graph-based approach in visualising the search space  , while attempting to balance the time and effort required during query formulation using a NL input feature. Their methodology is based on mapping the underlying domain ontologies into views  , which facilitates view-based search. This system may be implemented in SMART using the set of modules shown in figure 4. If the joint torque signal provides a poor measure of the tool contact forces  , then a force sensor may be used in conjunction with the master  , but the forces from the sensor must be brought into joint space by mapping through the manipulator Jacobian. 2  , this direction changes during movement  , even in the absence of other perturbations. Therefore  , we can control the closed-chain system with the same control structure in Equation This immediately provides an important result; the dynamically consistent null space mapping matrix for the closed-chain system is the same as the one for the open-chain system   , N in Equation 9. The time savings would be crucial in real-world applications when the category space is much larger and a real-time response of category ranking is required . The actual mapping time was reduced from 2.2 CPU seconds per document to 0.40 seconds. But since only partial term-document mapping is preserved  , a loss in retrieval performance is inevitable. This technique was proposed to mitigate the efficiency issue caused by operating a large index  , for that a smaller index loads faster  , occupies less disk space  , and has better query throughput. The expected disc space consumption for a buffered hashing organization BHash for WORM optical d.iscs is analyzed in 191. The " new " records will be merged with the old logically undeleted ones already bon the optical disc and written together on new tracks; the mapping table will also be updated to reflect the changes. Based on that  , a bridging mapping is learned to seamlessly connect these individual hamming spaces for cross-modal hashing . In this paper  , we propose a novel technique by learning distinct hamming space so as to well preserve the flexible and discriminative local structure of each modality. In addition  , superposition events come with a flexible way in quantifying how much evidence the observation of dependency κ brings to its component terms. The proposed mapping allows for the representation of relationships within a group of terms by creating a new quantum event in the same n-dimensional space. The order of this list was fixed to give a one-to-one mapping of distinct terms and dimensions of the vector space. Given the entire collection of shots  , we obtained a list of all of the distinct terms that appear in the ASR for the collection. Secondly  , transaction language constructs should be functions in the logic such that transactions can be represented as expressions mapping states to states that can be composed to form new transactions . The set of states should characterize the space of database evolution. This section contains the results of running several variations of the traversal portion of the 001 benchmark using the small benchmark database of 20 ,000 objects. Given our understanding of how OS works  , we believe this is partially due to the overhead of mapping data into the client's address space. We called this forest  , Reconfigurable Random Forest RRF. Since previously learned RRT's are kept for fkture uses  , the data structure becomes a forest consisting of multiple RRTs. We will give a brief summary of the random forest c1assifier. If the forest has T trees  , then Random Forest Classifier In our production entity matching system  , we sometimes use a Random Forest Classifier RFC 18 for entity matching. The rules with extensional predicates can be handled very naturally in our framework. We convert the random forest classifier into a DNF formula as explained in Section 4.3. The matcher is random forest classifier  , which was learnt by labeling 1000 randomly chosen pairs of listings from the Biz dataset. For the data set of small objects  , the Random Forest outperforms the CNN. For most of them  , the Random forest based classifiers perform similar to CNNbased classifiers  , especially for low false positive rates. We describe here a technique to approximate the matcher by a DNF expression. First  , we describe its overall structure Sec. We next present our random forest model. We use Survival Random Forest for this purpose. the user leaving the ad landing page. We use scikit-learn 28 as the implementation of the Random Forest Classifier. template. By averaging over the response of each tree in the forest  , the input fea ture vector is classified as either stable or not. The dimensionality of the template is very high when considering it as the input to the Random Forest The feature vector serves as an input to a Random Forest C lassifier which has been trained offline on a database. This method learns a random for- est 2  with each tree greedily optimized to predict the relevance labels y jk of the training examples. Random Forest. Training a single tree involves selecting √ m random intervals  , generating the mean  , standard deviation  , and slope of the random intervals for every series. Time Series Forest TSF 6: TSF overcomes the problem of the huge interval feature space by employing a random forest approach  , using summary statistics of each interval as features. All the random forest ranking runs are implemented with RankLib 4 . We have submitted 6 ranking-based runs. Similar to the balanced Random Forest 7  , EasyEnsemble generates T balanced sub-problems. The idea behind EasyEnsemble is quite simple. Solid lines show the performance of the CNNbased model. Dashed curves refer to the Random Forest based classifiers. The more correlated each tree is  , the higher the error rate becomes. The error rate of a random forest depends on two factors: the correlation between trees in the forest and the strength of each individual tree. The 90 th percentile say of the random contrasts variable importances is calculated. A random forest 5  is then built using original and random contrast variables and the variable importance is calculated for all variables. Other methods require  , in fact  , setting the dwell time threshold before the model is actually built. The survival random forest based model not only slightly outperforms all the other competing model including a suite of classification random forest but  , more importantly  , it allows to compute the survival at di↵erent thresholds. On Restaurants  , for example  , the random forest-based system had run-times ranging from 2–5 s for the entire classification step depending on the iteration. The reason why this observation is important is because the MLP had much higher run-times than the random forest. We discretize the height map into a grid of 48 x 48  , for all 3 channels. For each tree  , a random subset of the total training data is selected that may be overlapping with the subsets for the other trees. 2 Training a Random Forest: During trammg of the forest  , the optimization variables are the pairs of feature component cPij and threshold B per split node. There are only two parameters to tune in random forests: T   , the number of trees to grow  , and m  , the number of features to consider when splitting each node. First  , random forest can achieve good accuarcy even for the problem with many weak variables each variable conveys only a small amount of information. A random forest has many nice characteristics that make it promising for the problem of name disambiguation. The Random Forest model selects a portion of the data attributes randomly and generates hundreds and thousands of trees accordingly  , and then votes for the best performing one to produce the classification result. This reasoning may partially explain why ensemble tree models  , such as Random Forest  , are considered superior to standalone tree models. Our selected encoding of the input query as pairs of wordpositions and their respective cluster id values allows us to employ the random forest architecture over variable length input. The final output of the forest is a weighted sum of the class distributions output by all of the trees in the forest. The open parameters for the forest training are the minimum cardinality of the set of training points at a leaf node  , the maximum number of feature components to sampIe at each split node and the number of trees in the forest. There were 100 trees used in the random forest approach and in the ensemble for the random subspace approach. The ensemble size was 200 trees for the Dietterich and RTB approaches. The size of the ensembles was chosen to allow for comparison with previous work and corresponds with those authors' recommendations. We submitted two classification runs: RFClassStrict and RFClassLoose. We employ Random Forest classifier implementation in Weka toolkit 7 with default parameter settings. ICTNETVS07 is the Borda Fuse combination of three methods. ICTNETVS06 uses Random Forest text classification model  , the result is the sum of voting. High F1 score shows that our method achieves high value in both precision and recall. Random Forest is the classifier used. We experimented with several learning schemes on our data and obtained the best results using a random forest classifier as implemented in Weka. Learning scheme. the two baselines  , when using a random forest as the base classifier. Where applicable  , both F-Measures pessimistic and re-weighted are reported. Four types of documents are defined in CCR  , including vital  , useful  , neutral  , garbage. We employ Random Forest classifier in Weka toolkit 2 with default parameter settings. ClassificationCentainty as 'compute the Random forest 4  class probability that has the highest value'. This is an implementation of an entity identification problem 50. An Evidential Terminological Random Forest ETRF is an ensemble of ETDTs. C while the case of uncertain-membership will be labeled by L = {−1  , +1}. Figure 7 plots the accuracy of using different groups of features when applying Random Forest. Accuracy is defined as the percentage of answers classified cor- rectly. In Random Forest  , we  already randomly select features when building the trees. In both cases  , such features cause over-fitting in the prediction. ICTNETVS02 uses Random Forest text classification model  , the result is the sum of probabilities. ICTNETVS1 is based on traditional information retrieval IR model. The final classification P c|I  , x is given by averaging over these distributions. At test time  , the random forest will produce T class distributions per pixel x. Our choice is based on previous studies that showed Random Forests are robust to noise and very competitive regarding accuracy 9. For the relevance classifier we use an ensemble approach: Random Forest. RIF draws ideas from the interval feature classifier TSF 6  and we also construct a random forest classifier. To overcome this we propose a new classifier: the Random Interval Feature RIF Ensemble. Specifically  , our random forest model substantially outperforms all other models as query length increases. Yet  , in the CQA domain  , the differences are vast. We show further evidence for this statement in Section 4.4. The pairwise distance function is learned using a random forest. The examples of keyphrases extracted by SEERLAB system are shown in Table 1. The scores in Table 9show that our reduced feature set performs better than the baselines on both performance measures. The second is LTR's Random Forest LTR-RF. A classification tree is easier to understand for at least two reasons. classification tree is easier to understand than  , say  , a random forest. In particular  , each example is represented by two types of inputs. The input to our random forest is all categorical  , and is given as key-value pairs. Each tree is composed of internal nodes and leaves. Our random forest is composed of binary trees and a weight associated with each tree. Document-query pairs which are classified as relevant will award extra relevance score. For pointwise  , random forest is utilized to classify the candidate pairs in the new result. We disambiguate the author names using random forest 34. Note that different authors may share the same name either as full names or as initials and last names. The forest cover data contains columns with measurements of various terrain attributes  , which are fairly random within a range. In this case  , we see that RadixZip consistently loses. The Random Forest classifier delivers the best result for all three categories. The results show that our approach clearly outperforms both baseline approaches on all three categories. For large objects  , it performs significantly better at higher false positive rates. The classification accuracy of this model is lower than that of the CNN and Random Forest. This is only used to select positively classified test points. We use the entire 1.2k labeled examples   , which are collected in December 2014  , to train a Random Forest classifier. Experiment Setup. However  , this resulted in severe overfitting . We note that during our research we also trained our random forest using the query words directly  , instead of their mapped clusters. We leverage a Random Forest RF classifier to predict whether a specific seller of a product wins the Buy Box. Classifier Selection. Figure 8 : Compare the F 1 score higher is better when using different groups of features. 5: ROC curves for the datasets a Medium b Large c All . On Persons 1  , all three systems performed equally well  , achieving nearly 100 % F-Measure. Figure 2shows the results for the random forest base classifier. The metric we used for our evaluation is the F1-score. The remaining data are fed to a random forest classifier 4. On the other hand  , however  , no-one will contest that a small! An example for our CQA intent classification task may be {G : 0.3  , CQA : 0.7}  , which means that the forest assessment of an input query is that it is a general Web query G with 30% probability  , and a CQA query CQA with 70% probability. Standard generalization bounds for our proposed classifier can readily be derived in terms of the correlation between the trees in the forest and the prediction accuracy of individual trees. As a result  , we were able to train our multi-label random forest classifier on a medium sized cluster in less than a day. For the random forest approach  , we used a single attribute  , 2 attributes and log 2 n + 1 attributes which will be abbreviated as Random Forests-lg in the following. In the random subspace approach of Ho  , exactly half n/2 of the attributes were chosen each time.  Time Series Forest TSF 6: TSF overcomes the problem of the huge interval feature space by employing a random forest approach  , using summary statistics of each interval as features. Trees are trained on the resulting 3 √ m features and classification is by majority vote.  A deeper investigation confirms our intuition that defective entities have significantly stronger connections with other defective entities than with clean entities. The random forest classifier appears in the first rank. The model turned out to be quite effective in discriminating positive from negative examples. For each of the three tested categories we trained a different classifier based on the Random Forest model described in Section 3.2.2. Table 2presents the 15 most informative features to the model. We analyzed the contribution of the various features to the model by measuring their average rank across the three classifiers   , as provided by the Random Forest. Care was taken to avoid over fitting and to ensure that the learnt trees were not lopsided. A hundred trees were learnt in MLRF's random forest for each data set. We demonstrated that our proposed MLRF technique has many advantages over ranking based methods such as KEX. We evaluated the bid phrase recommendations of our multilabel random forest classifier on a test set of 5 million ads. Then a new result is achieved ordered by the combination of scaled scores of three retrieval model. Guild quitting prediction classifiers are built separately for 3 WoW servers: Eitrigg  , Cenarion Circle  , and Bleeding Hollow. Table 7 reports the classification performance for a random forest with 10 trees and unlimited depth and feature counts. We use a Random Forest that predicts stable grasps at similar accuracy as a Convolutional Neural Net CNN and has the additional ability to cluster locally similar data in a supervised manner. Furthermore  , it provides the aforementioned local shape representation. We are specifically considering templates that are classified to be graspable. In this section  , we show how our Random Forest classifiers can be used to predict global object shape from local shape information. A stopping criterion of the error leveling off suffices. It is possible to use the out of bag error to decide when to stop adding classifiers to a random forest ensemble or bagged ensemble. Our training set consists of 13 ,649 images; and among them  , 3 ,784 were pornography and 9 ,865 were not. Once the features have been computed for an image  , they are fed into a random forest 6 classifier. Predictions using our multi-label random forest can be carried out very efficiently. The active label distributions can be aggregated over leaf nodes and the most popular labels can be recommended to the advertiser. However  , the techniques we use in building the trees  , in particular the choice of variables and values used to split nodes of the tree  , are fairly distinct. Our system uses Random Forest RF classifiers with a set of features to determine the rank. In addition  , the system must issue a confidence score ∈0  , 1000 ∈ Z where 1000 is very confident. Classification results were similar for a number of prediction models. As such most digits after the first are randomly distributed. These features are: SessionCount  , SessionsPerUserPerDay and TweetsClickedPerSender. Figure 6 shows that with the three features contributing most to model accuracy a random forest model can achieve a similar result as it would with 80 features or more. For each user engagement proxy  , we trained a random forest RF classifier using the feature set described in Section 4.2. The value of our prediction task lies in the fact that we use highly discriminative yet low-cost features. Figure 2shows the system architecture of CollabSeer. To minimize the impact of author name ambiguity problem  , the random forest learning 34  is used to disambiguate the author names so that each vertex represents a distinct author. This OOB error estimate is also used later in the computation of variable importance. Random forest consistently outperforms all other classifiers for every data set  , achieving almost 96% accuracy for the S500 data. Each fold is stratified so that it contains approximately the same proportions of class distribution as the original dataset. Figure 1reports these scores. The mean decrease Gini score associated by a random forest to a feature is an indicator of how much this feature helps to separate documents from different classes in the trees. Then for each number of indicators  , we learn a Random Forest on the learning set and evaluate it. As mRMR takes into account redundancy between the indicators  , this should not be a major issue. The MIA and CDI validity index calculations are not comparable between datasets due to the different number of attributes used. The random forest and pam combination provides middling results. An alternate keypoint-based approach has been described by Plagemann et al. In the body-part detector used by Microsoft's Xbox Kinect 1   , each pixel is classified based on depth differences of neighbouring pixels using a random forest classifier. We base such evaluation on a dataset with 50K observations ad  , dwellT ime  , which refer to 2.5K ads provided by over 850 advertisers. The predictive accuracy of our implementation of survival random forest is assessed with an o↵-line test. We developed a novel multi-label random forest classifier with prediction costs that are logarithmic in the number of labels while avoiding feature and label space compression. Each label  , in our formulation   , corresponds to a separate bid phrase. Table 10 shows our best performance according to micro average F and SU. For example RF_all_13_13 stands for Random Forest using all features  , trained on 2013 and applied on 2013 9 . We view the CCR problem as a 3-class classification problem by combining garbage and neutral as a single non-useful class. After training the random forest c1assifier as above  , there is a minimum number of training data points at each leaf node. 4 consists of the union of all corresponding sets: ProductionBiz: This is the actual matcher used in the production system for matching the Biz dataset. From classification   , the 2-step approach's Random Forest is used as a baseline MC-RF. We have included two of the highly performing methods on 2012 CCR task as baselines. PF  , CmF  , TF  , CtF denotes the results when our frameworks used personal features  , community features  , textual features  , and contextual features  , respectively. Gini importance is calculated based on Gini Index or Gini Impurity  , which is the measure of class distribution within a node. There are two methods of measuring variable importance in a random forest: by Gini importance and by permutation importance. The former classifies the candidate documents into vital or useful  , while the latter classifies the candidate documents into relevant vital + useful or irrelevant neutral + garbage. 4 and 5 show the ROC curves for all five datasets. The classification is done using a random forest classifier trained on a set of 1700 positive and 4500 negative examples 18. The features are listed in Table Iand extend the set proposed in 3 and 4. 7 Given the large class imbalance  , we applied asymmetric misclassification costs. In the case of Persons 2 and Restaurants  , both methods performed equally well.  Incorporating both context i.e. forest-fire with random seeds seem to perform well for themes that are of global importance  , such as 'Social Issues' that subsumes topics like '#BeatCancer'  , 'Swine Flu'  , '#Stoptheviolence' and 'Unemployment'. In this paper  , the term isolation means 'separating an instance from the rest of the instances'. Hence  , when a forest of random trees collectively produce shorter path lengths for some particular points  , then they are highly likely to be anomalies. However   , instead of using time domain intervals  , we use intervals from the data transformed into alternate representations. To convert a random forest into a DNF  , we first convert the space of predicates into a discrete space. Different trees may have different thresholds for the same predicates  , and can use different matching functions on the same attributes. We found that for the random forest that we learnt  , the conversion resulted in a DNF formula with 10 clauses. In theory  , this conversion may generate a DNF with exponentially many clauses. This is a generic technique which we can apply in practice to any arbitrary pair-wise matching function. As of now we do not perform any person specific disambiguation however one could treat acknowledged persons as coauthors and use random forest based author disambiguation 30 . The assumption is reasonable given the patterns of acknowledgments described in the introduction. Variable importance is a measurement of how much influence an attribute has on the prediction accuracy. There is small change from 100 to 500 trees  , suggesting that 100 trees might be sufficient to get a reasonable result. Figure 3shows the accuracy on S500 data  , as the trees were grown in the random forest. CollabSeer is built based on CiteSeerX dataset. A pair where the first candidate is better than the second belongs to class +1  , and -1 otherwise. Specifically  , a Random Forest model is used in the provided Aqqu implementation. None of the classical methods perform as well. This table shows that after feature selection  , the proposed method is about three times faster than the sate-of-the-art random forest method  , and achieves greater accuracy. In both works  , the authors showed that there exist some data distributions where maximal unprunned trees used in the random forests do not achieve as good performance as the trees with smaller number of splits and/or smaller node size. In fact  , 25  , 27  validate the overfitting issue faced by random forest models when learning to classify high-dimensional noisy data. That way  , there is a set of contrast variables that we know are from the same distribution as the original variables and should have no relationship with our target variable Y since Z i is a 'shuffled' X i . Further  , we limit ourselves to the " Central " evaluation setting that is  , only central documents are accepted as relevant and use F1 as our evaluation measure. To remain focused  , we use a single representative for each family of approaches: Random Forest 3-step for classification and Random Forests for ranking. Random subspaces ties for the most times as statistically significantly more accurate than C4 .5  , but is also less accurate the most times. Compared to C4.5 a random forest ensemble created using log 2 n + 1 attributes is very good and RTB- 20 is the best by a rather small increment. Random forests use a relatively small number of attributes in determining a test at a node which makes the tree faster to build. We compare two strategies for selecting training data: backward and random. We use the most recent 400 examples as hold-out test set  , and gradually add in examples to the training set by batches of size 50  , and train a Random Forest classifier. Random forests provide information on how well features helps to separate classes and give insight on which ones help to characterize centrally relevant documents about an entity in a stream. In sum  , most of the previous work has tackled issues related to improving the choice of features or the quality of the forest of trees. Since the evaluation of the entire ensemble is critical for the reweighting step on the next iteration  , and the previous ensemble state may be already overfitted  , the errors may be unwittingly propagated as the random forest is built  , being not robust to such high dimensional noisy data. rate  , receive-rate  , reply-rate  , replied-rate yield the best performance with AUC > 0.78 for female to sample male  , and AUC > 0.8 for male to sample female to male under the Random Forest model among all graph-based features. result in the best performance with AUC > 0.76 for female to sample male  , and AUC > 0.8 for male to sample female under Random Forest model among all user-based features  , while the topological features Figure 5: Performance of classifiers with user-based  , graph-based  , and all features to predict reciprocal links from males to females. This random partitioning produces noticeable shorter paths for anomalies since a the fewer instances of anomalies result in a smaller number of partitions – shorter paths in a tree structure  , and b instances with distinguishable attribute-values are more likely to be separated in early partitioning . Laplacian kernels are defined mathematically by the pseudoinversion of the graph's Laplacian matrix L. Depending on the precise definition  , Laplacian kernels are known as resistance distance kernels 15  , random forest kernels 2  , random walk or mean passage time kernels 4  and von Neumann kernels 14. In order to apply Laplacian kernels to graphs with negative edges  , we use the measure described as the signed resistance distance in 17  , defined as: An example of generated classification tree is shown in Figure 1due to limited space  , we just show the left-hand subtree of the root node. Training data  , with pre-assigned values for the dependent variables are used to build the Random Forest model. In summary  , the recall precision curves of all three categories present negative slopes  , as we hoped for  , allowing us to tune our system to achieve high precision. This can be easily debugged in the random forest framework by tracing the ad down to its leaf nodes and examining its nearest neighbours. Many of the suggestions  , particularly those beyond the top 10  , were more relevant to an Italian restaurant rather than a Thai restaurant. This enabled us to efficiently carry out fine grained bid phrase recommendation in a few milliseconds using 10 Gb of RAM. Then  , calculate the error rate of the random forest on the entire original data  , where the classification for each data point is done only by its out-of-bag trees. For every data point x in the original data  , define the out-of-bag OOB trees of x as the set of trees where x is not included in their bootstrap samples. Given a query template that is c1assified by the Random Forest  , we can not only predict its probability to afford a successful grasp but also make predictions about latent variables based on the training examples at the corresponding leaf nodes. V for more detail on the database. In other words  , we can see that the HeteroSales framework is especially useful in the case when we only have a limited number of training data. For example  , in the scenario of training ratios to be 5% and 10%  , the AUCs of HS-MP are around 4%∼5% larger than the AUCs of the random forest. Given the feature set and the class labels stable or shrinking  , we want to predict whether a group or community is likely to remain stable or will start shrinking over a period of time. We achieve qualitatively similar results for the other two servers; for instance  , the random forest classifier produces a prediction accuracy of 81% on Bleeding Hollow  , and 84.3% on Cenarion Circle. The cost of traversing each tree is logarithmic in the total number of training points which is almost the same as being logarithmic in the total number of labels. For instance  , if two labels are perfectly correlated then they will end up in the same leaf nodes and hence will be either predicted  , or not predicted  , together. Finally  , while we did assume label independence during random forest construction  , label correlations present in the training data will be learnt and implicitly taken into account while making predictions. The only conceptual change is that now yi ∈ ℜ K + and that predictions are made by data points in leaf nodes voting for labels with non-negative real numbers rather than casting binary votes. However  , by deliberate design  , we need to make no changes to our random forest formulation or implementation as discussed in section 3. For example  , we can divide the range of values of JaroWinklerDistance into three bins  , and call them high  , medium and low match. The best fit between the number of trees and the learning time is given by the function T ime = #T rees · 0.22 1.65 with an adjusted R 2 coecient of 0.96. Hence  , connectivity-based unsupervised classifiers offer a viable solution for cross and within project defect predictions. In the within-project setting i.e. , models are built and applied on the same project  , our spectral classifier ranks in the second tier  , while only random forest ranks in the first tier. We conjecture that the decrease in performance when changing to a within-project setting is caused by the low ratio of defects i.e. , the low percentage of defective entities in the target project. In particular  , the random forest classifier achieves an AUC value of 0.71 in a cross-project setting  , but yields a lower AUC value of 0.67 in a within-project setting. Table 7shows 10 most indicative features in the MIX+CKP model according to this measurement. In random forest  , one way to measure the importance of a feature in a model is by calculating the average drops in Gini index at nodes where that feature is used as the splitting cri- teria 6. In the first experiment we apply the previously trained Random Forest model to identify matching products for the top 10 TV brands in the WDC dataset. The results show that we are able to identify a number of matches among products  , and the aggregated descriptions have at least six new attribute-value pairs in each case. Note that it was not always the case that the best performance was achieved in the last iteration. Given this disparity in run-times between the two classifiers  , the random forest is clearly a better base classifier choice for the IAEI benchmarks  , and considering only the slight performance penalty  , ACM-DBLP as well. We also found that adding implicit state information that is predicted by our classifier increases the possibility to find state-level geolocation unambiguously by up to 80%. We found that we are able to predict correctly implicit state information based on geospatial named entities using a Random Forest RF classifier with precision of 0.989  , recall 0.798  , and F1 of 0.883  , for Pennsylvania. Table 2The performance of submitted runs with vital only Table 3shows the retrieval performance of our submitted two runs for Stream Slotting Filling task. We can see from the table that runs using random forest have better retrieval performance than others. For each selected name  , we then manually cluster all the articles in Medline written by that name. To evaluate the performance of the random forest for disambiguation  , we first randomly select 91 unique author names as defined by the last name and the first initial from Medline database. For each pair of candidate answers Aqqu creates an instance  , which contains 3 groups of features: features of the first  , the second candidate in the pair and the differences between the corresponding features of the candidates. We employ a random forest classifier as the discriminative model and use its natural ability to cluster similar data points at the leaf nodes for the retrieval task. In this paper  , we simultaneously address grasp prediction and retrieval of latent global object properties. For Australian   , German and Ionosphere data sets there is improvement of 1.98%  , 5.06% and 0.4% respectively when compared with Random Forest Classifier. The proposed ensemble feature selection FS technique using TS/NN has achieved higher accuracy in all data sets except Diabetes. These results strongly support our claim that our generic ordering heuristic works well in a variety of application domains. Using the above evaluations we found that our generic heuristic dominates random ordering  , although the latter sometimes has increasingly competitive accuracy as more time passes before interruption  , particularly for 'Forest Cover Type' and 'Pen Digits' datasets. We design a Multi-Label Random Forest MLRF classifier whose prediction costs are logarithmic in the number of labels and which can make predictions in a few milliseconds using 10 GB of RAM. Our contributions are as follows: We pose bid phrase recommendation as a multi-label learning problem with ten million labels. We order the 1.2k labeled examples by time from the oldest to the most recent. Analyzing hundreds of tweets from Twitter timeline we noticed some interesting points. Table 5and 6 show the corresponding precisions  , recalls and F-measures of the Cost Sensitive classifier based on Random Forest  , which outperformed the other classifiers yielding an 90.32% success in classification for our trained model. In addition  , a random forest is very fast both in the training and making predictions  , thus making it ideal for a large scale problem such as name disambiguation. This is useful because users generally use such rules to disambiguate names; for an example  , " if the affiliations are matched  , and both are the first author  , then .. " . Here  , we first give the formal formulation of the author name disambiguation problem and then define the set of attributes  , called the similarity profile  , that will be used by random forest for disambiguation. English  , Chinese yeari = paperi's year of publication meshi = set of mesh terms in the paperi For both the intrinsic and the stacked models  , we use the Random Forest classifier provided by Weka  , set to use 100 trees  , and the default behavior for all other settings. In total  , 14 Stacked Features were added 7 aggregates each  , which were applied to the top k in-links and out-links separately. After another 500 random planning queries  , the empty area that was originally occupied by the obstacle is quickly and evenly filled with new nodes  , as shown in Figure 8d. The roots of these trees  , surrounding the moved obstacle  , indicate where the forest is split. Positive examples were obtained by setting up the laser scanner in an open area with significant pedestrian traffic; all clusters which lay in the open areas and met the threshold in Sec. Several appearance-based methods for hand detection in depth images have been proposed in recent research. The Forest Cover Type problem considered in Figure 9is a particularly challenging dataset because of its size both in terms of the number of the instances and the number of attributes. Because we have a much smaller testing set the curves are less smooth  , however  , SimpleRank clearly beats Random up to the first 2 ,000 examples. We discuss how to automatically generate training data for our Multi-Label Random Forest classifier and show how it can be trained efficiently and used for making predictions in a few milliseconds . We then develop our multi-label formulation in Section 3. In the rest of the experiments  , we configured Prophiler to use these classifiers. It can be seen that the classifiers that produced the best results were the Random Forest classifier for the HTML features  , the J48 classifier for the Java- Script features  , and the J48 classifier for the URL-and host-based features. Table 4  , and for project " Ivy v1.4 "   , the top four supervised classifiers experience a downgraded performance when changing from a crossproject setting to a within-project setting. The reduced random forest model using just those two variables can attain almost 90% accuracy. auth last idf   , auth mid  , af f tf idf   , jour year dif f   , af f sof ttf idf   , mesh shared idf for RF-P ity between author's middle name are the most predictive variables for disambiguating names in Medline. Our experiments with feature selections also demonstrate that near-optimal accuracy can be achieved with just four variables  , the inverse document frequency value of author's last name and the similarity between author's middle name  , their affiliations' tfidf similarity   , and the difference in publication years. Please note that we build a global classifier with all training instances instead of building a local classifier for each entity for simplicity. All the classifiers are implemented with random forest classification model  , which was reported as the best classification model in CCR. We will show that we can predict the global object shape based on the locally similar exemplars. We perform modelling experiments framed as a binary classification problem where the positive class consists of 217 of the re-clicked Tweets analysed above 5 . Therefore only results from the Random Forest experiments are reported  , specifying F1  , accuracy and the area under the ROC curve AUC. To understand which features contribute most to model accuracy and whether it is possible to reduce the feature manner. Given that the proposed system is evaluated over seven iterations   , we plot for each benchmark the precision-recall curve for the iteration in which the proposed system achieved the highest F-Measure. If the random forest-based classifier is used on Restaurants  , the difference widens by about 1 % see previous footnote. The table show that  , on average  , even the pessimistic estimate exceeds the next best the Raven boolean classifier system performance by over 4.5 %. The MLP-based system achieved run-times ranging from 17 s for the first iteration to almost 20 min for the final iteration. The final ranking is performed using the same learning-to-rank method as the baseline Aqqu system 3  , which uses the Random Forest model. As described in detail next  , this information is used to develop novel features for detecting entities and ranking candidate answers. If the impact is less significant  , then the difference between the original and re-test result may be not so noticeable  , as shown in the Page Blocks dataset. As we are using binary indicators  , some form of majority voting is probably the simplest possible rule but using such as rule implies to choose very carefully the indicators 13. In our future work  , we will compare Random Forest to simpler classifiers. We develop a sparse semi-supervised multi-label learning formulation in Section 4 to mitigate the effects of biases introduced in automatic training set generation. We then extend our MLRF formulation to train on the inferred beliefs in the state of each label and show that this leads to better bid phrase recommendations as compared to the standard supervised learning paradigm of directly training on the given labels. Only our proposed Random- Forest model manages to learn the discriminating features of long queries as well as those of short ones  , and successfully differentiates between CQA queries and other queries even at queries of length 9 and above. On the other hand  , PosLM  , which models only structure  , performs the worst  , showing that a combination of content and structure bearing signals is necessary. Table 4presents examples for queries of different length in each domain  , which illustrate the differences between the tested domains. The former one classifies the candidate documents into vital or non-vital  , yet the latter one classifies them into relevant vital + useful  or irrelevant unknown + non-referent. They also explored using random forest classification to score verticals run ICTNETVS02  , whereby expanded query representations based on results from the Google Custom Search API were used. For ICTNETVS1  , they calculated a term frequency based similarity score between queries and verticals. We achieved convergence around 300 trees  , We also optimized the percentage of features to be considered as candidates during node splitting  , as well as the maximum allowed number of leaf nodes. Considering the Random Forest based approaches we vary the number of trees ranging from 10 to 1000. A leaf node l stores a distribution P l c over class labels c. This distribution is modeled by a histogram computed over the class labels of the training data that ended up at this leaf node. Especially in our case where the input forms a local shape representation  , these reduced data sets are clusters of locally similar data. These variables can recover the global shape of the associated object. On Persons 1  , the three curves are near -coincidental  , while in the case of ACM-DBLP  , the best performance of the proposed system was achieved in the first iteration itself hence  , two curves are coincidental. This confirms earlier findings that the MLP can be slower by 1–2 orders of magnitude  , and has a direct dependence on the size of the training set 27. We can observe that the other classifiers achieve high recall  , i.e. , they are able to detect the matching pairs in the dataset  , but they also misclassify a lot of non-matching pairs  , leading to a low precision. The random forest classifier offers two means of determining feature importance: Out of Bag Permuted Variable Error PVE and the Gini Impurity measure 2 . We aim to identify the topics which best characterize this intent and use those topics to infer the latent community structure. These results indicate that these two feature sets are most influential among all feature sets. Thus  , the dependent variable is represented by the cluster implementation priority high or low   , while we use as predictor features: The number of reviews in the cluster |reviews|. Also in this step CLAP makes use of the Random Forest machine learner with the aim of labelling each cluster as high or low priority  , where high priority indicates clusters CLAP recommends to be implemented in the next app release. These features include the similarity between a and b's name strings  , the relationship between the authoring order of a in p and the order of b in q  , the string similarity between the affiliations  , the similarity between emails  , the similarity between coauthors' names  , the similarity between titles of p and q  , and several other features. We use a Random Forest model trained on several features to disambiguate two authors a and b in two different papers p and q 28. From feature perspective  , the user profile features age  , income  , education level  , height  , weight  , location  , photo count  , etc. The core problem in developing an efficient disk-based index is to lay out the prefix tree on disk in such a fashion as to minimize the number of disk accesses required to navigate down the tree for a query  , and also to minimize the number of random disk seeks required for all index operations. Let us now consider how to implement the LSH Forest as a diskbased index for large data sets. A similar approach is suggested by Lafferty and Zhai 9Table 1shows an example relevance model estimated from some relevant documents for TREC ad-hoc topic 400 " amazon rain forest " . For every word in the vocabulary  , their relevance model gives the probability of observing the word if we first randomly select a document from the set of relevant documents  , and then pick a random word from it see Section 2.3 for a more formal account of this approach. The clusters of reviews belonging to the bug report and suggestion for new feature categories are prioritized with the aim of supporting release planning activities. For instance  , it is straightforward to show that as the number of trees increases asymptotically  , MLRF's predictions will converge to the expected value of the ensemble generated by randomly choosing all parameters and that the generalization error of MLRF is bounded above by a function of the correlation between trees and the average strength of the trees. Viterbi recognizer search. This means that hypotheses about specific entities must be considered in the e.g. References will usually denote entities contained in the discourse model  , which is updated after every utterance with entities introduced in that utterance. When optimizing the model the most likely path through the second level model is sought by the Viterbi approxima- tion 24 . The modeled eye movement features are described in Section 4.1. Even though we have described the tasks of content selection and surface realization separately  , in practice OCELOT selects and arranges words simultaneously when constructing a summary . We apply generic Viterbi search techniques to efficiently find a near-optimal summary 7. This approach is similar to the one described in  Second  , we tested a more sophisticated named entity recognizer NER based upon a regularized maximum entropy classifier with Viterbi decoding. The sequence of states is seen as a preliminary segmentation. A 3-state Viterbi decoder is first used to find the most likely sequence of states given a stream. The decoder can handle position-dependent  , cross-word triphones and lexicons with contextual pronunciations. It is a time-synchronous Viterbi decoder with dynamic expansion of LM state conditioned lexical trees 3  , 18  , 20  with acoustic and language model lookaheads. served as ranking criterion. σ  , the number of documents to which a cluster's score is distributed Equation 3: {5 ,10 ,20 ,30 ,40} ρ  , the number of rounds: 1–2  , Cluster-Audition; 1–5  , Viterbi Doc-Audition and Doc-Audition. To bootstrap this rst training stage  , an initial state-level segmentation was obtained by a Viterbi alignment using our last evaluation system. In the rst stage  , a context independent system was build. This is a typical decoding task  , and the Viterbi decoding technique can be used. Once the score s is found  , it possible to align each frame of the performance with the corresponding event in the score. where y* is the class label with the highest posterior probability under the model IJ  , or the most likely label sequence the Viterbi parse. Il;PyT IXi; IJ  , where yT is the most likely label of the token Xi a linelblock in the title page of a book in the instance x a book. We have improved the Viterbi-based splitting model feeding it with a dataset larger than the one used in 1. Hash tag splitting As we did in 1  , in addition to the words of the tweet  , we have used a hashtag splitter to split the compound words representing the hashtags in common English words. The Viterbi Doc-Audition scoring method is a straightforward procedure that ranks those documents with repertoires containing a highly-weighted pseudoquery above those that are top renderers only of lowerweighted ones. We begin by restricting our consideration of possible renderers to documents. 2 A Viterbi distribution emitting the probability of the sequence of words in a sentence. Each state has the following exponential family emission distributions: 1 A multinomial distribution emitting the relevance of the line  , r. This distribution is fixed; for each state one of the probabilities is one and the other is zero. We can also adjust the model parameters such as transition  , emission and initial probabilities to maximize the probability of an observable sequence. The Viterbi path contains seven states as the seventh state was generated by the sixth state and a transition to the seventh state. Modelling the speech signal could be approached through developing acoustic and language models. There are many approaches for doing this search  , the most common approach that is currently used is Viterbi beam search that searches for the best decoding hypothesis with the possibility to prune away the hypotheses with small scores. τ1  , the number of best renderers retrieved at the first iteration: {5} ∪ {10  , 20  , ..  , 100} ∪ {200  , 300  , 400  , 500}. Augmenting each word with its possible document positions  , we therefore have the input for the Viterbi program  , as shown below: For this 48-word sentence  , there are a total of 5.08 × 10 27 possible position sequences. Stemming can be performed before indexing  , although it is not used in this example. We have used the Google N-grams collection 6   , taking the frequency of words from the English One Million collection of Google books from years 1999 to 2009. That is  , the system produces a gist of a document d by searching over all candidates g to find that gist which maximizes the product of a content selection term and a surface realization term. In this step  , if any document sentence contributes only stop words for the summary  , the matching is cancelled since the stop words are more likely to be inserted by humans rather than coming from the original document. The Viterbi program assigns each word in the input sequence a position in the document  , as long as the word appears in the document at least once. Scores are assigned to each expansion by combining the backward score g  , computed by the translation model from the end to the current position of i  , and the forward score h computed by the Viterbi search from the initial to the current position of i. Otherwise  , all possible one-word expansions of it are computed. 4 to be 0.0019 and the optimum path of states for this observation sequence is {FD  , WQ  , WQ  , CS  , FD  , FD  , FD} with probability 1.59exp-5. The actual decoding of the speech utterance is based on searching the acoustic and language models to find out the best fitting hypothesis. To appl9 machine learning to this problem  , we need a large collection of gistcd web pages for training. As mentioned earlier  , the most successful technique has been to apply Viterbi-type search procedure  , and this is the strategy that OCELOT adopts. There are a number of possible criteria for the optimality of decoding  , the most widely used being Viterbi decoding. Decoding is the attempt to uncover the hidden part of the model  , and it can be used to align couples of sequences. The connection to VT should be clear: if one introduces the hidden variable I denoting the index of the model that generated the sequence Y as a non-emitting state then the procedure can be thought of as the partial Viterbi alignment of Y to the states where only the alignment w.r.t. In order to mitigate this effect  , we adopted an intermediate option in which each sequence is assigned to the model that is the most likely to generate it. Therefore  , every word is determined a most likely document tion. The intermediate output of the Viterbi program is shown as follows: arthur : 1 ,01 b : 1 ,11 sackler : 1 ,21 2 ,340.6 .. 12 ,20.5 .. : the : 0 ,210.0019 0 ,260.0027 .. 23 ,440.0014 internet : 0 ,270.0027 1 ,390.0016 .. 18 ,160.0014 unique : 0 ,280.0027 Choosing the sequence with the highest score  , we find the most likely position sequence. Path finding and sub-paths in breadth-first search 3. Neither pattern is a true depth-first or breadthfirst search pattern. The greedy pattern represents the depth-first behavior  , and the breadth-like pattern aims to capture the breadth-first search behaviors. To be efficient and scalable  , Frecpo prunes the futile branches and narrows the search space sharply. The depth-first search instead of the breadth-first search is used because many previous studies strongly suggest that a depth-first search with appropriate pseudo-projection techniques often achieves a better performance than a breadth-first search when mining large databases. Here we use breadth-first search. Once the search space is structured  , a search strategy should be chosen. 6  holds the objects during the breadth-first search. 4first out queue called Q in Fig. In practice  , forward selection procedures can be seen as a breadth-first search. 10 . In the mathematical literature  , breadth first search Is typically preferred. Normally the user cares "~. , ,:"~ ,~ton ~v'" ""-. and search the other subranges breadth-first. For our implementation we select for a solution path using a standard method such as breadth-first search. Thus solving the graph search problem in Given the user behavior observed by Klöckner et al. , we used two browsing patterns to evaluate find-similar.   , vn−1}  , where the indices are consistent with a breadth-first numbering produced by a breadth-first search starting at node v0 1 see Section 3.4.1 for a formal definition. By a depth-first search of the set enumeration tree of transitive reductions of partial orders  , Frecpo will not miss any frequent partial order. In enumerative strategies  , several states are successively inspected for the optimal solution e.g. , by breadth-first  , best-first or depth-first search. If its implementation is such that the least recent state is chosen  , then the search strategy is breadth-first. We first conduct a breadth-first or depth-first search on the graph. We will deal with these cycles in the next step. However  , best-first search also has some problems. Clearly  , best-first search has advantages over breadth-first search because it " probes " only in directions where relevant pages locate and avoids visiting irrelevant pages. We compute the discrete plan as a tree using the breadth first search. robot and obstacles 12. Tabels 1 and 2 show that the breadth first search is exhaustive it finds solutions with one step fewer re- grasps. the search procedure is breadth first search which examines all the nodes on one level of the tree before any nodes of the next level ignoring the goal distance Ac. This will not always be feasible in larger domains  , and intelligent search heuristics will be needed. This amounts to a breadth first search of the frequent itemsets on a lattice. Apriori first finds all frequent itemsets of size § before finding frequent itemsets of size § ¦ . CLOSET 11 and CLOSET+ 16 adopt a depth-first  , feature enumeration strategy. A-close 10 uses a breadth-first search to find FCPs. We restrict the training pages to the first k pages when traversing the website using breadth first search. Thus  , we should use these pages for training as well. A sample top-down search for a hypothetical hierarchy and query is given in Figure 2. In practice  , it is closer to a depth-first search with some backtracking than to a breadth-first search. This simple method worked out well in our experiments. ; the maximal number of states between the initial state and another state when traversing the TS in breadth-first search BFS height; the number of transitions starting from a state and ending in another state with a lower level when traversing the TS in breadth-first search Back lvl tr. deg. The resulting path will have the minimum nilinher of turns i n it by definition of breadth-first search. Compute D and perform a breadth-first search of D as indicated above starting with To as the set of visited vertices and ending when some vertex in the goal set 7~ ha5 been reached. During our previous experiments 13  , a bidirectional breadth first search proved to be the most efficient method in practice for finding all simple paths up to certain hop limit. The path iterator  , necessary for path pattern matching  , has been implemented as a hybrid of a bidirectional breadth-first search and a simulation of a deterministic finite automaton DFA created for a given path expression. The breadth-first search weighted by its distance from the reference keyframe is performed  , and the visited keyframes are registered in the temporary global coordinate system. To optimize the poses and landmarks  , we create a metric environment map by embedding metric information to nodes by breadth-first search over graph. The objects in UpdSeedD ,l are not directly density-reachable from each other. Thus  , a breadth-first search for the missing density-connections is performed which is more efficient than a depth-first search due to the following reasons: l Johnson generalized it to other surface representations  , including NURBS  , by using a breadth-first search 9. Quinlan introduced this approach using a depth-first search of the bounding hierarchy  141. Since coverage tends to increase with sequence length  , the DFS strategy likely finds a higher coverage sequence faster than the breadth-first search BFS. We choose to traverse the tree using depth-first search DFS. This is done by recursively firing co-author search tactics. is done by performing a breadth-first search that considers all successor vertices of a given vertex first before expanding further. Then  , we navigate in a breadth-first search manner through this classification. Note that this approach enables to consider ontologies more expressive than RDFS  , e.g. , OWL2DL. Two cases have to be distinguished. Starting from the two entities e 1 and e 2 the intersection tree is built using breadth-first search. Starting from a random public user  , we iteratively built a mutual graph of users in a Breadth First Search BFS manner. 2014. b Matched loop segments will be included in LBA as breadth-first search will active the keyframes. The fixed keyframes are selected based on a common landmark. Then we do breadth first search from the virtual node. To be more specific  , we add a virtual node which connects to all known nodes. The CWB searches for subject keywords through a breadth-first search of the tree structure. Subject keywords are nouns and proper nouns from a title or subtitle. It downloads multiple pages typically 500 in parallel. Its default download strategy is to perform a breadth-first search of the web  , with the following three modifications: 1. This is done by querying DBpedia's SPARQL endpoint for concepts that have a relation with the given concept. we perform a breadth first search. Therefore Lye have the following result. We generate plans that minimize worst-case length by breadth-first AND/OR search Akella  11. However there is no finite bound on the length of the plan. Then we compute the single source shortest path from y using breadth first search. In both cases a uniform random distribution is used. For each node visited do the following. Traverse the measure graph starting at m visiting all finer measures using breadth-first search. We augmented this base set of products  , reviews  , and reviewers via a breadth-first search crawling method. We call this the root dataset. For parts with different push functions  , a breadth-first search planner can be used to find a sensorless plan when one exists. We cannot recognize the parts hlowever. In a breadth-first search approach the arrangement enumeration tree is explored in a top-bottom manner  , i.e. This information  , however  , is not available in DFS. Each of the initial seed SteamIDs was pushed onto an Amazon Simple Queue Service SQS queue. The crawling was executed via a distributed breadth first search. Stopping criterion. We augment this base set of products  , reviews  , and reviewers via a breadth-first search crawling method to identify the expanded dataset. The SearchStrategy class hierarchy shown in Figure 6grasps the essence of enumerative strategies. Moreover  , breadth first search will find a shortest path  , whereas depth first makes no guarantees about the length of the counter example it will find. However  , if all violations go through a small set of nodes that are not encountered on the early selected paths or these nodes get stuck on the bottom of the worklist  , then it may be worse than breadth first search. In Section 3.6.1  , we show that breadthfirst search appears to be more efficient than depth-first search. We have chosen to search the LUB-tree hierarchy in a breadth-first manner  , as opposed to a depth-first search as in Quinlan 24 . If a crawl is started from a single seed  , then the order in which pages will be crawled tends to be similar to a breadth first search through the link graph 27 the crawl seldom follows pure breadth first order due to crawler requirements to obey politeness and robots restrictions . Discovery date. Although breadth-first search crawling seems to be a very natural crawling strategy  , not all of the crawlers we are familiar with employ it. Our results have practical implications to search engine companies. The experiments reported used a breadth first search till maximum depth 3 using the words falling in the synsets category. This affects the time spent in search for related candidates of a word not present in training data. We believe that crawling in breadthfirst search order provides the better tradeoff. On the other hand  , crawling in breadth-first search order provides a fairly good bias towards high quality pages without the computational cost. ×MUST generates the second smallest test suite containing the largest number of non-redundant tests and the smallest number of redundant tests Fig. The differences between all strategies breadth-first  , random search  , and Pex's default search strategy were negligible. When looking at search result behaviour more broadly we see that what browsing does occur occurs within the first page of results. This reduced breadth of access is further evidence for the goaldriven behaviour seen in search. If all words in a title or subtitle are search keywords  , too many subject keywords will be generated. We have introduced a set of effective pruning properties and a breadth-first search strategy  , StatApriori  , which implements them. In this paper  , we have shown that its is possible to search all statistically significant rules in a reasonable time. In the first iteration  , only the reported invocations are considered  , starting with the most suspicious one working down to the least. We assume that a breadth-first search is performed over these top ranked invocations. These strategies typically optimize properties such as " deeper paths " in depth-first search  , " less-traveled paths " 35  , " number of new instructions covered " in breadth-first search  , or " paths specified by the programmer " 39. Existing DSE tools alleviate path explosion using search strategies and heuristics that guide the search toward interesting paths while pruning the search space. We have confirmed this expectation by running the MAY × MUST configuration with different exploration strategies on 20 methods for which exploration bounds were reached. This Figure 4: Use of case inheritance search travels upwards in the hierarchy  , i.e. , towards the roots. When determining the cases allowed for a given frame  , a breadth-first search of the case frame hierarchy collects the relevant cases. DFS may take very long to execute if it does not traverse the search space in the right direction. Depth Firat Search DFS and Breadth First Scorch BFS are examples of this class. This list is used by the predictor to perform a breadth first search of the possible concepts representing the input text. That partial structure is added as the first entry to the queue of partial structures. We perform the pose graph optimization first  , to make all poses metric consistent. It is in fact a similar hybrid reasoning engine which is a combination of forward reasoning breadth-first and backward reasoning depth-first search. This continues until there are no more transitions to be fired. During prediction  , we explore multiple paths  , depending on the prediction of the MetaLabeler  , using either depth-first or breadth-first search. A content-based MetaLabeler was built at each node in the taxonomy. A second dimension entails elaborating on line 3. Surprisingly  , they did not find any significant variation in the way users examine search results on large and small displays. Similarly to 23 they adopted taxonomy of three examination strategies: Depth-First  , Mixed  , Breadth-First. Because the expansion is breadth first  , the optimal trajectory will he the first one encountered that meets the desired uncertainty. For the fixed-uncertainty minimum-time optimization the search tree is expanded until the desired uncertainty is reached. Since large main memory size is available in Gigabytes  , current MFI mining uses depth first search to improve performance to find long patterns. But MaxMiner uses a breadth-first approach to limit the number of passes over the database. Although breadth-first search does not differentiate Web pages of different quality or different topics  , some researchers argued that breadth-first search also could be used to build domain-specific collections as long as only pages at most a fixed number of links away from the starting URLs or starting domains are collected e.g. , 18  , 21. All URLs in the current level will be visited in the order they are discovered before URLs in the next level are visited. This method assumes that pages near the starting URLs have a high chance of being relevant. Since the path down the tree is controlled by the nodes that are popped from the heap  , the search is neither a true depth.first nor a true breadth·first search of the hierarchy. They conducted two experiments to determine whether users engaged in a more exhaustive " breadth-first " search meaning that users will look over a number of the results before clicking any  , or a " depth-first " search. One other study used eye-tracking in online search to assess the manner in which users evaluate search results 18. For instance  , SAGE 28  uses a generational-search strategy in combination with simple heuristics  , such as flip count limits and constraint subsumption. Sine~ each node consists of only 24 bytes and the top-down search is closer to a depth-first search than a breadth-first search  , the amount of space required by the hierarchy n·odes is not excessive. The hierarchy nodes may be accessed more than once  , so they must be stored in separate locations. In our experience of applying Pex on real-world code bases  , we identify that Pex cannot explore the entire program due to exponential path-exploration space. Which branching points are flipped next depends on the chosen search strategy  , such as depth-first search DFS or breadth-first search BFS. In particular  , Pex flips some branching points from previous runs to generate test inputs for covering new paths. It is written in Java and is highly configurable. That is  , starting from the root pages of the selected sites we followed links in a breadth-first search  , up to 3 ,000 pages per site. In our experiment  , we crawled 3 ,000 pages at each site. After both connections are made  , we find a path in the roadmap between the two connection points using breadth-first search. If we still can't connect both nodes to the same connected component of the roadmap  , then we declare failure. In this case  , only one DFA in conjunction with a standard breadth first search is used to grow a single frontier of entities. A similar solution is used for single source path patterns. We will denote this approximate Katz measure as aKatz throughout the rest of the paper. We execute breadth-first-search from s up to k levels without visiting t  , while keeping track of all paths formed so far. Any objects that are reached during the traversal are considered live and added to the tempLive set. We construct a work list starting at persist.root so we can perform a breadth-first search of the object graph. To propagate the constraints on join variable bindings Property 2  , we walk over this tree from root to the leaves and backwards in breadth-first-search manner. Next  , we embed a tree on Gjvar discarding any cyclic edges. This module contains multiple threads that work in parallel to download Web documents in a breadth-first search order. The Spider module is responsible for collecting documents from the Web. We observed that the similarity scores for the neighbours often is either very close to one  , or slightly above zero. The corresponding histogram is shown in Fig. In order to sample the distribution of distances between nodes  , breadth first search trees were formed from a fraction of the nodes. By following the path with the minimum cost  , the robot is guided to the nearest accessible unknown region. The breadth-first search is begun simultaneously at all these locations. See Figure 11for an example plan. Sensorless plans  , which must bring all possible initial orientations to the same goal orientation  , are generated using breadth-first search in the space of representative actions. Since the MFI cardinality is not too large MafiaPP has almost the time as Mafia for high supports. The breadth-first or level-wise search strategy used in MaxMiner is ideal for times better than Mafia. The former hierarchy is used to inherit cases  , the latter to compose synonym sets. The backward search can be illustrated in Figure 4by traversing the graphs in reverse in a breadth-first manner. For example  , assume that we want to check whether machine A can be in on in a stable state. This " 3 ,000 page window " was decided for practical reasons. The crawl was breadth-first and stopped after one million html pages had been fetched. The crawl occurred in January  , 2002 and was made to mimic the way a real search service of the .gov pages might make a crawl. The predictor pops the top structure off of the queue and tries to extend it using the substantiator. Our solution combines a data structure based on a partial lattice  , and memoization of intermediate solutions. The number of traversals is bounded by the total number of elements in the model and view at hand. Checking for missing connections is done by a breadth-first search of the connectors in the model. We used JPF's breadth-first search strategy  , as done for all systematic techniques in 28. We specified sequence length 10 this was greater than the length required to find all the Java errors from Figure 7. Recall that we must regenerate the paths between adjacent roadmap nodes since they are not stored with the roadmap. This task is efficiently performed by an optimized implementation of the Breadth-first search BFS strategy through MapReduce 3. In particular  , we index all the shortest paths starting from a source and ending with a sink. At running time we use the index to retrieve the paths whose sink node matches a keyword. An estimate of L was formed by averaging the paths in breadth first search trees over approximately 60 ,000 root nodes. From the 259 ,794 sites in the data set  , the leaf nodes were removed  , leaving 153 ,127 sites. This allowed us to perform bidirectional breadth first search to answer the connectivity question. For each URL present in the dataset  , the crawler saved the link structure following links both forward and backward for two hops. Even this crawl was very time consuming  , especially when the crawler came across highly linked pages with thousands of in-and out-links e.g. Link types extracted include straight HREF constructs  , area and image maps  , and Javascript constants. Links are explored from the starting page in breadth-first search using order of discovery for links at the same depth. Each term is mapped to a synset in WordNet and a breadth-first search along WordNet relations identifies related synsets. Word- Net is also used to expand terms with semantically similar concepts  , following an approach similar to 9. Interestingly  , we can perform sensorless orienting with shape uncertainty. We determine these paths by breadth-first search throughG. Given the initial and desired final configurations of the system  , the high level problem is how to get from the initial to the final equivalence region. Apriori does a breadth first search and determines the support of an itemset by explicit subset tests on the transactions . The main differences between Apriori and Eclat are how they traverse this tree and how they determine the counter values. Comparing the running times we observe that MaxMiner is the best method for this type of data. RBFS using h 0 = 0 behaves similarly to the breadth-first search. Heuristic function h 0 evaluates all nodes equally so it has no heuristic power and does not provide any guidance. Then  , starting from this seed set  , we use the following five strategies to select five different account sets with the same selection size of k from the dataset 5 : random search RAND  , breath-first search BFS  , depthfirst search DFS  , random combination of breadth-first and depth-first search RBDFS 6   , and CIA. In this experiment  , we start from the same seed set of N identified criminal accounts   , which are randomly selected from 2 ,060 identified criminal accounts. The existing thread has the additional topic node 413 which is about compression of inverted index for fast information retrieval. shows the result of the experiment after the second step of the breadth-first search. Each operation produces a temporary result which must be materialized and consumed by the next operation. A combination of these operators induces a breadth-first search traversal of the DBGraph. bring the two parts to distinguishable states. The crawl started from the Open Directory's 10 homepage and proceeded in a breadth-first manner. For the experiments in this paper  , our search engine indexed about 130 million pages  , crawled from the Web during March of 2004. Each of these subsets is identified using a breadth first search technique. In an object like a dimpled sphere such as a golf ball  , the concavity regions are disjoint sets of features. We choose the appropriate face vector field and cell vector field for the two cases as described in Section IV. The trajectory design problem is solved by performing a pyramid  , breadth-first search. In practice four to six iterations are sufficient to achieve a heading space resolution of less than one degree. The search then proceeds in a breadth-first fashion with a crawling that is not limited to URL domain or file size. The start point for the crawl is the home page of the target site. An efficient implementation can use a data structure like the tree shown in Figure 1to store the counters  Apriori does a breadth first search and determines the support of an itemset by explicit subset tests on the transactions . JPF is built around first  , breadth-first as well as heuristic search strategies to guide the model checker's search in cases where the stateexplosion problem is too severe 18. JPF is an explicit-state model checker that analyzes Java bytecode classes directly for deadlocks and assertion violations. Therefore  , to perform concolic testing we need to bound the number of iterations of testme if we perform depth-first search of the execution paths  , or we need to perform breadth-first search. Since the function testme runs in an infinite loop  , the number of distinct feasible execution paths is infinite. This simple but extremely flexible prioritization scheme includes as a special case the simpler strategies of breadth-first search i.e. , a queue and depth-first search i.e. , a stack. Simply by assigning a priority to each alternative   , the DBC can determine the order of evaluation of invocations  , achieving flexible evaluation order  , one of our major objectives. A similar strategy was used by the Exodus rule-generated optimizer GDS ? In both studies  , users were significantly more likely to engage in the depthfirst strategy  , clicking on a promising link before continuing to view other abstracts within the results set. Thus pipelined and setoriented strategies have similar complexity on a DBGraph. They may constitute part of more complex execution plans Thev89The temporal complexity of a depth-first search is OmaxCardX ,CardA while that of a breadth-first search is OCardA Gibb85 . On the other hand  , the depth-first search methods e.g. , PrefixSpan 14 and SPAM 1 grow long patterns from short ones by constructing projected databases. On one hand  , the breadth-first search methods e.g. , GSP 15 and SPADE 21 are based on the Apriori principle 5  and conduct level-by-level candidategeneration-and-tests . The search is guaranteed to halt since there are a finite number of equivalence classes and our search does not consider sequences with cycles. If a plan is found it is guaranteed to be the shortest because of the nature of breadth first search and if the search fails to find any solution then no solution exists for the part. Thus  , a breadth-first search for the missing density-connections is performed which is more efficient than a depth-first search due to the following reasons: l The main difference is that the candidates for further expansion are managed in a queue instead of a stack. The error plateaus at the final level of the bounding hierarchy because a lower bound cannot be extracted until the level finishes. Non-promising URLs are put to the back of the queue where they rarely get a chance to be visited. The number of possible choices of values of c and s that concolic testing would consider in each iteration is 17. Abstractly we view a program as a guarded-transition systems and analyze transition sequences. We used depth-first search DFS as the basis for PRSS in this paper; we plan to explore the use of variants of breadth-first search in future work. An enumerative search strategy is first characterized by the choice of the next state to apply an action on  , performed by the setNextState method  , which determines in which way the states are investigated. Let's consider how the FI-combine see Figure 2 routine works  , where the frequency of an extension is tested. Thirdly  , the vertical format is more versatile in supporting various search strategies  , including breadth-first  , depth-first or some other  , hybrid search. For each top ranked search result  , they performed a limited breadth first search and found that searching to a distance of 4 resulted in the best performance. Vassilvitskii and Brill 6 used distance on the web graph to perform a reranking of search results given that relevant documents link to other relevant documents. For example  , the mean number of nodes accessed in the top-down search of the complete link hierarchy for the INSPEC collection is 873 requiring only 20 ,952 bytes of core. As indicated above  , there are basically two ways in which the search tree can be traversed We can use either a breadth first search and explicit subset tests Apriori or a depth first search and intersections of transaction lists Eclat. Furthermore  , the number of small SubStNCtureS 1 to 4 atoms can be enormous  , so that even storing only the topmost levels of the tree can require a prohibitively large amount of memory. The MSN Search crawler discovers new pages using a roughly breadth-first exploration policy  , and uses various importance estimates to schedule recrawling of already-discovered pages. These pages were collected during August 2004  , and were drawn arbitrarily from the full MSN Search crawl. Therefore   , pages crawled using such a policy may not follow a uniform random distribution; the MSN Search crawler is biased towards well-connected  , important  , and " high-quality " pages. The second tool  , Meta Spider  , has similar functionalities as the CI Spider  , but instead of performing breadth-first search on a particular website  , connects to different search engines on the Internet and integrates the results. A sample user session with CI Spider is shown in Figure 1. We make use of relations such as synonym  , hypernym  , hyponym  , holonym and meronym and restrict the search depth to a maximum of two relations. Our results explain their finding by showing that relevant documents are found within a distance of 5 or are as likely to be found as non-relevant documents. In this graph  , subsequent actions are connected  , and TransferReceive / TransferSend actions are additionally connected to each other's subsequent actions. During this search  , we check that the newly introduced transfer does not induce a cycle of robots waiting for each other by performing breadth first search on the graph formed by the robot's plans. Through several recent independent evaluations 17  , 6  , it is now well accepted that a prefix tree-based data set representation typically outperforms both the horizontal and the vertical data set representations for support counting. The itemset search space traversal strategy that is used is depth-first 18  , breadth-first 2  , or based on the pattern-growth methodology 22. The existing methods essentially differ in the data structures used to " index " the database to facilitate fast enumeration. The search can be performed in a breadth-first or depth-first manner  , starting with more general shorter sequences and extending them towards more specific longer ones. We iterate through every possible insertion point for the new pickup or delivery point in s plan   , and choose the plan of lowest cost. A node in the tree contains the set of orientations consistent with the push-align operations along the path to the node. Search procedure: To find an orienting plan  , we perform a breadth-first search of an AND/OR tree lS . The documents retrieved by the web browsers of focused crawlers are validated before they are stored in a repository or database. Focused crawlers  , in contrast to breadth-first crawlers used by search engines  , typically use an informed-search strategy and try to retrieve only those parts of the Web relevant to some given topic 1  , 5  , 9  , 15 . The experiments described in this paper demonstrate that a crawler that downloads pages in breadth-first search order discovers the highest quality pages during the early stages of the crawl. For example  , the Internet Archive crawler described in 3  does not perform a breadthfirst search of the entire web; instead  , it picks 64 hosts at a time and crawls these hosts in parallel. If a winning path exists  , then the path represents the search schedule for the two pursuers. In order to find a winning path  , it suffices to build the graph G and to perform breadth-first search beginning at a start and ending at a goal vertex. To guide the search  , we work backward from a unique final orientation toward a range of orientations of size 27r  , which corresponds to the full range of uncertainty in initial part orientation. After the push function is used to partition the space of push directions into equivalence classes  , we perform a breadth-first search of push combinations to find a fence design. Its first phase is a " crawler " or " spider " that automatically searches part of the Web for caption candidates  , given a starting page and the number of trailing domain words establishing locality so " www.nps.navy.mil 2 " indicates all " navy.mil " sites. The breadth-first search implies that density-connections with the minimum number of objects requiring the minimum number of region queries are detected first. In a non-split situation  , we stop as soon as all members of UpdSeedDel are found to be density-connected to each other. The preponderance of diagonal path lines is due to the search being 8-connected  , and being breadth-first. The waypoints marked on the image indicate equally spaced one-hour time increments  , with the exception of the first interval  , which is a half hour. Sequence mining is essentially an enumeration problem over the sub-sequence partial order looking for those sequences that are frequent. All experiments in this section use the breadth-first search strategy. Our J-Sim experiments build the OU T data structure from Figure 4 and write it to a file only for the first version  , and load the information for unmodified transitions from the file to the IN data structure for each subsequent version. Text is provided for convenience. It is the sort of crawl which might be used by a real .gov search service: breadth first  , stopped after the first million html pages and including the extracted plain text of an additional 250 ,000 non-html pages doc  , pdf and ps. We note that for every fixed query a node assignment requiring no calls to updateP ath always exists: simply label the nodes in order discovered by running breadth-first search from s. However  , there is no universally optimal assignment — different queries yield different optimum assignments. This means that we can start emitting results right away when we retrieve the first result from the index. Focused crawling  , on the other hand  , attempts to order the URLs that have been discovered to do a " best first " crawl  , rather than the search engine's " breadth-first " crawl. " Rather  , any and all newly discovered links are placed onto the crawl frontier to be downloaded when their turn comes. Generalised search engines that seek to cover as much proportion of the web as possible usually implement a breadth-first BRFS or depth-first A. Rauber et al. The visiting strategy of new web pages usually characterises the purpose of the system. MaxMiner also first uses dynamic reordering which reorder the tail items in the increasing order of their supports. MaxMiner 3 uses a breadth-first search and performs look-ahead pruning which prunes a whole tree if the head and tail together is frequent. The data set representation that is used is horizontal 2  , vertical 35  , or based on a prefix tree 22. In effect we find the last fence first and work upstream  , like a salmon. l A split situation is in general the more expensive case because theparts of the cluster to be split actually have to be discovered. For each public user  , we first counted the number of protected mutual neighbours as well as the ratio of protected to all mutual neighbours. The arrangement enumeration tree is created as described above  , using the set of operands defined in Section 2 and it is traversed using either breadth-first or depth-first search. In general  , on level : 1 is created by joining the nodes in -with those in   , 2 for every node   , is defined and then linked to . The effect of search pruning at all Rtree levels is that  , starting from the top level  , the two nodes  , one from each R-tree  , are only traversed for join computation if the MBRs of their parent nodes overlap . In 3  , search pruning is done by synchronously traversing the two input R-trees depth-first whereas in BFRJ it is achieved by synchronized breadth-first traversal of both R-trees. Tuplesn tionally  , a depth first search explores one path deeply  , and thus may find a violation quickly if it serendipitously picks nodes that lead to some violation. Instead  , we can set parameters which we term the window's breadth and depth  , named analogously to breadth-first and depth-first search  , which control the number of toponyms in the window and the number of interpretations examined for each toponym in the window  , respectively. we consider all possible combinations of resolutions for these toponyms  , this results in about 3·10 17 possibilities  , an astonishingly large number for this relatively small portion of text  , which is far too many to check in a reasonable time. Otherwise  , the planner identifies the set of " boundary conditions " for the search  , namely:  The search for a sequence of regrasp operations proceeds by forward chaining from the set of initial gpg triples performing an evaluated breadth-first search in the space of compatible gpg triples. If there exists at least one non-empty intersection the pick-and-place operation can be performed with a single grasp corresponding to a gripper configuration of the non-empty intersection. Beam-search is a form of breadth-first search  , bounded both in width W and depth D. We use parameters D = 4 to find descriptions involving at most 4 conjunctions  , and W = 10 to use only the best 10 hypotheses for refinement in the next level. The quality of such rules is expressed with a confidence-intervalP with P = .95  , and the employed search strategy is beamsearchW  ,D. To seed our crawler  , we generated 100 ,000 random SteamIDs within the key space 64-bit identifiers with a common prefix that reduced the ID space to less than 10 9 possible IDs  , of which 6 ,445 matched configured profiles. In this graph  , vetexes and edges represent nodes and links respectively. Considering each mashup as a path  , we found that about 80% of 4100 existing mashup depth was no more than 3  , so we decided to make the depth level of the breadth-first-search be 3. A recent study of Twitter as a whole  , gathered by breadth-first search  , collected 1.47 billion edges in total 13. The accurate celebrity subgraph has a total of 835  , 117  , 954  , or about 835 million  , directed edges in it which is actually a non-negligible fraction of edges in Twitter's social graph. In this implementation the transitive closure of the digraph G T is based on a breadth first search through G T . This module computes the classification of an OWL 2 QL TBox T by adopting the technique described in Section 3. The sequence of retrieved documents displayed to the user is ordered by the number of edges from the entry point document. Additional documents are then retrieved by following the edges from the starting point in the order of a breadth first search. However  , after a large number of Web pages are fetched  , breadth-first search starts to lose its focus and introduces a lot of noise into the final collection. Instead of traversing the BVTT as a strictly depthfirst or breadth-first search JC98  , we use a priority queue to schedule which of the pending tests to perform next. Limiting the queue size limits the worst case storage requirements and performance of the al- gorithm. Since the planner performs breadth-first search in the space of representative actions  , the planner is complete if the computed action ranges are accurate. Given a nominal part shape with bounded shape uncertainty  , does the planner always return an orienting plan when one exists and indicate failure when no plan exists ? The initial collection was created for day 1 using a Breadth-First crawl that retrieved MAX IN INDEX = 100  , 000 pages from the Web starting from the bookmark URLs. We simulated 5 days of the search engine-crawler system at work. Using a 4000-node subgraph summarized in Table 3  , we generated 1633185 candidate edges. We developed an application  , ljclipper  , to restrict the overall friends graph to that induced by a subset of nodes of fixed number  , found using breadth-first search starting from a given seed. the largest subset of nodes such that any node within it can be reached from any other node following directed links  , contained 64 ,826 sites. When the FM is traversed using the breadth-first search BFS  , the edges in the FPN are generated according to relations between features in the FM and the weights on edges are computed  Lines 4∼5. Then E N i ,j  and W i ,j  are initialized Lines 2∼3. After the completion of breadth first search  , there are no unknown nodes and each node has a location area. Once we meet an unknown node  , we use its known neighbour nodes to compute its location area as described above and then turn it to a known node. In many cases  , simple crawlers follow a breadth-first search strategy  , starting from the root of a website homepage and traversing all URLs in the order in which they were found. Today  , Web Crawling is the standard method for retrieving and refreshing document collections 8 within WMSs as opposed to searching  , see 12. The rightmost thread contains the discussion in hypertext system in the late 80's such as hypertext system implementation Topic 166 and 224 and formal defintion of hypertext system using petrinet Topic 232. The CWB computes the similarity-degrees of the title and/or subtitles through a breadth-first search because the title and subtitles are within a nested structure. Searching for a similar title and/or similar subtitles in the compared Web site. As the crawl progresses  , the quality of the downloaded pages deteriorates. So  , instead of trying to find the optimal allocation we do the allocation by using the heuristic of traversing the tree in a breadth first-BF search order: l We have shown that finding an overall optimal allocation scheme for our cuboid tree is NP-hard DANR96 . Once it has been established that a high level path exists  , the lower level trajectory planning problem for each equivalence region node is to determine the trajectory which the cone must follow to reorient the part. Path planning for individual modules uses a breadth-first search starting at the end of the tail. At this point  , the chain is also moved to the tail  , starting at the extreme module e S of the slice and ending at the root lines 10–12. A candidate path is located when an entity from the forward frontier matches an entity from the reverse frontier. This enables to compute the representation of all concepts such that any pair of concepts sharing a common ancestor in the concept hierarchy will share a common prefix in their representation corresponding to this common ancestor. Once a goal state is reached we have a sequence of desired relative push angles which we know will uniquely reorient a part regardless of its initial orientation because that initial orientation must be in the range of The goal of the breadth first search then is to arrive at a current state p   , such that lpgl = 27r. We use the push function to find equivalence classes of actions-action ranges with the same effect. In our implementation  , we use breadth-first search in the space of representative actions to find the shortest sequence of fence rotations to orient the part. Our approtach to solve the regrasp problem is as follows: We generate and evaluate possible grasp classes of an object and its stable placements on a table; the regrasping problem is then solved by an evaluated breadth-first search in a space where we represent all compatible sequences of regrasp operations. Otherwise  , these constraints require that at least one regrasp operation must be performed. This procedure is then applied to all URLs extracted from newly downloaded pages. To address the issue of intolerance to false positives  , we consider only the top ten ranked method invocations reported in the diagnosis reports; the rest is ignored. In order to follow the edges in one direction in time  , we treat the edges between topic nodes as directed edges. To discover a topic evolution graph from a seed topic  , we apply a breadth-first search starting from the seed node but only following the edges that lead to topic nodes earlier in time. Our initial examination revealed that the allocated users IDs are very evenly distributed across the ID space. Capturing LCC Structure: To capture the connectivity structure of the Largest Connected Component LCC  , we use a few high-degree users as starting seeds and crawl the structure using a breadth-first search BFS strategy. Recently  , Microsoft Academic Search released their paper URLs and by crawling the first 7.58 million  , we have collected 2.2 million documents 4 . Third  , we import paper collections from other repositories such as arXiv and PubMed to incorporate papers from a breadth of disciplines. A lattice is defined over generated word sets for formulae  , and a breadth-first search starting from the query formula set is used to find similar formulae. convert operator trees to a bag of 'words' representing individual arguments and operator-argument triples 15. The graph pattern included in a SPARQL query is converted into a composition of such iterators  , according to a created query plan. That is  , for each node a set of SPARQL query patterns is generated following the rules depicted in Table 3w.r.t. This breadth-first search visits each node and generates several possible triple patterns based on the number of annotations and the POS-tag itself. The next step is to choose a set of cuboids that can be computed concurrently within the memory constraints . So  , instead of trying to find the optimal allocation we do the allocation by using the heuristic of traversing the tree in a breadth first-BF search order: l Clearly these computations can be done in time 0  m  once the minimum free radii have been calculated. We assign priority to the pending BVTT visits according to the distance: the closest pending BV pair is given a higher priority and visited next. The search is breadth-first and proceeds by popping a node from the head of OPEN list and generating the set of child nodes for the constituent states steps 1-4. If a node has a single state it is labeled solved. Search engines conduct breadth first scans of the site  , generating many requests in short duration. The Keynote robot can generate a request multiple times a minute  , 24 hours a day  , 7 days a week  , skewing the statistics about the number of sessions  , page hits  , and exit pages last page at each session. An estimate of the total number of edges by the present authors suggests there are around 7 billion edges in the present social graph. For each instance of the iterator created for a path pattern  , two DFAs are constructed. These pages contain 17 ,672 ,011 ,890 hyperlinks after eliminating duplicate hyperlinks embedded in the same web page  , which refer to a total of 2 ,897 ,671 ,002 URLs. Our web graph is based on a web crawl that was conducted in a breadth-first-search fashion  , and successfully retrieved 463 ,685 ,607 HTML pages. PD-Live does a breadth-first search from the document a user is currently looking at to select a candidate set of documents. Citation links and other similarity measures form a directed graph with documents as the nodes and similarity relationships as the edges. For clarity of exposition  , the database operations introduced in Section 3 have been described in a setoriented way  , independent of their integration in a query execution plan. Specifically   , we collected the previous Amazon reviews of each reviewer in the root dataset and the Amazon product pages those reviews were associated with. The topological map stores only relative information in edges while the metric map contains location of nodes with respect to the specified origin. The sensor-based planner performs breadth-first AND/OR search to generate sensor-based orienting plans for parts with shape uncertainty. Given a nominal part shape  , radius values of the center of mass and vertex uncertainty circles  , and maximum sensor noise  , they return a plan when they can find one and indicate failure otherwise. The sensorless planner uses breadth-first search to find sensorless orienting plans. For the parts in Figure 14  , going from top to bottom  , left to right  , the sensor-based planner took an average of 0.192 secs  , 1.870 secs  , 0.756 secs  , 0.262 secs  , 0.262 secs  , 0.224 secs  , and 0.188 secs respectively on a SPARC ELC. Using the enumeration tree as shown in Figure 2  , we can describe recent approaches to the problem of mining MFI. A simple breadth-first search is quite effective in discovering the topic evolution graphs for a seed topic Figure 4and Figure 5a. In this subsection  , rather than focusing on finding the single best parameter values  , we explore the parameter space and present multiple examples of graphs obtained with varying parameter values. show informative evolutionary structure  , carrying concrete information about the corpus that are sometimes previously unknown to us. We initially clone the live object set to know what it was set to before we begin walking the object graph. At every jvar-node  , we take intersection of bindings generated by its adjacent tp-nodes and after the intersection  , drop the triples from tp-node Bit- Mats as a result of the dropped bindings. They found that crawling in a breadth-first search order tends to discover high-quality pages early on in the crawl  , which was applied when the authors downloaded the experimental data set. 20 studied different crawling strategies and their impact on page quality. OVERLAP does the allocation using a heuristic of traversing the search tree in a breadth-first order  , giving priority to cuboids with smaller partition sizes  , and cuboids with longer attribute lists. For other cuboids  , only a single page of memory can be allocated -these cuboids are said to be in the " SortRun " state. The entry point can be directly provided by the user by selecting a document icon  , or determined by the system as the document that best matches the query. If the action ranges are overly conservative  , the planner may not find a solution even when one exists. Our evaluation is based on two data sets: a large web graph and a substantial set of queries with associated results  , some of which were labeled by human judges. Unlike pure hill-climbing  , MPA in DAFFODIL uses a node list as in breadth-first search to allow backtracking  , such that the method is able to record alternative  " secondary " etc. As desired by the user the list can be reduced to terminal authors. To detect deadlocks or paths to be folded we scan graph C with the BFS Breadth-First-Search algo­ rithm. and Next to the folding we introduce operations that re­ move from the systerl1 the vehicles that can visit all the vertices of their mission vectors. Unlike the simple crawlers behind most general search engines which collect any reachable Web pages in breadth-first order  , focused crawlers try to " predict " whether or not a target URL is pointing to a relevant and high-quality Web page before actually fetching the page. Focused crawlers are programs designed to selectively retrieve Web pages relevant to a specific domain for the use of domainspecific search engines and digital libraries. In addition  , focused crawlers visit URLs in an optimal order such that URLs pointing to relevant and high-quality Web pages are visited first  , and URLs that point to low-quality or irrelevant pages are never visited. Since the position bias can be easily incorporated into click models with the depth-first assumption  , most existing click models 4  , 11  , 13 follow this assumption and assume that the user examines search results in a top-to-bottom fashion. The breadth-first strategy  , however  , draws a different picture: a user will look ahead at a series of results before clicking on the favorite results among them. The subgraph returned by BFS usually contains less vertices in the target community than the subgraph of the same size obtained by random walk technique. It is worthwhile noting that other expansion methods such as breadth-first-search BFS would entirely ignore the bottleneck defining the community and rapidly mix with the entire graph before a significant fraction of vertices in the community have been reached. If the similarity-degree of a title and/or subtitles is higher than the threshold ­  , the title and/or subtitles are regarded a similar title and/or similar subtitles  , and the contents of the title and subtitles are considered similar contents. The use of the succ-tup and succ-val* primitives defines a traversal of the DBGraph following a breadth-first search EFS strategy Sedg84  , as follows : The transitive closure operation is performed by simply traversing links  , Furthermore testing the termination condition is greatly simplified by marking. The resulting operation  , called SIKC val*v ,R.k  , delivers and marks all non marked tuple ve&es connected to the value v by one edge valued by R.k. One possible source of this difference is that the crawling policies that gave rise to each data set were very different; the DS2 crawl considered page quality as an important factor in which pages to select; the DS1 crawl was a simpler breadth-first-search crawl with politeness. The DS1 and DS2 curves differ significantly: DS2 contains about twice as many documents which contain no popular shingles at all. If the target community exists for the seed set  , then according to 6  , this target community would serve as a bottleneck for the probability to be spread out. Before searching for a regrasp sequence  , the regrasp planner checks if the pick-and-place operation can be achieved within a single grasp. The division of the planning into ofRine and online computation with as much a priori knowledge as possible used for the offline computation turns out to be an efficient and powerful concept  , operating online in connection with the evaluated breadth-first search in the space of compatible regrasp operations. Two gpg triples Gi  ,  ,Pj  ,  ,Gkl sumes less than 5.0 sec CPU time on a SPARC station 5. The unions D:=DuAD and AD':=AD'usucc~val*v'  , R.1 can be efficiently implemented by a concatenation since marking the tuples avoid duplicate generation. To capture the full semantics of an input question  , HAWK traverses the predicated-argument tree in a pre-order walk to reflect the empirical observation that i related information are situated close to each other in the tree and ii information are more restrictive from left to right. Starting from this seed set  , we performed a breadth-first crawl traversing friendship links aiming to discover the largest connected component of the social graph. To initiate the crawl  , we used the search facilities on PornHub to retrieve all users from the 60 largest cities within and the 48 largest cities outside of the USA based on population  , giving us a seed set of 102k users. This figure suggests that breadth-first search crawling is fairly immune to the type of self-endorsement described above: although the size of the graph induced by the full crawl is about 60% larger than the graph induced by the 28 day crawl  , the longer crawl replaced only about 25% of the " hot " pages discovered during the first 28 days  , irrespective of the size of the " hot " set. The overlap continues in the 60- 80% range through the extent of the entire 28 day data set. the node that has the shortest average path to all the other nodes in Λ pred and to perform a breadth-first-search from this node in G pred subgraph of G containing only the nodes in Λ pred and their interconnects to create a tree of information spread and to use the leaves of that tree as the newly activated nodes. The first  , rather naive approach we implemented to predict Ξ pred was to select the most central node in set Λ pred ; i.e. Some connectivity-based metrics  , such as Kleinberg's al- gorithm 8  , consider only remote links  , that is  , links between pages on different hosts. In addition to the standard language features of Java  , JPF uses a special class Verify that allows users to annotate their programs so as to 1 express non-deterministic choice with methods Verify.randomn and Verify.randomBool  , 2 truncate the search of the state-space with method Verify.ignoreIfcondition when the condition becomes true  , and 3 indicate the start and end of a block of code that the model checker should treat as one atomic statement and not interleave its execution with any other threads with methods Verify.beginAtomic and Verify.endAtomic. Our experiments revealed that the influentials identified using this method have poor performance which led us to identify the next method of prediction. If the edges of a lockdown graph are weighted by the number of images constituting the part of the segment between the two lockdown points or more appropriately  , the sub-nodes on which the two lockdown points lie  , choosing the smallest-sized cycle basis will reduce computational cost in computing HHT to a small extent. In our work we use a simple breadth-first-search routine  , modified along the suggestions in 3  , to find a cycle basis for graphs that are allowed to have multiple self-edges and multiple edges between vertices. BSBM supposes a realistic web application where the users can browse products and reviews. The Berlin SPARQL Benchmark BSBM is built like that 5. BSBM generates a query mix based on 12 queries template and 40 predicates. We randomly generated 100 different query mix of the " explore " use-case of BSBM. We used Berlin SPARQL Benchmark BSBM 5 as in 16 with two datasets: 1M and 10M. Each dataset has its own community of 50 clients running BSBM queries. We run an experimentation with 2 different BSBM datasets of 1M  , hosted on the same LDF server with 2 differents URLs. On the BSBM dataset  , the performance of all systems is comparable for small dataset sizes  , but RW-TR scales better to large dataset sizes  , for the largest BSBM dataset it is on average up to 10 times faster than Sesame and up to 25 times faster than Virtuoso. The two diagrams in Figure 5show how the performance changes  , when the LUBM and BSBM queries are executed on increasingly large datasets. This behavior promotes the local cache. The flow of BSBM queries simulates a real user interacting with a web application. Two synthetic datasets generated using RDF benchmark generators BSBM 2 and SP2B 3 were used for scalability evaluation. Datasets. We extend the BSBM by trust assessments. The generated data is created as a set of named graphs 11. The BSBM executes a mix of 12 SPARQL queries over generated sets of RDF data; the datasets are scalable to different sizes based on a scaling factor. For our tests we use an extended version of the Berlin SPARQL Benchmark BSBM 10. The queries are in line with the BSBM mix of SPARQL queries and with the BSBM e-commerce use case that considers products as well as offers and reviews for these products. Furthermore   , we developed a mix of six tSPARQL queries. Due to space limitations   , we do not present our queries in detail; we refer the reader to the tSPARQL specification instead. The sp2b uses bibliographic data from dblp 12 as its test data set  , while the bsbm benchmark considers eCommerce as its subject area. The sp2b sparql performance benchmark 17  and the Berlin sparql Benchmark bsbm 3 both aim to test the sparql query engines of rdf triple stores. This is normal because the cache has a limited size and the temporal locality of the cache reduce its utility. As we can see  , the calls to the local cache depends considerably on the size of the data  , the percentage of hit-rate is 47 % in the case of BSBM with 1M  , and it decreased to 11 % for BSBM with 10M. We note that BSBM datasets consist of a large number of star substructures with depth of 1 and the schema graph is small with 10 nodes and 8 edges resulting in low connectivity. However  , for BSBM dataset  , DFSS outperforms ITRMS for both scalability experiments see Figure 4c and Figure 5a. We compare the native SQL queries N  , which are specified in the BSBM benchmark with the ones resulting from the translation of SPARQL queries generated by Morph. Out of the 12 BSBM queries  , we focus on all of the 10 SELECT queries that is  , we leave out DESCRIBE query Q09 and CONSTRUCT query Q12. 5 BSBM is currently focused on SPARQL queries  , therefore we plan to develop a set of representative SPARQL/Update operations to cover all features of our approach. Furthermore  , we will evaluate the performance and expressiveness of our approach with the Berlin SPARQL Benchmark BSBM. We found that for the BSBM dataset/queries the average execution time stays approximately the same  , while the geometric mean slightly increases. garbage collections. The Berlin SPARQL Benchmark 17 BSBM also generates fulltext content and person names. In the area of RDF stores  , a number of benchmarks are available. Figure 6 shows the results of these evaluations. For this  , we measured the performance on large BSBM and LUBM data sets while varying the number of nodes used. For more details of the evaluation framework please refer to 15 ,16. We use an evaluation framework that extends BSBM 2 to set up the experiment environment. Therefore  , 5 entries in the profile is sometimes not enough to compute a good similarity. The query mix of BSBM use often 16 predicates. The experimental results in Table 5show that exploiting the emergent relational schema even in this very preliminary implementation already improves the performance of Virtuoso on a number of BSBM Explore queries by up to a factor of 5.8 Q3  , Hot run. We compare a classic Virtuoso RDF quad table Virt-Quad and this CS-based implementation Virt-CS on the BSBM benchmark at 10 billion triples scale. The geometric mean does not change dramatically  , because most queries do not touch more data on a larger dataset. Finally  , we present our conclusions and future work in Section 5. In Section 4 we describe our evaluation using the BSBM synthetic benchmark  , and three positive experiences of applying our approach in real case projects. We also take into account that resources of BSBM data fall into different classes. For a given resource  , we use this generator to decide the number of owl:sameAs statements that link this resource with other randomly chosen resources. We generate about 70 million triples using the BSBM generator  , and 0.18 million owl:sameAs statements following the aforementioned method. In the following sections we will provide details of LHD-d  , and evaluate it afterwards in the above environment. Our extension  , available from the project website  , reads the named graphs-based datasets  , generates a consumer-specific trust value for each named graph  , and creates an assessments graph. Figure 6shows the distribution of queries over clients. As in the previous experimentation  , we run a new experimentation with 2 different BSBM datasets of 1M hosted on the same LDF server with 2 different URLs. The size of table productfeatureproduct is significantly bigger than the table product 280K rows vs 5M rows. BSBM SQL 5 is a join of four tables product  , product   , productfeatureproduct  , and productfeatureproduct . For the LUBM dataset/queries the geometric mean stays approximately the same  , whilst the average execution time decreases. In this section we further study the distribution of co-reference in Linked Data to set up an environment in which LHD-d is evaluated. We generate co-reference for each class separately to make sure that resources are only equivalent to those of the same class. In Section 3 we formalise our extension to consider R2RML mappings. Figure 4bshows that the number of calls answered by caches are proportional with the size of the cache. We used the following parameters: BSBM 10M  , 10 LDF clients  , and RP S view = 4 and CON view = 9. Query Load. Two set of queries are used to perform two tasks: building a type summary and calculating some bibliometrics-based summary. We experimented with BSBM 4 and SP2B 29 datasets  , varying the sizes of data.  BSBM SQL 4 contains a join between two tables product and producttypeproduct and three subqueries  , two of them are used as OR operators. However  , in some queries the translation results show significant differences  , such as in Q04 and Q05. As we can see  , ≈40 % of calls are handled by the local cache  , regardless the number of clients. The SP 2 Bench and BSBM were not considered for our RDF fulltext benchmark simply due to the fact of their very recent publication. Both benchmarks pick terms from dictionaries with uniform distribution. The BSBM benchmark 1 is built around an e-commerce use case  , and its data generator supports the creation of arbitrarily large datasets using the number of products as scale factor. courses  , students  , professors are generated. Although not included here  , we also evaluated those queries using D2R 0.8.1 with the –fast option enabled. The measured total time for a run includes everything from query optimization until the result set is fully traversed  , but the decoding of the results is not forced. For BSBM we executed the same ten generated queries from each category  , computed the category average and reported the average and geometric mean over all categories. During query execution the engine determines trust values with the simple  , provenance-based trust function introduced before. To measure how determining trust values may impact query execution times we use our tSPARQL query engine with a disabled trust value cache to execute the extended BSBM. Enriching these benchmarks with real world fulltext content and fulltext queries is very much in our favor. Additionally  , a subset of the realworld data collection Biocyc 1 that consists of 1763 databases describing the genome and metabolic pathways of a single organism was used. Most surprisingly  , the RDFa data that dominates WebDataCommons and even DBpedia is more than 90% regular. We see that synthetic RDF benchmark data BSBM  , SP2B  , LUBM is fully relational  , and also all dataset with non- RDF roots PubMed  , MusicBrainz  , EuroStat get > 99% coverage. For this setting  , the chart in Figure 9b depicts the average times to execute the BSBM query mix; furthermore  , the chart puts the measures in relation to the times obtained for our engine with a trust value cache in the previous experiment. The situation changes for a local cache with 10 ,000 entries  , in this case  , the hit-rate of local cache is 59 % and 28 % for behavioral cache  , only 13 % of calls are forwarded to the server. The BSBM SPARQL queries are designed in such a way that they contain different types of queries and operators  , including SELECT/CONTRUCT/DESCRIBE  , OPTIONAL  , UNION. We have run all queries with 20 times with different parameters  , in warm mode run. The resulting sets of queries together with query plans generated by PostgreSQL9.1.9  , and the resulting query evaluation time are available at http://bit.ly/15XSdDM. To understand this behaviour better  , we analyzed the query plans generated by the RDBMS. We can observe that all translation types native  , C  , SQE  , SJE  , SQE+SJE have similar performance in most of BSBM queries  , ranging from 0.67 to 2.60 when normalized  ing to the native SQL queries. We executed ten runs of each LUBM query and in the diagrams report both the average and geometric mean over the fastest runs. The Social Intelligence BenchMark SIB 11  is an RDF benchmark that introduces the S3G2 Scalable Structure-correlated Social Graph Generator for generating social graphs that contain certain structural correlations. In the same spirit  , the corresponding SQL queries also consider various properties such as low selectivity  , high selectivity  , inner join  , left outer join  , and union among many others. All the triples including the owl:sameAs statements are distributed over 20 SPARQL endpoints which are deployed on 10 remote virtual machines having 2GB memory each. To measure the impact of this extension on query execution times we compare the results of executing our extended version of the BSBM with ARQ and with our tSPARQL query engine. As presented in Section 4.2 tSPARQL redefines the algebra of SPARQL in order to consider trust values during query execution. As the chart illustrates  , determing trust values during query execution dominates the query execution time. The data generator is able to generate datasets with different sizes containing entities normally involved in the domain e.g. , products  , vendors  , offers  , reviews  , etc. The BSBM benchmark 5  focuses on the e-commerce domain and provides a data generation tool and a set of twelve SPARQL queries together with their corresponding SQL queries generated by hand. To eliminate the effects of determining trust values in our engine we precompute the trust values for all triples in the queried dataset and store them in a cache. Both benchmarks allow for the creation of arbitrary sized data sets  , although the number of attributes for any given class is lower than the numbers found in the ssa. While the BSBM benchmark is considered as a standard way of evaluating RDB2RDF approaches  , given the fact that it is very comprehensive  , we were also interested in analysing real-world queries from projects that we had access to  , and where there were issues with respect to the performance of the SPARQL to SQL query rewriting approach. All the resulting queries together with their query plans are also available at http://bit.ly/15XSdDM. Nevertheless  , this approach is clearly not scalable e.g. , in Q07 and Q08 the system returned an error while performing the operations  , while the native and the translation queries could be evaluated over the database system. In all the cases  , we compare the queries generated by D2R Server with –fast enabled with the queries generated by Morph with subquery and self-join elimination enabled. For questions with a simple answer pattern  , the answer candidates can be found by fixed pattern matching. Our pattern matching component consists of two parts  , fixed pattern matching and partial pattern matching. As for those with complex answer patterns  , we try to locate answer candidates via partial pattern matching. We now define the graph pattern matching problem in a distributed setting. Distributed graph pattern matching. Patterns are organized in a list according to their scores. Fixed pattern matching scans each passage and does pattern matching. One promising technique to circumvent this is soft pattern matching. We conjecture that current pattern matching applications may be hindered due to the rigidity of hard matching. Pattern matching is simple to manipulate results and implement. Instead of building a classifier we use pattern matching methods to find corresponding slot values for entities. Consequently  , we believe that any practical IE optimizer must optimize pattern matching. The work 6 describes other large-scale pattern matching examples. Next  , each model's location is estimated. When an eye image is input  , the pattern matching is carried out with the pattern matching model  , memorized previously. A Basic Graph Pattern is a set of statement patterns. Definition 15 Basic Graph Pattern Matching. Graph pattern matching Consider the graph pattern P from Fig. Example 2. The Pattern Matching stream consists of three stages: Generation  , Document Prefetch and Matching. In most applications  , however  , substring pattern matching was applied  , in which an " occurrence " is when the pattern symbols occur contiguously in the text. Pattern matching has been used in a number of applications . However  , their pattern languages are limited by a small number of pattern variables for matching linguistic structures. SCRUPLE 10 and JaTS 5  provide pattern matching facilities that can interpret source-code like patterns. Matching is meant here as deciding whether either a given ontology or its part is compliant matches with a given pattern. Introducing a pattern language opens another interesting direction: pattern matching and induction. Kumar and Spafford 10 applied subsequence pattern matching to intrusion detection. If no matching pattern is found  , the exception propagates up the call stack until a matching handler is found. For the first matching pattern  , the exception handler of that catch block is executed. Surface text pattern matching has been applied in some previous TREC QA systems. The answer extraction methods adopted here are surface text pattern matching  , n-gram proximity search and syntactic dependency matching . Feature matching method needs to abstract features e.g. Two kinds of matching methods are oftcn uscd: Feature matching method and pattern matching method 8. Recognizing the oosperm and the micro tube is virtually a matching problem. This is the value used for pattern matching evaluation. 9 Let us examine a small pattern-matching example . Example. This package provides reawnably fast pattc:rn matching over a rich pattern language. To effect the pattern matching it.self  , finite automata techniques l such as the UNIX regec package can be used. The final score of a sentence incorporates both its centroid based weight and the soft pattern matching weight. We compute each input sentence's pattern matching weight by using Equation 6. But in our case  , pattern matching occurs relatively less frequently than during a batch transformation. The restriction of axes in XSLT has been introduced for performance reasons and the goal was to allow efficient pattern matching. The output of this pattern matching phase is tuples of labels for relevant nodes  , which is considered as intermediate result set  , named as RS intermediate . As mentioned previously  , we adopt VERT for pattern matching. Note that these early work however do not consider AD relationship  , which is common for XML queries. Bottom-up tree pattern matching has been extensively studied in the area of classic tree pattern matching 12. The correlation operation can be seen as a form of convolution where the pattern matching model Mx ,y is analogous to the convolution kernel: Normalized grayscale correlation is a widely used method in industry for pattern matching applications. Once a matching sentiment pattern is found  , the target and sentiment assignment are determined as defined in the sentiment pattern. SA first identifies the T-expression  , and tries to find matching sentiment patterns. var is a set of special alternative words  , which are usually shared by various patterns and also assigned in question pattern matching. A question chunk  , expected by certain slots  , is assigned in question pattern matching. Let E k 1 ≤ k ≤ m denote the kth named entity in the annotated passage  , T i denotes the ith query keyword Different from previous empirical work  , we show how soft pattern matching is achieved within the framework of two standard probabilistic models. In this paper  , we build upon the earlier work in soft pattern matching. Each pattern matching step either involves the use of regular expressions or an external dictionary such as a dictionary of person names or product names. These navigational features are then fed into a sequence of pattern matching steps. For the first variation the text collection was the Web  , and for the second  , the local AQUAINT corpus. Two variations of this stream were implemented  , Web Pattern Matching and Collection Pattern Matching. However  , they do not maintain the hierarchical structure of a single stack since Lemma 1 does not hold for graph data. In addition  , not all types of NE can be captured by pattern matching effectively. That is exactly the rational behind our hybrid approach to IE combining pattern matching rules and statistical learning Srihari 1998 The triple pattern matching operator transforms a logical RDF stream into a logical data stream  , i.e. Therefore  , the triple pattern matching operator must be placed in a plan before any of the following operators. By incorporating 'anchor control' logic it is possible to operate some sub-sets of cascades in the unanchored mode  , sub-pattern matching mode  , variable precursor matching mode or a combination thereof. The techniques of unanchored mode operation  , sub-pattern matching   , 'don't care' symbols  , variable precursor position anchoring and selective anchoring as described for a single cascade can be extended to this twodimensional pattern matching device. If a text segment matches with a pattern  , then the text segment is identified to contain the relationship associated with the pattern. A pattern matching program was developed to identify the segments of the text that match with each pattern. Each pattern box provides visual handles for direct manipulation of the pattern. Using the generated pattern as a starting point  , the developer interactively modifies the pattern by inserting wildcards and matching constraints. This eases parsing  , pattern declaration and matching  , and it makes the composition interface explicit. We explicitly declare the pattern type i.e. , the associated nonterminal of the pattern root and of the variable symbols in σΓ in the pattern specification. This is a problem that has received some attention from the pattern matching research community. 4 also propose to find relevant formulae using pattern matching. Kamali et.al. pressive language. Particularly complex operations on software graphs are pattern matching and transitive closure computation. The patterns are described in Table 2. 8is to recognize a parameter by pattern matching. In our simplified version of pattern matching  , the search trajectory was designed as follows. The averagc Previously examined by Cui et al. Function recParam in Fig. The tree-pattern matching proceeds in two phases. As a result  , it may have false positives. proposed a similar method to inverse pattern matching that included wild cards 9. Lee et al. A type constraint annotation restricts the static Java type of the matching expression. More generally  , pattern annotations control the scope of the pattern match. In the Generation stage  , the question is analyzed and possible answer patterns are generated. There are several main differences between string matching and the discovery of FA patterns. Note that in this paper  , we focus on ordered twig pattern matching. Both '/' and '//' in the pattern are treated as regular tree edges. Patterns are sorted by question types and stored in pattern files. Only patterns with score greater than some empirically determined threshold are applied in pattern matching. For example  , consider the tree representation of the pattern Q 1 in Figure 3 . Overlapping features: Overlapping features of adjacent terms are extracted. String pattern matching and domain knowledge are used for features of formula pattern. Note that it contains variables that have already been bound by the change pattern matching. Next  , we consider the graph pattern in the first loop. The final score is the product of the pattern score and matching score. The matching score is calculated according to how well the semantic features are matched. The lookup-driven entity extraction problem reduces to the well studied multi-pattern matching problem in the string matching literature 25. During In our scenario  , if each entity is modeled as a pattern  , the lookup-driven entity extraction problem reduces to the multi-pattern matching problem. The goal of multi-pattern matching is to find within a text string d all occurrences of patterns from a given set. Although surface text pattern matching has been applied in some previous TREC QA systems  , the patterns used in ILQUA are better since they are automatically generated by a supervised learning system and represented in a format of regular expressions which contain multiple question terms. The answer extraction methods adopted here are surface text pattern matching  , n-gram proximity search  , and syntactic dependency matching . All of the points have the same pattern and this is suitable for a template matching because the points may be able to be extracted through a template matching procedure using only one template. And a chess board pattern is adopted as a calibration pattern because it is full of intersections of lines and supplies the enormous points in one image. In such a case  , we first need to distribute the expression " GRAPH γ " appropriately to atomic triple patterns in order to prescribe atomic SPARQL expressions accessible by basic quadruple pattern matching. Basic quadruple pattern matching is not directly applicable  , if an expression " GRAPH γ " appears outside a complex triple pattern . The pattern-matching language is based on regular expressions over the annotations; when a sequence of annotations is matched by the left-hand side pattern  , then the right-hand side defines the type of annotation to be added Organization in the example case above. Rule writing requires some knowledge of the JAPE pattern-matching lan- guage 11 and ANNIE annotations. In addition to surface pattern matching  , we also adopt n-gram proximity search and syntactic dependency matching. The Sparkwave 10 system was built to perform continuous pattern matching over RDF streams by supporting expressive pattern definitions  , sliding windows and schema-entailed knowledge. Finally  , OPS examined the matching trees that emerged from the graph traversal to determine the matching subscriptions. Pattern matching with variable 'don't care' symbols can now be easily performed  , if the input signals set the D flip-flop values throughout the duration of pattern matching. If the 'don't care' operation is to be externally controlled  , a cascade of'don't care' flip-flops D and F as shown in Figure 19.5  , similar to the anchor flip-flops  , has to be set prior to the beginning of the pattern matching operation. Our system focuses on ordered twig pattern matching  , which is essential for applications where the nodes in a twig pattern follow the document order in XML. Rather the twig pattern is matched as a whole due to sequence transformation. We enhanced the pattern recognition engine in ViPER to execute concurrent parallel pattern matching threads in spite of running Atheris for each pattern serially. It is therefore necessary to annotate all patterns before sending the page to the client. We tested our technique using the data sets obtained from the University of New Mexico. Finally  , a novel pattern matching module is proposed to detect intrusions based on both intra-pattern and inter-pattern anomalies. This approach benefits from a better performance by avoiding multiple input parsing. For each token  , we look for the longest pattern of token features that matches with pattern rules. -Named Entity analyzer uses language specific context-sensitive rules based on word features recognition pattern matching. Such overlap relationship characterizes the normal behavior of the application. Therefore  , the system works in stages: it ranks all sentences using centroid-based ranking and soft pattern matching  , and takes the top ranked sentences as candidate definition sentences. The purpose of using such hard matching patterns in addition to soft matching patterns is to capture those well-formed definition sentences that are missed due to the imposed cut-off of ranking scores by soft pattern matching and centroid-based weighting. used ordered pattern matching over treebanks for question answering systems 15. A recent paper by Müller et al. their rapid evaluation. Proposals for pattern-matching operators are of little use unless indices can be defined to permit . Yet ShopBot has several limitations. ShopBot relies on a combination of heuristic search  , pattern matching  , and inductive learning techniques. The interesting subtlety is that pattern matching can introduce aliases for existing distinguishing values. These rules handle match statements. Approaches that use pattern matching e.g. Furthermore  , accuracy can usually be varied at the cost of recall. TwigStack 7  , attract lots of research attention. That also explains why many twig pattern matching techniques  , e.g. The research question is: pattern. Determining the changes between two versions enables matching of their code elements. 18 have demonstrated that soft pattern matching greatly improves recall in an IE system. Xiao et al. We have so far introduced features of the matching rule language mainly through examples. The pattern symbols are: Regarding input data generation  , all sequences  , matching the pattern are favored and get higher chance to occur. Results of query " graph pattern " with terms-based matching and different rankings: 1 Semantic richness  , 2 Recency. The *SENTENCE* operator reduces the scope of the pattern matching to a single sentence. We allow seoping using two functions. with grouping  , existing pattern matching techniques are no longer effective. This query can be expressed in XQuery 1.1 as follows: For each context pattern and each snippet search engine returned  , select the words matching tag <A> as the answer. YATL is a declarative  , rule based language featuring pattern matching and restructuring operators. Tree models form an instantiation hierarchy. + trying to have an "intellioent" pattern matching : The basic problem is then to limit combinatorial explosion while deducinc knowledge. In SPARQL 5 no operator for the transformation from RDF statements to SPARQL is defined. Triple Pattern Matching. In the pattern matching step  , we will compare performance of the several kernel functions e.g. Daubechies' wavelet. They primarily used heuristics and pattern matching for recognizing URLs of homepages. —the first system for homepage finding. SPARQL  , a W3C recommendation  , is a pattern-matching query language. The rewriting is sound iff Q G is contained in Each template rule specifies a matching pattern and a mode. A basic XSLT program is a collection of template rules. Tree-Pattern Matching. A run-time stack keeps the states reached and allows such a state backtracking. It also leverages existing definitions from external resources. It identifies definition sentences using centroid-based weighting and definition pattern matching. We obtain We assume  , however  , that indexes are used to access triples matching a triple pattern efficiently. Listing1.2 shows a simple SPARQL query without data streams. The triple pattern matching operator transforms RDF statements into SPARQL solutions. 4 have demonstrated the utility of DTW for ECG pattern matching. Many researchers including Caiani et al. In this paper  , an improved circuit structure corresponds to the complex regular expressions pattern matching is achieved. REFERENCE Second  , the notions of pattern matching and implicit context item at each point of the evaluation of a stylesheet do not exist in XQuery. First  , although xsl:apply-templates may resemble a function call  , its semantics does not correspond to explicit function calls  , but instead relies on a kind of dynamic dispatch based on pattern matching  , template priority  , import precedence  , and modes. We will focus our related work discussion on path extraction queries. These languages can be classified into two categories: path pattern matching queries 22  , 23  , 20  , 10  , 17 that find node pairs connected by paths matching a path pattern; and path " extraction " queries 21  , 13  , 4  , 18  , 12 that return paths. Likewise  , the pattern-matching language in REFINE provides a powerful unification facility   , but this appears to be undecidable—no published results are available about the expressive power of its pattern-matching language. One writes analyzers essentially by programming in a full pro- gramming language; no guarantees can be made about the complexity of analyzers written in ARL  , or VTP. We integrated Mathematica8 into our system  , to perform pattern matching on the equations and identify occurrences within a predefined set of patterns. To address this issue  , we relied on pattern matching  , a very powerful feature that current Computer Algebra Systems CAS provide. On the other hand  , our pattern matching approach is more suitable for determining supporting documents and is therefore the preferable approach for answer projection. Thus we always prefer its answers over results obtained with pattern matching  , which we use as a backup for the remaining questions. Concept assignment is semantic pattern matching in the application domain  , enabling the engineer to search the underlying code base for program fragments that implement a concept from the application domain. Plan recognition is semantic pattern matching in the programming-language domain  , for example identifying common and stereotypical code fragments known as cliches. Traditional pattern-matching languages such as PERL get " hopelessly long-winded and error prone " 5   , when used for such complex tasks. Using pattern matching for NE recognition requires the development of patterns over multi-faceted structures that consider many different token properties e.g orthography  , morphology  , part of speech information etc. The result of unsupervised pattern learning through PRF is a set of soft patterns as presented in Section 2 Step 3a. Option −w means searching for the pattern expression as a word. -bash-2.05>echo "test1 test test2" | grep -Fw test -bash-2.05> Option −F prescribes that the pattern expression is used as a string to perform matching. For example  , the pattern language for Java names allows glob-style wildcards  , with " * " matching a letter sequence and "  ? " String and numeric literals  , Java names  , access modifier lists  , and other non-structured entities are represented using simple text-based pattern languages. The recognition module of person's name  , place  , organization and transliteration is more complex. We use a pattern-matching module to recognize those OODs with fixed structure pattern  , such as money  , date  , time  , percentage and digit. This method requires users to learn specific query language to input query " pattern " and also requires to predefine many patterns manually in advance. Pattern induction   , in contrast  , is intended as detecting the regularities in an ontology  , seeking recurring patterns. The pattern was initially mounted on a tripod and arbitrarily placed in front of the stereo head Fig. Moreover   , the advantage of using this software and pattern is to eliminate human-introduced errors in the selection and matching of points. At the end of this pattern-matching operation  , each element of the structure is associated with a set of indexing terms which are then stored in the indexing base. Moreover the pattern-matching procedure controls  , through nonnalization any excessive growth of the indexing term set. While it is easy to imagine uses of pattern matching primitives in real applications  , such as search engines and text mining tools  , rank/select operations appear uncommon. Typical examples include: pattern matching exact  , approximate  , with wild-cards ,..  , the ranking of a string in a sorted dictionary  , or the selection of the i-th string from it. By adopting regular expressions as types  , they could include rich operations over types in their type structure  , and that made it possible to capture precisely the behavior of pattern matching over strings in their type system. for a minimal functional language with string concatenation and pattern matching over strings 23. Basic pattern matching now considers quadruples and it annotates variable assignments from basic matches with atomic statements from S and variable assignments from complex matches with Boolean formulae F ∈ F over S . Only the basic pattern matching has been changed slightly. However  , we assume that the structure is flat for some operations on pattern-matching queries  , which would not be applicable if the structure was not flat. This restriction is not essential  , since those pattern-matching expressions could perfectly well generate a nested structure. The conceptual definition of pattern matching implies finding the existence of parent node such that when evaluating XPath P with that parent node as a context node yields the result containing the testing node to which template is applicable. The XPath P used in the pattern matching of a template can have multiple XPath steps with predicates. A pattern matched in a relevant web page counts more than one matched in a less relevant one. Third and most important  , we contextualize the pattern matching by distinguishing between relevant and non-relevant pages. As discussed in Section 5  , the size is strongly related to the selectivity . Note that in the following we refer to the number of triples matching a pattern as the size of the pattern. We believe that much information about patterns can be retrieved by analyzing the names of identifiers and comments. Fourth  , we have launched a Master's project to investigate recovery of pattern-based design components with full-text  , pattern-matching techniques. Once the pattern tree match has occurred we must have a logical method to access the matched nodes without having to reapply a pattern tree matching or navigate to them. edge in the APT. Only the definition of windows over the data streams and the new triple pattern operator need special rules. However  , we can still simplify the pattern by removing the parent axis check as shown in We have developed an alternative method based on auxiliary data constructs: condition pattern relations and join pattern relations Segev & Zhao  , 1991a. That structure requires propagating matching patterns to multiple relations when the dimension of joins is larger than two. Semantic pattern discovery aims to relate the data item slots in Pm to the data components in the user-defined schema. Therefore  , each slot of a line can be identified by matching Pc and the line pattern. This is a type of template matching methodology  , where the search region is 1074 examined for a match between the observed pattern and the expected template  , stored in the database. Once the pattern is justified  , the door is successfully detected. Some sentiment patterns define the target and its sentiment explicitly. In a recent survey 19   , methods of pattern matching on graphs are categorized into exact and inexact matching. However  , such structural join approaches often induce large intermediate results which may severely limit the query efficiency. We mainly focus on matching similar shapes. In all of the above tasks  , the central problem is similarity matching: 'find tumors that are similar to a gaven pattern' including shape  , shape changes  , and demographic patient data. The semantics of SPARQL is defined as usual based on matching of basic graph patterns BGPs  , more complex patterns are defined as per the usual SPARQL algebra and evaluated on top of basic graph pattern matching  , cf. 7 In this paper  , we use correlation based pattern' matching to realize the recognition of the oosperm and micro tube in real time. , F k  of data graph G  , in which each fragment Fi = GVi  , Bi i ∈ 1  , k is placed at a separate machine Si  , the distributed graph pattern matching problem is to find the maximum match in G for Q  , via graph simulation. Answering these queries amounts to the task of graph pattern matching  , where subgraphs in the data graph matching the query pattern are returned as results. They represent patterns because either predicate  , subject or object might be a variable  , or is explicitly specified as a constant. We assume that the answer patterns in our pattern matching approach express the desired semantic relationship between the question and the answer and thus a document that matches one of the patterns is likely to be supportive . To minimize the number of unsupported answers  , we decided to always prefer documents identified with pattern matching over those found by the answer type approach. Pleft_seq|SP L  and Pright_seq|SP R  give the probabilistic pattern matching scores of the left and right sequences of the instance  , given the corresponding soft pattern SP matching models. where ins represents a test instance and C denotes the context model. Unknown viruses applying this technique are even more difficult to detect. Self-encrypting and polymorphic viruses were originally devised to circumvent pattern-matching detection by preventing the virus generating a pattern. As an enhanced version of the self-encrypting virus  , a polymorphic virus was designed to avoid any fixed pattern. Moreover  , patterns can only be determined from the unencrypted segment i.e. , the decryption code  , impeding pattern matching even further . In Snowball  , the generated patterns are mainly based on keyword matching. Specificity means the pattern is able to identify high-quality relation tuples; while coverage means the pattern can identify a statistically non-trivial number of good relation tuples . Our pattern matching approach uses textual patterns to classify and interpret questions and to extract answers from text snippets. For instance  , the following is an answer pattern for the property profession: <Target> works as a <Property>. Patterns for answer extraction are learned from question-answer pairs using the Web as a resource for pattern retrieval. Our pattern matching approach interprets a question by creating a concise representation of the question string that preserves the semantics. p i and sq i are the index of pattern and sequence respectively  , indicating from where the further matching starts. value is a probability of sequence segment containing pattern segment. These approaches focus on analyzing one-shot data points to detect emergent events. Pattern-based approaches  , on the other hand  , represent events as spatio-temporal patterns in sensor readings and detect events using efficient pattern matching techniques. In order to identify the list of instructions to re-evaluate  , a pattern matching is performed on the entire re-evaluation rules set. The second optimization is the pattern inclusion. Missing components or sequences in a model compared to an otherwise matching pattern are classed as " incomplete " . A set of intermodel checks between different requirements representation pattern is classed as " incorrectness " . Input rule files are compiled into a graph representation and a depth first search is performed to see if a certain token starts a pattern match. Rose starts by invoking a traditional pattern matching and lexicon based information extraction engine. This way  , when no pattern has been successfully validated  , the system returns NIL as answer. Consequently  , our approach performs probable answer detection and extraction by applying syntactic pattern-matching techniques over relevant paragraphs. Second-order relationships: The relationship between two or more variables is influenced by a third variable. In other words  , a précis pattern comprises a kind of a " plan " for collecting tuples matching the query and others related to them. Joins on a précis pattern are executed in order of decreasing weight. The max-error criterion specifies the maximum number of insertion errors allowed for pattern matching. The min-support criterion specifies the minimum num-ber of times a pattern has to be observed to be considered frequent. In more recent systems  , Lucene  , a high-performance text retrieval library  , is often deployed for more sophisticated index and searching capability. Searching can be as simple as token matching Math- World or pattern matching 15. This includes: word matching  , pattern matching and wildcards  , stemming  , relevance ranking  , and mixed mode searchmg text  , numeric  , range  , date. The IR ,-engine provides the core set of text-retrieval capabilities required by Super- Pages. This system employs two novel ideas related to generic answer type matching using web counts and web snippet pattern matching. 2   , which does not make use of advanced NLP tools. A simpler  , faster subset of this approach is to perform pattern matching based on features. The above method could employ a variety of pattern­ matching and optimization techniques for sensory interpretation. We choose pattern matching as our baseline technique in the toolkit  , because it can be easily customized to distill information for new types of entities and attributes. In KBP2010  , we developed three pipelines including pattern matching  , supervised classification based Information Extraction IE and Question-Answering based 1. Although surface text pattern matching is a simple method  , it is very effective and accurate to answering specific types of ques- tions. Surface text pattern matching has been adopted by some researchers Ravichandran & Hovy 2002  , Soubbotin 2002 in building QA system during the last few years. In addition   , system supports patterns combining exact matching of some of their parts and approximate matching of other parts  , unbounded number of wild cards  , arbitrary regular expressions  , and combinations  , exactly or allowing errors. For each word of a pattern it allows to have not only single letters in the pattern   , but any set of characters at each position. The patterns are assumed to be always right-adjusted in each cascade. We adopted existing code for SQL cross-matching queries 2 and added a special xmatch pattern to simplify queries. Users can either write their own SQL queries or choose cross-matching queries from a predefined set. However automatic pattern extraction can introduce errors and syntactic dependency matching can lead to incorrect answers too. The performance is based on the automatically extracted patterns and n-gram syntactic matching . Because matching is based on predicates  , DARQ currently only supports queries with bound predicates. The matching compares the predicate in a triple pattern with the predicate defined for a capability and evaluated the constraint for subject and object. It is widely used for retrieving RDF data because RDF triples form a graph  , and graph patterns matching subgraphs of this graph can be specified as SPARQL queries. Basically  , SPARQL rests on the notion of graph pattern matching. Answer extraction methods applied are surface text pattern matching  , n-gram proximity search and syntactic dependency matching . ILQUA has been built as an IE-driven QA system ; it extracts answers from documents annotated with named entity tags. δ represents a tunable parameter to favor either the centroid weight or the pattern weight. where Centroid_weight denotes the statistical weight obtained by the centroid based method and Pattern_weight is the weight of soft pattern matching. Instructions associated to a pattern that matches that node need to be re-evaluated. This is achieved by applying a pattern matching between re-evaluation rule patterns and the node currently being modified. We call all the sessions supporting a pattern as its support set. A session S supports a pattern P if and only if P is a subsequence of S not violating string matching constraint. None of these tools are integrated with an interactive development environment  , nor do they provide scaffolding for transformation construction. However  , developers have to write these pattern specifications as an overlay on the underlying code. Cossette and colleagues 9 used a pattern matching approach to link artifacts among languages. The matching is holistic since FiST does not break a twig pattern into root-to-leaf paths. Ambiguous strings are handled at the same time. To demonstrate the flexibility and the potential of the LOTUS framework  , we performed retrieval on the query " graph pattern " . Existing tools like RepeatMasker 12 only solve the problem of pattern matching  , rather than pattern discovery without prior knowledge. It is therefore worth the effort to mine the complete set. Three classes of matching schemes are used for the detection of patterns namely the state-  , the velocity-and the frequency-matching. Regularity Detection is used to detect specific patterns of movement from a properly structured database Itinerary Pattern Base. That is  , the specific pattern-matching mechanism has to influence only that application context. Of course  , only those access events performed by agents of the application example must trigger the reaction leading to the new pattem-matching mechanism. We expect melodic pattern matching to involve what we call " complex traversal " of streamed data. Especially with unpitched sources  , we expect that searching for a melody will be complex  , not simply a matter of literal string matching. To answer " Factoid " and " List " questions  , we apply our answer extraction methods on NE-tagged passages. The pattern matching problem in IE tasks are formally the same as definition sentence retrieval. Our two soft matching models are generic and can be extended to related areas that require modeling of contextual patterns  , such as information extraction IE. The merit of template matching is that it is tolerant to noise and flexible about template pattern. In this paper  , to resolve the problems in conventional methods  , a template matching which is accompanied with projective transformation is proposed. Many commercially available anti-virus programs apply a detection system based on the " pattern signature matching " or " scanner " method. Existing patterns are rendered inapplicable to matching simply with partial modification of the virus code as seen in numerous variants. The results of the pattern-matching are also linguistically normalized  , i.e. We overcome this problem by actually downloading the pages  , analyzing them linguistically  , and matching the patterns instead of merely generating them and counting their Google hits. The prototypes of data objects must be considered during entity matching to find patterns. Thus  , pattern mining that relies solely on matching type names for program entities would not work. Applicability in an Epoq optimizer is similar in function to pattern-matching and condition-matching of left-hand sides in more traditional rule-based optimizers. Thus  , they are used to improve the efficiency of the optimizer. With the manual F 3 measure  , all three soft pattern models perform significantly better than the baseline p ≤ 0.01. 2 that soft matching patterns outperform manually constructed hard matching patterns in both manual and automatic evaluations. To answer " Factoid " and " List " questions  , we apply our answer extraction methods on NE-tagged passages or sentences. 9  , originally used for production rule systems  , is an efficient solution to the facts-rules pattern matching problem. The main disadvantage of predicate-based matching is that predicates should be pre-defined in advance. As mentioned above  , the pattern should skip this substring and start a new matching step. From the foregone information of success matching  , it can get the substring T8 ,9= " is "   , while there is no substring " is " in P1 ,2 ,3 ,4. Besides the detection and localization of a neural pattern  , the comparison and matching of the observed pattern to a set of templates is another interesting question 18. QUANTUM PATTERN RECOGNITION Pattern recognition is one of the basic problems in BCI systems. They also discuss the subtlety we mention in Sec. Haack and Jeffrey 6 discuss their pattern-matching system in the context of the Spi-calculus. Presence of modes allows different templates to be chosen when the computation arrives on the same node. The recursion in the SPARQL query evaluation defined here is indeed identical to 11  , 13. The same assumption is made for grouping constraint and output aggregate function. Later on  , standard IR techniques have been used for this task. Backtracking moves to the next breakpoint fget or the next visible variable current-var. Pattern matching checks the attributes of events or variables. This paper focuses on the ranking model. The extraction can be done using simple pattern matching or state-of-the-art entity extractors. slot is bound to the key chunks of questions. ANSWER indicates the expected answer. The basic cell for all pattern matching operations is shown in Figure 19.2. The equations describing the cell can be written as Wiki considers the Wikipedia redirect pairs as the candidates. Pattern considers the words matching the patterns extracted from the original query as candidates. Encounters green are generated using a camera on the quadrotor to detect the checkerboard pattern on the ground robot and are refined by scan matching. In order to avoid this drawback  , we implemented a new module of text-independent user identification based on pattern matching techniques. System overview. The center coordinates of iris are estimated from each model that is estimated its location by pattern matching. Fig.4shows the situation of eye movement detection. proposed an inverse string matching technique that finds a pattern between two strings that maximizes or minimizes the number of mis- matches 1 . Amir et al. They analyze the text of the code for patterns which the programmer wants to find. Pattern matching tools help the programmer with the task of chunking. Other languages for programming cryptographic protocols also contain this functionality. We discuss our method of soft pattern generalization and matching in the next section. Both experiments show significant improvement over baseline systems. The pages that can be extracted at least one object are regarded as object pages. Each URL not matching any patterns is regarded as a single pattern. A new technique is required to handle the grouping operation in queries. The value which is determined by pattern matching is DataC KK the server's public key for the signature verification . kgenArgS 12. This information is then logically combined into the proof obligations. Implementability and operation decomposition are expressed similarly: pattern matching is used to extract the necessary in- formation. Tuples are anonymous  , thus their removal takes place through pattern matching on the tuple contents. Tuples can be removed from a tuple space by executing inp. The Concern Manipulation Environment CME supports its own pattern-matching language for code querying. SOQUET on the other hand  , emphasizes relations specific to crosscutting concerns implementation. The instrumentation is based on rules for pattern-matching and is thus independent of the actual application. The satellites automatically instrument the application using Javassist 25. Siena is an event notification architecture . Replace performs pattern matching and substitution and is available in the SIR with 32 versions that contain seeded faults. There have been many studies on this problem. In the literature " approximate string matching " also refers to the problem of finding a pattern string approximately in a text. To tackle this problem  , other musical features e.g. , chord progressions  , change in dynamics  , etc. This was mainly caused by the inaccuracy of the approximate pattern matching. The definition generation module first extracts definition sentences from the document set. Perfect match is not always guaranteed. The candidate sentences are parsed and the parse trees are traversed bottom-up to do pattern matching. Our patterns are flexible -note that the example and matched sentences have somewhat different trees. ple sentence to pattern  , and then shows a matching sentence. All the following described operators consume logical streams. For every pattern tp i in query Q  , a sorted access sa i retrieves matching triples in descending score order. Access. The argument can be any expression of antecedent operators and concepts and text. Thereby  , the amount of informa3. When w  , r  , or w 0 is *  , the frequency counts af all dependency triples matching the rest of the pattern are summed up. For the first run  , definition-style answers were obtained with KMS definition pattern-matching routines as described. Two runs were made. Some question types have up to 500 patterns. Morph considers the morphologically similar words as candidates.  ls: lightly stemmed words  , obtained by using pattern matching to remove common prefixes and suffixes. 2g  , 3g  , … 7g: character n-grams 2-7 gram. As an example  , consider the problem of pattern matching with electrocardiograms. However  , one may wish to assign different weights to different parts of the time series. Autonomous robots may exhibit similar characteristics. Biological swarm members often exhibit behavioral matching based on the localized group's pattern  , such that behaviors are synchronized 4. The reason is the handling of pattern matching in the generated Java code with trivially true conditional statements. Coverage does not exceed 79%. A search token is a sequence of characters defining a pattern for matching linguistic tokens. Leaf nodes in an XML document tree may contain multiple linguistic tokens. Xcerpt's pattern matching is based on simulation unification. 2 Novel evaluation methods for Xcerpt  , enabled by high-level query constructs  , are being investigated. We do not present an exhaustive case study. In the following  , we give some formulas in order to perform pattern matching between expressions and patterns. It can extract facts of a certain given relation from Web documents. Leila is a state-ofthe-art system that uses pattern matching on natural language text. Second  , po boils down to " pattern matching  , " which is a major function of today's page-based search engine. We can similarly handle factors 3 and 4. Through training  , each pattern is assigned the probability that the matching text contains the correct answer. based on a training set of given question-answer pairs. Stream slot filling is done by pattern matching documents with manually produced patterns for slots of interest. In Figure 1we refer to this as Streaming Slot Value Extraction. We have reviewed the newly-adopted techniques in our QA system. Also  , there is a need to find ways to integrate numberic matching into the soft pattern models. -relevance evaluation  , which allows ordering of answers. This important feature IS based on a syntacttc pattern matching between user's concepts and system known concepts. These patterns were automatically mined from web and organized by question type. Some question type has up to several hundred patterns. Similar to the twig query  , we can also define matching twig patterns on a bisimulation graph of an XML tree. We call this bisimulation graph the twig pattern. The searching trajectory can be designed intentionally to ease detection of such features. The template of a character is represented by a dot pattern on the 50*50 grid. Character recognition is conducted using template matching.  s: aggressively stemmed words  , found using the Sebawai morphological analyzer. ls: lightly stemmed words  , obtained by using pattern matching to remove common prefixes and suffixes. The generated file is used for programming of FPGA and pattern matching. Snort library is sorted  , optimized and compiled by rule compiler. The general idea behind the approach is pattern matching. The advantage of our approach is that it is not limited to a pre-defined set of semantic categories. Researchers using genetic data frequently are interested in finding similar sequences. Consequently searches need to be based on similarity or analogy – and not on exact pattern-matching. Then we insert randomly some sequences  , defined as " suspicious "   , and detect them through our threshold mechanism. The program slice is smaller than the whole program  , and therefore easier to read and understand. The representation for data objects and their relationships with each other is a relational data base with a pattern-matching access mechanism. goal-directed invocation. A more likely domain/range restriction enhances the candidate matching. For this pattern  , dbo:City is more likely to be a domain than dbo:Scientist  , and so for the range. A somewhat different approach to facilitate multiple language comprehension is DSketch. Application designers can exploit the programmability of the tuple spaces in different ways. We chose the first 20 changed versions. Feasible ? Nevertheless  , such pattern matching is well supported in current engines  , by using inverted lists– our realization can build upon similar techniques. Implementation We have developed a prototype tool for coverage refinement . The time overhead of event instrumentation and pattern matching is approximately 300 times to the program execution. One aspect of our work extends CPPL to include match statements that perform pattern matching. In this paper we are only interested in SPARQL CONSTRUCT queries. SPARQL is a query language for RDF based on graph pattern matching  , which is defined in 4. The result is empty  , if negatively matched statements are known to be negative. This definition of basic graph pattern matching treats positively matched statement patterns as in 4. Some question type has up to 500 patterns. Additionally  , a classifier approach is more difficult to evaluate and explain results. Two sets of rules are developed to generate numbers and entities  , respectively. AskDragon uses pattern matching rules to generate candidate answers. In this paper we will consider only B-tree indices. Blank nodes have to be associated with values during pattern matching similiar to variables. Besides variables SPARQL permits blank nodes in triple patterns. it changes the schema of the contained elements. The Entrez Gene database and MeSH database were used for query expansion. Our system first extracted key terms from topic narratives by pattern matching. In their most general forms these ope~'a~ors are somewhat problematic. Similarly  , the *PARAGRAPH* operator reduces the scope of the pattern matching to a single paragraph. Pattern matching approaches are widely used because of their simplicity. The idea of the so-called pyramid search is depicted in figure 3. attack or legitimate activity  , according to the IDS model. This step performs the intrusion detection task  , matching each test pattern to one of the classes i.e. µ is a solution in evalG W   , BGP   , if it is complete for BGP and Our context consistency checking allows any data structure for context descriptions. The first context instance in Figure 1has a matching relation with the first pattern in Figure 2. A type system based on regular expressions was studied by Tabuchi et al. Definition 5.4 Complex graph pattern matching. Now we define the evaluation of complex graph patterns by operations on sets of variable assignments similar to 11  , 13. Normal frames with a hea.der pattern can be used for both matching and inheritance . These frames are used for inheritance only. However  , header patterns of those frames cannot be inherited -only their cases. Worse  , some JS variables might not have declared types O5. Since the combinator used in the event pattern is or  , matching el is sufficient to trigger the action . Say that an announced event that matches el is received . It was shown in the PRIX system 17  that the above encoding supports ordered twig pattern matching efficiently. Refer to Section 3. In order to print matches and present the results in root-to-leaf order  , we extended the mechanism proposed by 5. For example  , tree pattern matching has also been extensively studied in XML stream environment 7  , 15 . There are many promising future directions. due to poor lighting conditions  , reflections or dust. The matching can fail in the case that the pattern does not appear  , e.g. It identifies definition sentences using centroid-based weighting and then applies the soft-pattern model for matching these definition sentences. The liberty to choose any feature detector is the advantage of this method. Also  , this method can be accelerated using hierarchical methods like in the pattern matching approach. Also  , interfaces based on structured query languages  , SPARQL in particular  , are widely employed. To cope with the problem of blank nodes we need to extend the definition for an RDF instance mapping from 9: The relative calibration between the rigs is achieved automatically via trajectory matching. The individual stereo rigs are calibrated in a standard way using a calibration pattern. Otherwise  , if no graph pattern from C matches  , the source graph pattern P represents graphs that can be transformed into unsafe graphs by applying r  , and If a graph pattern from C matches the source graph pattern  , the application of r is either irrelevant  , as the source graph pattern already represents a forbidden state  , or impossible   , because it is preempted by another matching rule with higher priority. Note that one image-pattern neuron is added at every training point and the target's pose at that point is stored in conjunction with the image-pattern neuron for use later. The existing or newly created layer-pattern neurons one per layer image exactly matching the training image are then in tum defined as an image pattern  , and a corresponding imagepattern neuron is introduced with connections to the appropriate layer-pattern neurons. Generic tree pattern matching with similar pattern description syntax is widely used in generic tree transformation systems such as OPTRAN 16  , TXL 5  , puma 11  , Gentle 18  , or TRAFOLA 13  , as well as in retargetable code generation  , such as IBURG 10. The idea is to model  , both the structure of the database and the query a pattern on structure  , as trees  , to find an embedding of the pattern into the database which respects the hierarchical relationships between nodes of the pattern. Matching 15: is a query model relying on a single primitive: tree inclusion. To avoid such an overhead  , each time a pattern is converted from an expression  , the expression's instruction is added to the re-evaluation rules that include the new pattern. After experimenting with several structural pattern languages based on text  , we discovered that any moderately sophisticated tern quickly becomes difficult to understand. This led us to a pattern language that consists of fragments of Java source code  , augmented with wildcards  , pattern variables  , and semantic matching constraints such as " static type of this expression must be a subtype of java.lang. Serializeable " . We are aware that an exact matching between correlation matrices respectively relying on role pattern and users' search behaviors is dicult to reach since the role pattern is characterized by negative correlations equal to -1. The role pattern correlation matrix is the most likely similar to the collaborative group correlation matrix. Therefore  , a reasonable role-based identification is to assign the role pattern correlation matrix F R 1 ,2 which is the most similar to the one C We are aware that an exact matching between correlation matrices respectively relying on role pattern and users' search behaviors is dicult to reach since the role pattern is characterized by negative correlations equal to -1. Standard pruning is straightforward and can be accomplished simply by hashing atomsets into bins of suhstructures based on the set of mining bonds. This approach is faster than traditional approaches both because counting occurs without the need to go back to the entire molecule and because counting is done through pattern-pattern instead of pattern-dataset matching  , which results in far fewer comparisons. A pattern describes what will be affected by the transformation; an action describes the replacement for every matching instance of the pattern in the source code. This experiment showed that a traditional pattern/action-based description of a searchand-replace transformation is a natural way to describe code changes. Given the fact that a question pattern usually share few common words with each perspective  , we can hardly build effective matching models based on word-level information. Since the question pattern represents what information is being asked irrespective of the topic entity  , intuitively a correct candidate chain should match the question pattern from the above three perspectives. This year  , we further incorporated a new answer extraction component Shen and Lapata  , 2007 by capturing evidence of semantic structure matching. In TREC 2006 Shen et al. , 2006   , we developed a maximum entropy-based answer ranking module  , which mainly captures the evidences of expected answer type matching  , surface pattern matching and dependency relation correlation between question and answer sentences. Approximate string matching 16 is an alternative to exact string matching  , where one textual pattern is matched to another while still allowing a number of errors. from the LOD Laundromat collection to be findable through approximate string matching on natural language literals. Nevertheless  , we anticipate that pattern-matching operations on NEUMES data as distinct from literal string matching will be required during melodic search and comparison operations. These patterns  , mainly consisting of appositives and copulas  , are high-precision patterns represented in regular expressions  , for instance " <SEARCH_TERM> is DT$ NNP " . Each time cgrep returns matching strings  , they are removed from the document representation and the procedure is repeated with the same phrase. For each subphrase in the list we use cgrep – a pattern matching program for extracting minimal matching strings Clarke 1995 to extract the minimal spans of text in the document containing the subphrase. The exact matching requires a total mapping from query nodes to data nodes  , i.e. , all query nodes are exactly matched by their corresponding data nodes  , and each parent-child " / " resp. To perform a matching operation with respect to a contiguous word phrase  , two approaches are possible. The straightforward solution  , which recursively Figure 3: Tree-pattern matching by subsequence matching identifies matches for each node within the query sequence in order  , requires quadratic time in the document size and therefore becomes not competitive. The heart of those methods is the subsequence matching module. For example  , if OOPDTool detects an instance of the FactoryMethod design pattern  , it would detect not only the presence of this pattern in the design but also all classes corresponding to the Abstract Creator  , Concrete Creator  , Abstract Product  , and Concrete Product participants found in this design pattern instance. By selecting the desired design patterns  , the user is able to receive a report indicating the design patterns found  , and all the elements matching each participant role involved in the pattern. The problem of mining graph-structured data has received considerable attention in recent years  , as it has applications in such diverse areas as biology  , the life sciences  , the World Wide Web  , or social sciences. The conjunctivequery approach to pattern matching allows for an efficiently checkable notion of frequency  , whereas in the subgraph-based approach  , determining whether a pattern is frequent is NP-complete in that approach the frequency of a pattern is the maximal number of disjoint subgraphs isomorphic to the pattern 20. Results The data are summarized in Table 1   , which gives totals for each pattern/scope combination  , and in Fig- ure 4  , which graphs the totals for each pattern and scope examples not matching any pattern are grouped under UNKNOWN. PATTERN: Response SCOPE: Global PARAMIZTERS: Propositions boolean vector LTL: RequestedRegisterImpli As noted above  , all of the specifications we found are available on the World Wide Web at 8. The problem of finding the top-k lightest loopless path  , matching a pre-specified pattern  , is NP-hard and furthermore   , simple heuristics and straightforward approaches are unable to efficiently solve the problem in real time see Section 2.3. We then formalize the problem as top-k lightest paths problem   , targeting the top-k lightest loopless paths between the entities   , matching the path pattern see Section 2.1. We have adopted a " query language " approach  , using a well understood  , expressively limited  , relatively compact query language; with GENOA  , if an analyzer is written strictly using the sublanguage Qgenoa  , the complexity is guaranteed to be polynomial. The idea proposed in 9  is to compile XSLT <applytemplates/> instruction into a combination of XQuery's conditional expressions where the expression conditions literally model the template pattern matching and the expression bodies contain function calls that invoke the corresponding XQuery function that translated from the XSLT template. The challenging aspect here is to how to translate <apply-templates/> instruction  , which implicitly demands the template pattern matching. If the pattern has a 'don't care' symbol  , then the cell should essentially perform a 'unit stage delay' function to propagate the match signal from the previous stage to the next stage. The operation of the pattern matching cascade with sub-string matching capability and 'don't care' characters is illustrated in If the anchor vector has ls in positions s1  , $2 ,.. s k positions the strings x ,.. x ,  , x ,~ .. x ,  , x~  , .. x. have occurred in the text string. For example  , if we know that the label " 1.2.3.4 " presents the path " a/b/c/d "   , then it is quite straightforward to identify whether the element matches a path pattern e.g. " With the knowledge of this property  , we further consider that if the names of all ancestors of u can be derived from labelu alone  , then XML path pattern matching can be directly reduced to string matching . We have already mentioned bug pattern matchers 10  , 13  , 27: tools that statically analyze programs to detect specific bugs by pattern matching the program structure to wellknown error patterns. The authors' experience is similar to ours in that ESC/Java used without annotating testee code produces too many spurious warnings to be useful alone. To detect both known and unknown viruses effectively and accurately  , we must be able to combat viruses that are capable of self-encryption and polymorphism. Exact queries in Aranea are generated by approximately a dozen pattern matching rules based query terms and their part-of-speech tags; morpho-lexical pattern matches trigger the creation of reformulated exact queries. is likely to appear within ten words and fifty bytes to the right of the exact phrase " the Mesozoic period ended " . We describe one such optimization in this paper  , which is called pattern indexing and is based on the observation that a document typically matches just a relatively small set of patterns. Annotated Pattern Trees accept edge matching specifications that can lift the restriction of the traditional oneto-one relationship between pattern tree node and witness tree node. Instead we will try to show the intuition on APTs and LCs and walk through an example with them. This method creates a definition of length N by taking the The extracted partial syntax-tree pattern contains Figure 2: Pattern extraction and matching for a Genus-Species sentence from an example sentence. Then the individual sentences are sorted in order of decreasing " centrality  , " as approximated by IDF-weighted cosine distance from the definition centroid. first N unique sentences out of this sorted order  , and serves as the TopN baseline method in our evaluation . In 1  , we came to the conclusion that the pattern matching approach suffers from a relatively low recall because the answer patterns are often too specific. An answer pattern covers the target  , the property  , an arbitrary string in between these objects plus one token preceding or following the property to indicate where it starts or ends. Higher-level problems  , including inconsistency  , incompleteness and incorrectness can be identified by comparing the semi-formal model to the Essential interaction pattern and to the " best practice " examples of EUC interaction pattern templates. Low-level inconsistency problems can be identified such as natural language phrases without matching semi-formal model elements and meta-model constraint violations of the extracted model. Given a back-point βintv  , p index  , the uncertain part of sequence S is the sequence segment S i that is inside β.intv  , while the pattern segment P i   , which is possibly involved in uncertain matching  , could be any pattern segment starting from β.p index. After greedy testing fails  , we acquire a list of back-points. For example  , the head-and-shoulder pattern consists of a head point  , two shoulder points and a pair of neck points. In 16  , we proposed a flexible time series pattern-matching scheme that was based on the fact that interesting and frequently appearing patterns are typically characterized by a few critical points. Such tools do not generate concrete test cases and often result in spurious warnings  , due to the unsoundness of the modeling of language semantics. A straightforward way to solve the top-k lightest paths problem is to enumerate all paths matching the given path pattern and pick the top-k lightest paths. Here vertex 6 can be mapped to both the second vertex label and the fourth vertex label in the path pattern. Therefore  , we need to convert a triple pattern into a set of coordinates in data space  , using the same hash functions that we used for index creation  , to obtain coordinates for a given RDF triple. To determine relevant sources we first need to identify the region in data space that contains all possible triples matching the pattern. Section 3 describes the architecture of our definition generation system  , including details of our application of PRF to automatically label the training data for soft pattern generalization. It also became clear that developers want to use high-level structural concepts e.g. , a class  , a variable  , an if-statement to describe patterns  , and prefer to use fragments of source code to describe actions. A rewrite rule is a double grafting transformation consisting of a tree pattern T also called " the lefthand side "  and advice Γ that is applied to the source at all locations where T matches. An aspect is a set of pattern-matching-based rewrite rules that statically extend a given program with sets of programming statements and declarations that together implement a crosscutting concern. The worst case scenario would be for the optimizer to not incorporate sorting into the pattern tree match and apply it afterwards. The selectivity of such query is determined by the original selection and the trees produced when matching the pattern tree of the selection to the database. Pattern matching deal with two problems  , the graph isomorphism problem that has a unknown computational complexity  , and the subgraph isomorphism problem which is NP-complete. in determining if there exists a mapping or isomorphism  between a graph pattern and a subgraph of a database graph use cases 2.1  , 2.12 and 2.13 in DAWG Draft 58. We expressly do not wish to support this because it would correspond to replay attacks and violate freshness assump- tions. We assume that XML documents are tokenized by a languagedependent tokenizer to identify linguistic tokens. To demonstrate how an application can add new facts to the YAGO ontology  , we conducted an experiment with the knowledge extraction system Leila 25 . Third  , template parameters  , as opposed to XQuery function parameters   , may be optional. However  , sequence < 1  , 3  , 2 > supports < 1  , 3 >. They hence can be pushed to be executed in the navigation pattern matching stage for deriving variable bindings. Intuitively  , this can be done because these constraints and conditions are  , in a sense  , analogous to the relational selection operations. Nevertheless  , CnC possibly suffers more than bug pattern matching tools in this regard because it has no domain-specific or context knowledge. As mentioned earlier  , every automatic error checking system has this weakness. With the use of AI techniques for semantic pattern matching  , it may be possible to build a relatively successful library manager. Simple keyword searching is unlikely to be adequate. In IntelliJ IDEA  , there is a facility called Structural Search and Replace that enables limited transformations by pattern matching on the syntax tree. In Eclipse  , it requires writing a new plugin  , and mastery of a number of complex APIs. The pattern-matching techniques  , such as PMD  , are unsound but scale well and have been effectively employed in industry. These likely locations are reported to programmers typically at coding-time. Patterns were originally developed to capture recurring solutions to design and coding prob- lems 12 . We adopted a pattern-based approach to presenting our specification abstractions because of its focus on the matching of problem characteristics to solution strategies. Furthermore   , it allows for restriction of the query domain  , similar to context definitions in SOQUET 8 . When a group of methods have similar names  , we summarize these methods as a scope expression using a wild-card pattern matching operator . The only methods transformed are those in the scope but not in the exceptions. Certain PREfast analyses are based on pattern matching in the abstract syntax tree of the C/C++ program to find simple programming mistakes. As a result  , the PREfast analyses are inexpensive  , accounting for negligible percentage of compile time. For instance  , in the following case. We have thus decided to combine navigational probing with FSMs and present a new method SINGLEDFA for this category. At each point  , partial or total pattern matching is performed  , depending on the existing partial matches and the current node. In a first step the name is converted to its unique SMILES representation: For each matching SMARTS pattern  , we set the corresponding bit to 1. Let us consider our chemist searching for Sildenafil. Exact pattern matching in a suux tree involves one partial traversal per query. We currently estimate this threshold to be in the region of minimum query length of 10 to 12 letters for human chromosomes.  We show the efficient coordination of queries spanning multiple peers. This paper has explored the integration of traditional database pattern matching operators and numeric scientific operators. Integrating support for arrays  , as well as operations on them  , is an important extension of this research which we are currently investigating. These patterns  , such as looking for copular constructions and appositives  , were either hand-constructed or learned from a training corpus. Many systems used pattern-matching to locate definition-content in text. The patterns used in ILQUA are automatically learned and extracted. However in some situations  , external knowledge is helpful  , the challenge here is how to acquire and apply external knowledge. The what questions that are classified by patterns are in Table  ? ?. Some What questions are classified by pattern matching and  , for the rest of questions  , the question focus is used to classify the questions. For query generation  , we modify verb constructions with auxiliaries that differ in questions and corresponding answers  , e.g. " To facilitate pattern matching   , all verbs are replaced by their infinitives and all nouns by their singular forms. The system then builds semantic representation for both the question and the selected sentences. We try to find the answer from the sentence list returned without a match by the pattern matching step. We found that 12 ,006 reports had one visit associated while 2 ,387 of the reports had more than or equal to 10 visits. Using simple pattern matching we extracted section headings and identified segments pertaining to different population and age groups. Besides generating seed patterns  , the Pattern Matching method also relies on the ability of tagging the words correctly. In order to do that  , we collected list of cities  , list of states  , and list of countries. An example of the pattern matching operation is shown in Figure 19 The 'anchor' input line could be pulsed with arrival of every text character  , in which case the operations will take place in the 'unanchored' mode. This occurs because  , during crawling  , only the links matching the regular expression in the navigation pattern are traversed. Notice that  , in all cases  , the numbers in the " Crawling " column are smaller than the numbers in the " Generation " column. We run each generated crawler over the corresponding Web site of Table 2two more times. When certain characters are found in an argument  , they cause replacement of that argument by a sorted list of zero or more file names obtained by pattern-matching on the contents of directories. Many command arguments are names of files. Most characters match themselves. In general  , mining specifications through pattern matching produces a large result set. Adding then becomes a sequence of Boolean operations: we intersect the value to add with the " adder " BDD and remove the original value by existential quantification. Previous work 10  , 18  , 25 on mining alternating specifications has largely focused on developing efficient ranking and selection mechanisms . The type of the exception thrown is compared with the exception types declared as arguments in each catch block. No matching pattern indicates that PAR cannot generate a successful patch for a bug since no fix template has appropriate editing scripts. Generating this predicate from scratch is challenging. Word expert parsers 77  seem particularly suitable ; the TOPIC system employs one to condense information from article abstracts into frames 39. Flexible parsing methods  , often based on pattern matching  , are of value in these situations 41. In particular  , there are two sets of rules predicates which work together to identify the set of successor tasks. The generation of potential candidates i s performed by Prolog's pattern matching. EDITOR is a procedural language 4 for extraction and restructuring of text from arbitrary documents. Furthermore  , pattern matching across hyper-links which is important for Web Site navigation is not supported. by embedding meta data with RDFa. Atheris relies on the robust pattern-matching technique of ViPER and introduces an abstraction layer between web pages and additional functionality for these pages changing the appearance  , adding information  , etc. outline preliminaries in Sect. For a partial binding b  , we refer to a pattern tp i with no matching triple as unevaluated and write * in b's i-th position: The deletion of triples also removes the knowledge that has been inferred from these triples. RDF triples can also be removed from the knowledge base by providing a statement pattern matching the triples to be deleted delete. Relevant datasets are selected using the predicate-matching method  , that a triple pattern is assigned to datasets that contains its predicate. However  , as admitted by the authors  , detailed VoID files are unlikely to be available on a large scale. There is often not much texture in indoor man-made environments for high coverage dense stereo matching. We can see that the coverage of the 3D model is increased substantially with the pattern projector. Accordingly  , it is able to localize points more precisely even if an image is suffering from noise. On the other hand  , pattern matching method performs directly on original image. Because the feature abstracting is time-consuming  , this kind of method is difficult to realize in real time. We use a method  , which is based on binary morphological operation  , to recognize the micro tube. It is not suitable to use pattern matching method to recognize the micro injector because of the low efficiency and poor accuracy. During the preliminary system learning two binary images are formed fig. Due to high TV raster stability and precision of manipulators that are used for the next LCD positioning the task was reduced to binary pattern matching. Another advantage is that for the rotation of the object only few sample points have to be rotated. Subgrouping may occur based on the group's task  , position within the swarm  , entity size  , role or a combination of factors. Figure 9shows an interesting inversed staircase pattern due to the reverse presentation order. Typical cross reactions between similar patterns are actually desired and illustrate a certain tolerance for inexact matching. Our second major enhancement to traditional parallel coordinates visualization allows the user to query shapes based on approximate pattern matching. The query can be formed either by indicating an example data point or by specifying the shape of interest explicitly. However  , conversations are bound to evolve in different conversational patterns  , leading to a progressive decay in the matching ambiguity. This is observed   , first  , because most conversations in the beginning exhibit a customary dialog pattern  " hi  , " " how are you  , " etc. each sentiment phrase detected Section 3.3  , SA determines its target and final polarity based on the sentiment pattern database Section 3.1.2. Note that figures 7 and 8 represent matching results of the sequences grouped into the same cluster. Since this pattern was commonly observed regardless of virus type and administration of IFN  , it implied ineffective cases of IFN treatment. As joins are expressed by conjunctions of multiple triple patterns and associated variables  , a prerequisite for join source selection is the identification of relevant sources for a given triple pattern. It matches the exact source code fragment selected by the user and all the other source code fragments that are textually similar to the selection whitespace and comments are ignored by the pattern matcher. The generated pattern is concrete  , that is  , it contains no wildcards and no matching constraints. For example  , the proximity function can be evaluated by keeping track of the word count in relation to specified set of pattern matches. The queries in classes 7 and 8 can be implemented by the SFEU by combining the results of basic pattern matching. The elementary graph pattern is called a basic graph pattern BGP; it is a set of triple patterns which are RDF triples that may contain variables at the subject  , predicate  , and object position. SPARQL is based on graph patterns and subgraph matching. The C-SPARQL 1 extension enabled the registration of continuous SPARQL queries over RDF streams  , thus  , bridging data streams with knowledge bases and enabling stream reasoning. Two important types of patterns are the value change pattern and the failure pattern. After the matching is completed  , sorting of variables is performed to enable the user to view those most interesting patterns in nearby sections of the horizontal axis. An example for this definition is given by evaluating the query from Example 5.1 on the dataset of Example 5.2 delivering the result as indicated in example 5.3. These potential problems are highlighted to the engineer using visual annotations on the EUC model elements. Matching of a substantial part of an extracted EUC model to an EUC pattern indicates potential incompleteness and/or incorrectness at the points of deviation from the pattern. In terms of CASE tools support  , we are testing a few mechanisms that allow generation of constraints for pattern verification as well as matching rules for pattern recovery given a UML design model. We are currently working on the specification of leitmotif behavior with the aid of Action Semantics. This model can represent insertion  , deletion and framing errors as well as substitution errors. In this model  , a pair i  , j of original and recognized string lengths is used as an error pattern of OCR and weight  , or a penalty of incorrect recognition is assigned to each pattern to calculate the similarity of two strings by dynamic programming matching. Then extracted sentences are scanned  , detecting the constructs matching the template < person1 >< pattern >< person2 > such as <Barack Obama><and his rival><John McCain>  , using a person names dictionary and a sliding window with a pattern length of three words. The entity pairs are extracted from the body of the archived documents first by splitting the documents into sentences using the Stanford CoreNLP library 4 . To reduce noise in the data we exclude pairs with identical names and discard overly long sentences and patterns. The individual right that the teacher Martin holds  , allowing him to reproduce an excerpt of the musical piece during a lesson  , is derived from the successful matching between the instances describing the intended action and the instances describing the pattern. The intended action is highlighted on the bottom half and the top half is the permission pattern. Thus in the experiments below  , for the target set any attribute value that is not specifically of interest as specified by the target pattern retains its original value for determining matching rules. Where target pattern means: the set of attribute values in the target set that are being evaluated. 3 Many research works for the repeating patterns have been on an important subtype: the tandem repeats 10  , where repeating copies occur together in the sequence. The matching degree is calculated in two parts. Using the same window size w  , the token fragment S surrounding the <SCH_TERM> is retrieved: The matching degree of the test sentence to the generalized definition patterns is measured by the similarity between the vector S and the virtual soft pattern vector Pa. On the other hand  , the pattern in Figure 2a will not capture all resale activities due to the limitation of using the single account matching. It is much desired that an elastic matching of items can be used to accurately identify resales. The second factor requires matching specific tuple occurrences γ Section 4.2  , which can only be executed when the query terms e.g. , " amazon " and #phone and patterns e.g. , ow are specified. For example   , one cannot constrain the matching of events that logically match various parts of the same event pattern to those events that were generated by the same user or on the same machine. Snoop  , however  , does not provide mechanisms for using contextual in- formation to constrain event matching. Typically  , a Web browser interprets an HTML file just once  , in sequential order  , and so the semantics of character data do not need to be spot-checked by 'random access'. Incorporating individual slots' probabilities enables the bigram model to allow partial matching  , which is a characteristic of soft pattern matching. This is because the position of a token is important in modeling: for instance  , a comma always appears in the first slot right of the target in an appositive expression. In other words  , even if some slots cannot be matched  , the bigram model can still yield a high match score by combining those matched slots' unigram probabilities. If no handler is found in the whole call stack  , the exception handler mechanism either propagates a general exception or the program is terminated. In LOTUS  , query text is approximately matched to existing RDF literals and their associated documents and IRI resources Req1. To train these semantic matching models  , we need to collect three training sets  , formed by pairs of question patterns and their true answer type/pseudopredicate/entity pairs. Three matching models shall be learned for question pattern respectively paired with answer type  , pseudopredicate   , and entity pairs. The tool implementation of MATA has been extended to include matching of any fragments using AGG as the back-end graph rule execution engine. Since MATA is based on graph transformations  , sequence pointcuts can be handled in a straightforward manner since they are just another application of pattern matching-based weaving.  The FiST system provides ordered twig matching for applications that require the nodes in a twig pattern to follow document order in XML. The stack enables the testing of parent-child and ancestor-descendant relationships and limits the search space during the subsequence matching. system  , with rules maximizing recall  , 2 Pass the grammar annotated data through an ML system based on Carreras  , X. et al  , 2003  , and 3 In the spirit of Mikheev  , A. et al  , 1998 perform partial matching on the text. Techniques were used for query expansion  , tokenization  , and eliminating results due solely to matching an acronym on the query side with an acronymic MeSH term. The first phase consisted of pattern matching between query terms and MeSH terms that are found in MeSH regular descriptor file and MeSH supplementary concept records file. However the matching is not straightforward because of the two reasons. Since the positions of the acoustic landmarks are independent of the current position of a mobile robot  , we may localize the mobile robot by matching the newly acquired two dimensional pattern of the reflectors with that of the acoustic landmarks. Consider a software system that is modeled by its inheritance and containment graphs  , and the task is to analyze how many instances of the design pattern Composite are used in the design of the system. During the training session  , the above extraction pattern is applied to the web page and the first table matching the pattern is returned as the web clip. Thus  , one of the extraction patterns would be a <TABLE> element that immediately follows a <DIV> with the text " Sample Round-trip fares between: " . This is presented to the user by Figure 4: Training session highlighting the clipped element with a blue border. Therefore  , in the following components we treat URLs matching with each pattern as a separate source of information. It is shown by our experiments that each selected URL pattern usually matches with a large number of URLs of the same format. If a sample graph vertex label matches the pattern but is not correctly mapped to the model graph vertex then the fitness of the projection is reduced. Accordingly   , our approach allows the user to specify regular expression patterns as part of the fitness function such that sample graph vertices matching the pattern should be clustered and mapped to a particular model graph vertex. The tool compares extracted EUC models to our set of template EUC interaction patterns that represent valid  , common ways of capturing EUC models for a wide variety of domains. As part of an earlier task on a system that supported the visualization of object connections in a distributed system  , the subject had implemented a locking mechanism to allow only one method of an object to execute at one time. But the pattern is quite difficult to understand so it helps to have this pattern level view and this matching into the source code. 3 In case some attributes are non-nullable  , we use SET DEFAULT to reset attributes values to their default value. 2 In contrast  , when matching a data tuple t and a pattern tuple tp  , tX tpX is false if tX contains null  , i.e. , CFDs only apply to those tuples that precisely match a pattern tuple  , which does not contain null. Certainly  , if the lexicon is available in main memory it can be scanned using normal pattern rnatching techniques to locate partially specified terms. Standard languages for interactive text retrieval include pattern-matching constructs such as wild characters and other forms of partial specification of query terms 1121. Each of the rewriting patterns contains a * symbol  , which encodes the required position of the answer in the text with respect to the pattern. Each rewriting rule is composed of one Perl-like question matching pattern and one or more rewriting patterns. In the next section  , we will see that estimating the intended path from an incomplete sequence of the subject's motion even after it is started holds technical utility. Here  , the problem of estimating an intended path pattern is defined as a pattern matching problem from incomplete data sequence of a human motion in its early stage. Each fragment matching a triple pattern fragment is divided into pages  , each page contains 100 triples. The key is a triple pattern fragment where the predicate is a constant and the value is the set of triples that matches the fragment 16. Thus  , by saving the 3D edge identifiers in dlata points of a CP pattern  , correspondence between the model edges and the image edges can be obtained after matching. However  , the data points of the CP pattern are related to a corresponding edge of a CAD model. This is done without any overhead in the procedure of counting conditional databases. The difference of CMAR from other associative classification methods is that for every pattern  , CMAR maintains the distribution of various class labels among data objects matching the pattern. Threshold-based approaches consider an event to occur when sensor readings exceed a pre-defined threshold value. In the first case  , the Triplify script searches a matching URL pattern for the requested URL  , replaces potential placeholders in the associated SQL queries with matching parts in the request URL  , issues the queries and transforms the returned results into RDF cf. when a URL in the Triplify namespace is accessed or in advance  , according to the ETL paradigm Extract-Transform-Load. That allowed us to achieve the purpose of this method which was the extraction of a much larger number of matching points than in the previous method. A truly robust solution needs to include other techniques  , such as machine learning applied to instances  , natural language technology  , and pattern matching to reuse known matches. While we believe we have made progress on the schema-matching problem  , we do not claim to have solved it. N-grams of question terms are matched around every named entity in the candidate sentences or passages and a list of named entities are generated as answer candidate. contiguous and non-contiguous combinations of words are generated and ranked in the descending order of their length. In addition to weighting the importance of matching data in the high-information regions  , it would also be appropriate to weight the most current data more strongly. The described general procedure for pattern matching could utilize the entire history of data acquired durin g an assembly attempt. Characteristics of projective transformation is also utilized to perform correspondences between two coordinate systems and to extract points. The pattern matching for the rules is done by recursive search with optimisations  , such as identifying an optimal ordering for the evaluation of the rules and patterns. For inferencing Cwm uses a forward chain reasoner for N3 rules. Like ML  , it has important features such as pattern matching and higher-order functions  , while allowing the use of updatable references. JunGL is primarily a functional language in the tradition of ML. The advantages of this type of programming language in compiler-like tools is well-known 1. XOBE is an extension of Java  , which does support XPath expressions  , but subtyping is structural. Navigation of XML values in Xtatic is accomplished by pattern matching  , which has different characteristics than those of XPath expressions. But they cannot combine data streams with evolving knowledge  , and they cannot perform reasoning tasks over streaming data. As mentioned above  , current EP systems 1  , 6  , 8  do real-time pattern matching over unbound event streams. Secondly  , having a more accurate selection in an incremental transformation allows minimizing the instructions that need to be re-evaluated. Clearly  , video indexing is complex and many factors influence both how people select salient segments. This also makes automatic summarization easier because human voices can be easily recognized and pattern matching should be useful for recognizing many natural sounds. The Jena graph implementation for non-inference in-memory models supports the look-up for the number of triples matching either a subject  , a predicate or an object of a triple pattern. We use such information to compute selectivities. 630 where Φ 1 and Φ 2 are relations representing variable assignments and their annotations. The following definition will specify how complex formulae from F  , which serve as annotations for results of matching complex graph pattern  , will be derived. In standard SPARQL query forms  , such as SE- LECT and CONSTRUCT  , allow to specify how resulting variable bindings or RDF graphs  , respectively  , are formed based on the solutions from graph pattern matching 15 . Query forms. We proposed VERT  , to solve these content problems   , by introducing relational tables to index values. Traditional twig pattern matching techniques suffer from problems dealing with contents  , such as difficulty in data content management and inefficiency in performing content search. In addition to the data provided by Zimmermann et al. 31  , extracted the data from the Eclipse code repository and bug database and mapped defects to source code locations files using some heuristics based on pattern matching. The argument p is often called a template  , and its fields contain either actuals or formals. Another related work is a recent study in 2 on approximate string joins using functions such as cosine similarity. The remainder of this paper is organized as follows. The FiST system provides ordered twig matching for applications that require the nodes in a twig pattern to follow document order in XML. The FSM stores partial results as the document is parsed sequentially in document order. Thus  , treating a Web repository as an application of a text retrieval system will support the " document collection " view. in conjunction with query languages that enable keyword querying  , pattern matching e.g. , regular expressions  , substrings  , and structural queries 2. The set of common attributes is preconfigured as domain knowledge  , which is used in attribute matching as well. Therefore  , by incorporating this pattern in the grammar  , the same form extractor automatically recognizes such exclusive attributes. A large body of work in combinatorial pattern matching deals with problems of approximate retrieval of strings 2  , 11. 16 study how to estimate selectivity of fuzzy string predicates. We designed our method for databases and files where records are stored once and searched many times. We describe a novel string pattern matching principle  , called n-gram search  , first proposed in preliminary form in 10. In the general computer science literature  , pattern matching is among the fundamental problems with many prominent contributions 4 . 15 propose a different method that trades search capability for much less security. They are sorted according to question type and can handle more anchor terms. However  , datadriven techniques Section 5 offer additional protection from false or extraneous matches by lowering the importance ranking of information not corroborated elsewhere in the data. From all these images  , the software mentioned above detected matching points on the calibration pattern for each pan and tilt configuration. Stereo images at different pan and tilt angles were captured. Most current models of the emotion generation or formation are focused on the cognitive aspects. The one extracts a cognitive image aimed at pattern matching  , and the other creates a perceptual imagelO  , 111. Systems like EP-SPARQL 4 define pattern matching queries through a set of primitive operators e.g. the context of SFP  , the query model is a discriminating property between systems. Thus question answering cannot be reduced to mere pattern matching  , but requires firstorder theorem proving. Though the proposed annotations have a simple structure  , background knowledge is complex  , and in general involves quantification  , negation  , and disjunction. Further  , research methods and contextual relations are identified using a list of identified indicator phrases. In information extraction  , important concepts are extracted from specific sections and their relationships are extracted using pattern matching. More like real life.. pattern matching using the colours can be used for quicker reference. " Many positive comments were made about the opportunity of using colour to discriminate between tabs  , e.g. " In parallel  , semantic similarity measures have been developed in the field of information retrieval  , e.g. In the fields of image recognition and general pattern matching  , geometric similarity measures have been a topic of study for many years 9. First  , the new documents are parsed to extract information matching the access pattern of the refined path. Adding new documents to the refined path index is accomplished in two steps. We compared the labels sizes of four labeling schemes in Table 2. As we will show in our experiments  , it is worth using this additional space-overhead  , since it significantly improves the performance of XML twig pattern matching. Finally  , K query partitions are created by assigning the queries in the i th bucket of any pattern to query partition i. If the query involves multiple patterns  , it is randomly assigned to one of the matching buckets. When a new instrument is created matching the the pattern  , a notification is sent to GTM which in turn creates the track.2 To accomplish creation of inventory on future patterns   , a trigger as implemented in DBAL is defined . It entails a match step to find all rules with a context pattern matching the current context. Selection rules allow a straight-forward and efficient implementation of recommender selection. Building on the suffix array   , it also incorporates ideas embedded in the Burrows-Wheeler transform. The FM-INDEX  , introduced by Ferragina and Manzini 6  , is a data structure used for pattern matching. The idea is to use that view to model pat t em-mat thing queries  , which we impose to have flat structure. Given an external concept  , we perform a pattern matching on the thesaurus  , made of the following operations : a-1 inclusion step : We look for a thesaurus item i.e a clique which includes the given group. a The transformation step :. The resulting fingerprint for Sildenafil is 1100. Therefore  , it is effective in giving the number n of unmatched characters permitted on pattern matching. Generally  , it is useful to deal mechanically with a misspelling  , an inflection and the different representation such as 'three body' and 'three-body'. Thus  , the larger the text collection is  , the greater the probability that simple pattern matching techniques will yield the correct answer. Naturally  , this simple technique depends crucially on the corpus having an answer formulated in a specific way. A considerable number of NEs of person  , organization and location appear in texts with no obvious surface patterns to be captured. We used pattern matching to extract and normalize this information. Some of the demographic information  , such as gender  , age  , and specific conditions  , such as patients weight  , were only mentioned in the text. We have plans on generating classifiers for slot value extraction purposes. Furthermore  , on extracting slot values  , pattern matching might not be the best options but definitely can produce some good results at hand. This automatic slot filling system contains three steps. For the Streaming Slot Filling task  , our system achieved the goal of filling slots by employing a pattern learning and matching method. In evaluations  , we only vary the definition pattern matching module while holding constant all other components and their parameters. In our experiments  , the base definition generation system used is the system discussed in Section 2 and illustrated in Figure 1. Additionally  , ultrasonic diagnosis images were obtained for which pattern matching was performed to measure the virtual target position. At the time  , both the force acting on the needle and the displacement of the needle were measured. Normally  , the For the detection of the same object rotated around the z-axis of the image plane  , the template has to be rotated and searched from scratch. Detection time with angle increment 6 5 5 varies between 2-4 seconds. Pattern Matching In our case  , a highly optimized routine of the MATROX library 19  was employed using hierarchical search. In other words  , the object features used for pattern matching refer to the latter distribution. For a particular object template  , they consist of a representation of the distribution of the object's color histogram. We emphasize that nothing is encoded about how t o construct a successor node from a given node. Our stereo-vision system has been designed specifically for QRIO. If there are other peaks with similar matching scores then the disparity computation is ambiguous repeating pattern and the reliability is set to a low value. Each sign is recognized by matching the operator's finger positions to the corresponding pattern acquired during calibration. The operator communicates to the robot via four hand signs: point  , preshape  , halt  , and estop emergency stop. Each of the 41 QA track runs ~ ,vas re-scored using the pattern matching judgments. In addition  , assessors accepted various model numbers such as Peugeot 405s  , 309s  , 106s  , 504s  , 505s  , 205s  , and 306s. However they are quite often used probably  , unconsciously! The main challenge here has been that two sequences are almost never identical. The testing system of improved pre-decode pattern matching circuit is described in Figure 7. . The experiment environment is Xilinx Virtex4 xc4vlx200 which is synthesis by Synplify and is implemented by ISE. which the other components on this level rely. Typical examples of parameter estimators are pattern matching with video cameras; collision prediction; detection of task switching conditions; identification of dynamic parameters of the load of the system; etc. Each size of the model of quadrangle  , each location of the pattern matching model  , and the location of the center of iris are established. Fig.5shows an example of model location setting on the basis of the inputted eye image. An online pattern matching mechanism comparing the sensor stream to the entire library of already known contexts is  , however  , computational complex and not yet suitable for today's wearable devices. Therefore  , an ongoing monitoring of the sensor stream is needed. In general  , introducing uncertainty into pattern discovery in temporal event sequences will risk for the computational complexity problem. However  , within an uncertain interval   , the computational complexity for matching increases. The time points are identified for the best matching of the segments with pattern templates. Moreover  , the time points identified using different dlen are independent comparing Fig.l7a with Fig.17@ and Fig.lBa with Fig.l8@. Second  , we allow for some degree of tolerance when we try to establish a matching between the vertex-coordinates of the pattern and its supporting transaction. some of which are shown in Figure 2b. Entity annotation systems  , datasets and configurations like experiment type  , matching or measure are implemented as controller interfaces easily pluggable to the core controller. GERBIL abides by a service-oriented architecture driven by the model-view-controller pattern see Figure 1. Whenever a context change is detected  , the change is immediately examined to decide its influence on pat. In contrast  , each pattern  , say pat  , maintains a matching queue to store the last matched context instances i.e. , pat. words are mapped to their base forms thus completely solving the problem with the generation of plural forms. Unlike the approach presented in this paper  , PORE does not incorporate world knowledge  , which would be necessary for ontology building and extension. PORE is a holistic pattern matching approach  , which has been implemented for relation-instance extraction from Wikipedia. Most approaches applicable to our problem formulation use some form of pattern matching to identify definition sentences. The learned soft patterns are used to judge whether sentences are definitional. In a similar fashion  , it keeps track of the provenance of all entities being retrieved in the projections getEntity. For each molecule inspected  , our system keeps track of the provenance of any triple matching the current pattern being handled checkIfTripleExists. Similar to most existing approaches  , our information extractor can only be applied to web pages with uniform format. □ When matching a URL with a pattern there are three outcomes: These techniques have also been used to extend WordNet by Wikipedia individuals 21 . These approaches use information extraction technologies that include pattern matching  , natural-language parsing  , and statistical learning 25  , 9  , 4  , 1  , 23  , 20  , 8 . Multi-level grouping can be efficiently supported in V ERT G . After that  , in the second phase we use the table indices on values  , together with the result from pattern matching  , to perform grouping and compute aggregate functions. Details can be found in 26. From a matching logic perspective  , unlike in other program verification logics  , program variables like root are not logical variables; they are simple syntactic constants. Variables like  ?root are existentially quantified over the pattern  , while E  , T  , H  , C are free. The system scaffolds the creation of a transformation by automatically generating initial patterns from a textual selection in source code. The authors of the data set  , Zimmermann et al. All experiments reported in this section are conducted in a Sun Linux cluster with 20 nodes running CentOS 5  , x86_64 edition. Breakpoint preparation asks GDB to set a number of breakpoints on lines which could possibly correspond to events requested by a fget. If a token is found in a database  , this information is added to token feature. Our approach combines a number of complementary technologies  , including information retrieval and various linguistic and extraction tools e.g. , parsing  , proposition recognition  , pattern matching and relation extraction for analyzing text. In TREC 2003 QA  , we focused on definitional questions. The weight of the matched sub-tree of a pattern is defined by the formula: For the evaluation of the importance of partially matching sub-trees we use a scoring scheme defined in Kouylekov and Tanev  , 2004. We describe herein a Web based pattern mining and matching approach to question answering. The power of textual patterns for question answering looks quite amazing and stimulating to us. The system overview is shown in Fig.2. In the data set  , we are given 4 months of data October 2011 -February 2012 as training data. The traditional method employed by PowerAnswer to extract nuggets is to execute a definition pattern matching module. For this reason we used the semantic classifications generated by our named entity recognizer to discover such relations when looking for relevant passages. Third  , further work needs to be done for answering Other questions for events. For either representation  , we first drop unnecessary punctuation marks and phrases such as " socalled " or " approximately " . This subsection gives an overview of the basic ideas and describes recent enhancements to improve the recall of answer extraction. In 1   , we discussed our pattern matching approach in detail. Graph matching has been a research focus for decades 2  , especially in pattern recognition  , where the wealth of literature cannot be exhausted. In order to tackle graph containment search  , a new methodology is needed. Yet usually  , there are many possible ways to syntactically express one piece of semantic information making a na¨ıvena¨ıve syntactic " pattern matching " approach problematic at best. This would be less expensive than the semantic approach. Since they end with the word died  , we use pattern matching to remove them from the historic events. The dates of death serve as a baseline since they are likely represented by a single sentence in Wikipedia. Automatic music summarization approaches can be classified into machine learning based approaches 1 ,2 ,3 and pattern matching based approaches 4 ,5 ,6. Automatic music summarization and video summarization have attracted research activity in the past few years. An interesting goal of an intelligent IRS may be to retrieve information which can be deduced from the basic knowledoe given by the thesaurus. + trying to have an "intellioent" pattern matching : Further difficulties result from the occurrence of grammatical and spelling errors  , which are very common in unpublished communications 11. We take both patterns and test instances as sequences of lexical and syntactic tokens. When conducted on free texts  , an IE system can also suffer from various unseen instances not being matched by trained patterns. Here  , " Architecture " is an expression of the pattern-matching sublanguage. A book has an introduction  , a number of chapters  , a bibliography and chapter parent title same " Architecture "   , is the set of all chapters of all books titled " Architecture " . Other words in the question might be represented in the question by a synonym which will not be found by simple pattern matching. Proper nouns from the question are going to be represented in any paragraph containing a possible answer. We have shown that a mixed algebra and type model can be used to perform algebraic specification and optimization of scientific computations. Com* * Work partially funded by the EGov IST Project and by the Wisdom project  , http://wisdom.lip6.fr. Using it for pattern matching promises much higher efficiency than using the original record. The information contained in a single character in the CAS encoding includes information about all preceeding characters in the string. This is the biggest challenge of rewriting XSLT into XQuery. The main difference is however  , that XSLT templates are activated as a result of dynamic pattern matching while XQuery functions are invoked explicitly. The rules with the highest weights then indicate the recommenders to be applied. The third interaction module that we implemented is a rhythmic phrase-matching improvisation module. Arm i plays a phrase based on a probabilistic striking pattern  , which can be described as a vector of probabilities Normalized grayscale correlation is a widely used method in industry for pattern matching applications. A similar landmark is used in 7  , two concentric circles that produce in the sensor image an elliptical edge. This ensures that there is no simple pattern  , such as the query always precisely matching the title of the page in question. In other cases words were added or omitted. Although we endeavored to keep queries short  , we did not sacrifice preciseness to do so. At the end of this phase  , the logical database subset has been produced. This means that all data has to be imported and converted once  , making it less suitable for Web views. Modifying these lists is an easy task and was successfully carried out by non-expert users. In the Collocation matching activity  , students compete in pairs to match parts of a collocation pattern. LLDL is particularly useful for learning collocations because it contains a large amount of genuine text and provides useful search facilities. Listing 1 shows an example query. XSPARQL extends XQuery by two additional grammar expressions: the SparqlForClause to use SPARQL's graph pattern matching facility including operators  , and the ConstructClause to allow straightforward creation of RDF graphs. Then the position data are transmitted to each the satellite. The CCD camera installed over the flat floor detmnines the positions of the satellites by the pattern matching to markers drown on the satellites. For assessing pattern validity  , we use a simple measure based on the relative frequency of matching contexts in the context set. More specialized patterns have lower thresholds  , but are only induced if the induction of more general patterns fails. The procedure of creating start-point list is illustrated in Fig. Since the malicious part is encrypted  , the behavior of the active virus cannot be determined by program code checking. Others 51  , 32 can automatically infer rules by mining existing software; they raise warnings if violations of the rules occur. Most of the pattern-matching tools 10  , 14  , 13  , 9 require users to specify the buggy templates. Figure 2illustrates two patterns: 1 somebody enters Classroom 2464; 2 somebody is staying in some place.  We propose two optimizations based on semantic information like object and property  , which can further enhance the query performance. V ERT G inherits all the advantages of VERT  , including the efficiency in matching complex query pattern. Thus at the end of initialization  , each tp-node has a BitMat associated with it which contains only the triples matching that triple pattern. Note that all these operations are done directly on the compressed BitMats. In general we observed that a small but specific set of attributes are sufficient indicators of a navigational page. Note that we use rounded rectangles to depict extraction steps and hexagons to depict pattern matching steps. While Prolog is based on unification and backtracking  , B is based on a simple but powerful pattern-matching mechanism whose application is guided by tactics. 111 that sense  , it has a similar philosophy as a Prolog interpreter. The scope of these free variables is restricted to the rule where they appear just like for Prolog clauses. In this first rule  , X and Y are used as free variables for the pattern matching. For example  , one instrumentation rule states " Measure the response time of all calls to JDBC " . Second  , it would be useful to investigate customization solutions based on shared tree pattern matching  , once such technology is sufficiently developed. First  , we plan to support additional features such as ordering and aggregation in result customization. Leading data structures utilized for this purpose are suffix trees 11 and suffix arrays 2. For brevity  , we have omitted most of the components used to support keyword queries. For example  , Figure 1shows an example query plan for a path query in which some constraints involve standard graph pattern matching. For example  , suppose an input text contains 20 desired data records  , and a maximal repeat that occurs 25 times enumerates 18 of them. The matching percentage is used because the pattern may contain only a portion of the data record. In our work  , a rule-based approach using string pattern matching is applied to generate a set of features. Advanced features can be inferred using rulebased approaches or machine-learning approaches. 5  employed a simple method which defines several manuallyconstructed definition patterns to extract definition phrases. Using standard negation as failure here as in 4  , would let the bound negation succeed. For example  , the extended VarTrees and TagTrees of example Q1 and Q2 are depicted in Figure 6respectively. In reporting on KMS for TREC 2004  , we described in detail the major types of functions employed: XML  , linguistic  , dictionary  , summarization  , and miscellaneous string and pattern matching. This method only obtained 9 additional answers in TREC 2005. They are sorted according to question types and can handle more anchor terms. A list of over 150 positive and negative precomputed patterns is loaded into memory. The composition of the patterns  , the testing methodology  , and the results  , are detailed in Fernandes  , 2004. Examples of sentences from the corpus matching each pattern are shown in Figure 5  , with emphasis on targets from this year's competition . It does not have natural language understanding capabilities  , but employs simple pattern matching and statistics. Our QA system is constructed using methods of classical IR  , enhanced with simple heuristics. The idea of partial pattern matching is based on the assumption that the answer is usually surrounded by keywords and their synonyms. , " When was the first camera invented "   , etc. Geometric hashing 14 has been proposed aa a technique for fast indexing. Efficient indexing based matching of two and three dimensional 2D/3D models to their views in images has been addressed in computer vision and pattern recognition. So the translation between these constructs is straightforward. The strategy of the pattern-matching can be ruled by an action planner able to dynamically define partial goals to reach. -Application step : It consists in the execution of an elementary operation item substitution  , item comparison  , .. An extension of the cascade of Figure 19.6 in two dimensions with k cascades can be used to do pattern matching operations with respect to k distinct patterns. The system tries to infer new knowledge right after the publication operation. This principle will be applied decoupling the functional properties from the non functional properties matching. In the broker design  , we intent to create a discovery pattern that will be based on the well-known principle of the " separation of concerns " . The other extracts the structure in some way from the text parsing  , recognizing markup  , etc. One of them indexes the text to answer text pattern-matching queries this indexing is performed by the text engine. We conduct a series of extrinsic experiments using the two soft pattern models on TREC definitional QA task test data. In addition to our theoretical work  , we also assess the performance of the formal soft matching models by empirical evaluation. Definition pattern matching is the most important feature used for identifying definitions. Systems fielded at TREC rank definition sentences using two sets of features: definition patterns and bagof-words pertinent to the target. Providing formal models for modeling contextual lexico-syntactic patterns is the main contribution of this work. The models can help IE systems overcome difficulties caused by language variations in pattern matching. With the help of appropriate hardware  , it is easy to fast realize. We assume that the occurrence of significant patterns in nonchronological order is more likely to arise as a local phenomenon than a global one. In this work  , a significant pattern is obtained from the matching of a pair of sequences. The grep program searches one or more input files for lines containing a match to a specified pattern  , and prints out matching lines. We choose grep-2.2 as the subject program in this study. We can therefore define the notion of a strand  , which is a set of substrings that share one same matching pattern. For each substring  , the bounding boxes indicate the parts that match exactly with S 2 . In the tradeoff between space and time  , most existing graph matching approaches assume static data graphs and hence prefer to pre-compute the transitive closure or build variablelength path indexes to trade space for efficient pattern matching. This is known as the transitive closure of a graph. For instantiation   , we exploit an index as well as a pattern library that links properties with natural language predicates. In a next step  , c has to be instantiated by a matching class  , in the case of using DBpedia onto:Film  , and p has to be instantiated with a matching property  , in this case onto:producer. A data record is said to be enumerated by a maximal repeat if the matching percentage is greater than a bound determined by the user. Pattern inflexibility: Whether using corpus-based learning techniques or manually creating patterns  , to our knowledge all previous systems create hard-coded rules that require strict matching i.e. , matching slot by slot. These approaches have two shortcomings that we have identified and address in this work: 1. Although such hard patterns are widely used in information extraction 10  , we feel that definition sentences display more variation and syntactic flexibility that may not be captured by hard patterns. In addition to surface text pattern matching  , we also adopt N-gram proximity search and syntactic dependency matching. The answer patterns used in ILQUA are automatically summarized by a supervised learning system and represented in form of regular expressions which contain multiple question terms. N-grams of question terms are matched around every named entity in the candidate passages and a list of named entities are extracted as answer candidate. We empirically showed that these two search paradigms outperform other search techniques  , including the ones that perform exact matching of normalized expressions or subexpressions and the one that performs keyword search. We characterized several possible approaches to this problem   , and we elaborated two working systems that exploit the structure of mathematical expressions for approximate match: structural similarity search and pattern matching. The input sources include data from lexico-syntactical pattern matching  , head matching and subsumption heuristics applied to domain text. This method converts evidence into first order logic features  , and then uses standard classifiers supervised machine learning on the integrated data to find good combinations of input sources. Afterwards  , the location of eye can be measured by detecting a agreement part with the paltern matching model in the eye image input. At first  , the pattern matching model is memorized on the basis of eye image which was captured previously. In this paper  , we have proposed  , designed and implemented a pattern matching NIDS based on CIDF architecture and mature intrusion detection technology  , and presented the detailed scheme and frame structure. It speeds up the matching and significantly increases the detection speed of IDS. In a first pilot study 71  , we determined whether the tasks have suitable difficulty and length. The syntax errors we introduced can be located without understanding the execution of the program; they merely require some kind of pattern matching. The relationship between context instances and patterns is called the matching relation  , which is mathematically represented by the belong-to set operator . Intuitively  , each pattern categorizes a set of context instances. Wang et al 41 have presented an approach called Positive-Only Relation Extraction PORE. With that improvement one can still write filenames such as *.txt. We first have to introduce an additional XPath function Named match to allow Unix filename pattern matching within XPath. The matching problem is then defined as verifying whether GS is embedded in GP or isomorphic to one or more subgraphs of GP . We denote GP as the publication graph and GS as the subscription graph pattern. All the possible axes were permitted except ancestor  , ancestor-self  , following and preceding. In the following section  , we will describe two techniques that we have introduced to minimize the number of these instructions. We simply take their common prefixes as patterns since different parameters are usually at the end of URLs. One class of approaches focuses on extracting knowledge structures automatically from text corpora. In order to define these two functions we need the statistics defined in Table 1 . Consequently   , for i ≥ 1  , we estimate the cost of matching a pattern as: costpi = f rontierpi−1 × explorepi. It is less restrictive than subgraph isomorphism  , and can be determined in quadratic time 16. To reduce the complexity and capture the need of novel applications  , graph simulation 16 has been adopted for pattern matching 5  , 13. // " -axis query and documents with recursively appearing tags  , file scan is neither efficient  , nor effective to return correct answers. For example  , we use the POS tag sequence between the entity pairs as a candidate extraction pattern. Our extraction patterns are based on both the general POS tags and the strict keyword matching. KIM has a rule-based  , human-engineered IE system  , which uses the ontology structure during pattern matching and instance disambiguation. For new previously unknown entities  , new instances are added to the semantic repository. In order to express extractions of parts of the messages a pattern matching approach is chosen. The conditional equations use the binary function equala  , b which is a predefined expression of TPTP syntax and represents the equality relation. 2 Specification based on set-theoretic notations. So we can retrieve related information by pattern matching using a subspace as a unit actually with some generic information in knowledge structure which contains more information than a predicate in logical formulas. Seven propositions  , or " patterns " in were found. A pattern matching technique was used  , in which several pieces of information from one or more cases are related to a theoretical proposition. In typical document search  , it is also commonly used– e.g. , a user can put " " around keywords to specify matching these keywords as a phrase. On the one hand  , such pattern restriction is not unique in entity search. Our FiST system matches twig patterns holistically using the idea of encoding XML documents and twig patterns into Prüfer sequences 17. used ordered pattern matching over treebanks for question answering systems 15. Since the automata model was originally designed for matching patterns over strings  , it is a natural paradigm for structural pattern retrieval on XML token streams 7  , 8  , 4. State-of-the-Art. Then  , this information is encoded as an Index Fabric key and inserted into the index. Users can request creation of a track by giving patterns for instrument names. When a new instrument is created matching the the pattern  , a notification is sent to GTM which in turn creates the track.2 The algebraic properties of AS allow us to quickly calculate the AS of an n-gram from the CAS encoded record. To achieve this goal we should re-formulate queries avoiding " redundant " conditions. The efficiency of the matching operation greatly depends on the size of the pattern 8  , so it is crucial to have queries of minimum size. This allows us to detect if the equation contains certain types of common algebraic structures . Here  , pattern matching can be considered probabilistic generation of test sequences based on training sequences. Previously  , a list of over 200 positive and negative pre-computed patterns was loaded into memory. We rely on hand-crafted pattern-matching rules to identify the main headings  , in order to build different indices and allow for field-based search. The documents contain different sections  , with their corresponding headings. To identify the target of a question  , pattern matching is applied to assign one of the 18 categories to the question. The two missing categories what:X and unknown will shortly be dis- cussed. Two-stage hill climbing 5.2.1. T o obtain a successor node during hill climbing mode  , the following steps are taken. Let the cmt at any node m for hill climbing. In the hybrid SSH  , localization by hill-climbing is replaced by localization in an LPIM. following and hill-climbing control laws  , moving between and localizing at distinctive states. Two hill climbing scenarios are considered below. 8shows a modified Pioneer 3-AT at the bottom of a hill attempting to climb the hill. The hill climbing method generates solutions very fast if it does not encounter deadends. This measure is then used for a search method similar to the hill climbing method. This experiment validates the effectiveness of the weighted LHS combined with the Smart Hill-Climbing. Furthermore   , the final result of the search is better than that of Smart Hill-Climbing with LHS. percolation "  ? .. -the way this task can bc achicvcd : " hill-climbing " gradient methods  ? " edges  ? 12 and 13show the concave and convex transition of climbing up hill respectively. Figs. hill there may exist a better solution. Accepting Qud moves corresponds I ,O a " hill climbing " IC91: on the other side of IJtc! robot path PhMing. Hence  , the solution most likely converges to local minimum. Hill-climbing method is used for its simplicity and effectiveness. 4. GA optimization combined with simple hill climbing is used to improve gaits. Not all selected Fig. Finally  , it describes how SBMPC was specialized to the steep hill climbing problem. Next  , it disusses the benefits of SBMPC. Hill climbing starts from a random potentially poor solution  , and iteratively improves the solution by making small changes until no more improvements are found. In both cases  , concave and convex transition gait are performed sequentially. The one is climbing up the hill with 35 degrees of the slope and the other is the going down the hill. The detailed tracing results show that hill-climbing started from choosing topfacets and gradually replaced similar facets by less similar ones. Although hill-climbing had a slightly worse target article coverage than the other two 5% less  , it outperformed them in pair-wise similarity which means the facets selected have smaller overlap of navigational paths. However  , even if we combine DP with hill-climbing  , the planning problem is not yet free from combinatorial explosion . In 8  , we analyzed a simple vision-motion planning problem and concluded that hill-climbing is useful to limit a search space at each stage of DP. However   , we adjust all the weights in a WNB simultaneously  , unlike the hill climbing method  , in which we adjust each weight individually. Like the hill climbing method  , we stop adjusting the weights when the increase between the current AUC and the previous AUC is less than a very small value ¯. The final facets selected by hill-climbing usually were still within the top 30%  , while the ones selected by random-were evenly distributed among the results from single-facet ranking. In the sequel we describe several alternatives of hill climbing and identify the problem properties that determine performance by a thorough investigation of the search space. The good performance of hill climbing motivates the current work  , since fast search for sub-optimal solutions is the only way to deal with the vast amounts of multimedia information in several applications. The general approach can be used to specify the vehicle velocity at the top of the hill in the steep hill climbing problem. This ongoing work will be reported in a future publication. Alternatively  , we can follow the hill climbing approach but it is computationally more expensive and requires more scans of the database 18. However  , unlike the hill climbing approach where all the points are reassigned to the clusters  , we do not reassign the points already assigned to the 'complete' clusters . Each experiment performed hill climbing on a randomly selected 90% of the division data. Ten experiments were performed with each of the two divisions. The hill-climbing match procedure typically requires about one minute. A SPARCstation 10 is used both for robot control and for relocalization. This hill-climbing search was conducted on COCOMO II data divided into pre-and post-1990 projects. A * search is therefore more computationally expensive on average than hill climbing. Portions of many different paths may therefore be explored before a solution path is finally found. The heuristical method can be enhanced with known methodologies such as hill climbing. We believe that much future work can be done. Finally  , a hill-climbing phase in which different implernentation choices are considered reintroduces some of the interactions. This effectively rules out all choice interactions in this phase. Figure 2 only shows the most often influential attributes; i.e. This procedure is formalized in Alg. In each hill climbing iteration  , we select the best grasp from N C l  until no improvement is achieved. As the robot climbed the hill  , it decelerated  , resulting in a continual decrease in velocity. 14shows the result for hill climbing using SBMPC  , which commanded the robot to accelerate to a velocity of 0.55 m/s at 3 s  , the time at which the vehicle was positioned at the bottom of the hill. Metaheuristic algo- rithms 9 are elaborate combinations of hill climbing and random search to deal with local maxima. The impracticability of examining every possible partition naturally leads to the adoption of a hill climbing strategy  , which essentially consists of iteratively rearranging existing partitions by moving individual objects to different clusters  , and keeping the new partition only if it provides an improvement of the objective function. The other dramatic effect is the time taken with hill-climbing; not only is it just a fraction of the time taken without hill-climbing  , it is very close to being a constant  , varying between 32- 42ps for this set of randomised motion parameters and hull sizes between 10 and 500. The sequence length here is that the average number of iterations per calculation is indeed quite close to 1. However  , no results have been produced for mixed level arrays using these methods. Computational search techniques to find fixed level covering arrays include standard techniques such as hill climbing and simulated annealing. For feature smoothing  , we found that it is valuable to apply different amounts of smoothing to single term features and proximity features 5. Feature weights are learned by directly maximizing mean average precision via hill-climbing. The current implementation of the VDL Generator has been equipped with a search strategy adopting the dynamic programming with a bottom-up approach. dynamic programming  , greedy  , simulated annealing  , hill climbing and iterative improvement techniques 22. This prompts a need to develop a technique to escape from local minima through tunnelling or hill-climbing. The path formed at the local minima may not be collision-free and may be much longer than the optimal one. Three basic search techniques are combined to perform the search through the octree space. In our experiments  , the parameter pair Second  , we use the hill-climbing a1 orithm and the crossover-swapping operator in paralfel. The performance of EVIS on Lawrence's instances is shown in Table 2 Although it is not possible to avoid deadends completely during the search  , we can minimize the probability of encountering deadends based on the measure developed here. Such a path always exists for a connected graph. Therefore  , we propose as an " optimal " path the one obtained by a hill-climbing method with Euclidean distances as the metric for edge weight. Further parallelization is possible by batching up all the states to be evaluated in a single optimizer step. This allows us to use iterative hill-climbing approaches  , such as coordinate ascent  , to optimize the classifier in under an hour. For performance reasons  , the iterative medoid-searching phase is performed on a sample using a greedy hill-climbing technique. The Manhattan Distance divided by the subspace dimension is used as normalized metric for trading between subspaces of different dimensionality. After this iterative search  , an additional pass over the data is performed for refinement of clusters  , medoids and associated subspaces. 11shows the result for hill climbing using SBMPC  , which commanded the robot to back up and then accelerate to a velocity of 0.55 m/s at 1.5 s  , a velocity maintained until approximately 2.3 s  , the time at which the vehicle was positioned at the bottom of the hill. When the objective function has an explicit form  , Hill-climbing could quickly reach an optimal point by following the local gradients of the function. The first three are generally applicable as they require little a priori knowledge of the problem. One approach to reducing the number of choice interactions that must be considered is described by Low 'Low  , 1974. If the stopping condition is not met  , the framework will use a hill-climbing strategy to find a new value for N and a new iteration will start. This performance metric is compared with the target value. Two very important parts of this formulation  , which are often overlooked or not present in similar models  , are feature weighting and the feature smoothing. Now that the model has been fully specified  , the final step is to estimate the model parameters. Since our parameter space is small  , we make use of a simple hill climbing strategy  , although other more sophisticated approaches are possible 10. We now describe a technique that incorporates hill-climbing and is roughly We assume that which vertices are adjacent to each vertex is pre-computed and stored as a part of the polyhedron representation. Mobile manipulators may have difficulties for the stability in climbing up a hill  , maneuvering on unstructured terrain  , and fast manipulation. Basically  , however  , the stability problem of the whole system is very important. The Spatial Semantic Hierarchy SSH 2 The basic SSH explores the environment by selecting an alternating sequence of trajectory. The JUKF functioned as expected. For the few times that the position uncertainty became too large  , we were able to re-estimate initial positions using hill-climbing and GSL. The transformation that produces the best match is then used to correct the dead reckoning error. We have developed a technique that uses a hill-climbing search to match evidence grids constructed at the same estimated position at different times. High and low values were chosen empirically based on reasonable values for level ground and hill climbing. The parameter variation experiments were conducted on level ground and at a moderate slope of 8 degrees. All parameter values are tuned based on average precision since retrieval is our final task. At the current stage of our work  , the parameters are selected through exhaustive search or manually hill-climbing search. In experiments  , we find an appropriate ¡Û value manually for each dataset. Expert knowledge can be included in the methods  , and the definition of the problem can be changed in different ways to reflect different user envi- ronments. Overall  , hill-climbing helps us reducing overlapping facets without losing much coverage of target articles. Therefore the fanout of internal nodes and the length of navigational paths are within a reasonable range for the users. For each sentence-standard pair  , we computed the soft cardinalitybased semantic similarity where the expert coreness annotations were used as training data. Additional parameters are tuned by running a hill-climbing search on the training data. There exist two general approaches: the hill-climbing approach based on the MDL score 16  , 23  , the prevalent  , more practical one which is used here  , and the constraint-based approach. However  , the general problem is NP-complete 4. First  , it can localize unambiguously at any pose within the LPM rather than relying on the basic SSH strategy of hill-climbing to an unambiguous pose. In return  , the robot obtains two substantial benefits in terms of its spatial knowledge. The method of simulated annealing provides suck a technique of avoiding local minima. As there is no analytical method available for the solution of differential equations  , the problem is solved by numerical method. In our approach to GSL  , data patterns are first matched to HEC cluster patterns through hill-climbing 8201. In general  , the initial first-and second-order statistics are estimated through global self-localization GSL. Then mobile robots can plan motion using the multi-functional and efficient traversability vector t-vector obstacle detection model 6. To reduce the computational cost  , pruning using problem specific constraints is necessary. ORCLUS 3  , finds arbitrarily oriented clusters by using ideas related to singular value decomposition. PROCLUS 2 seeks to find axis-aligned subspaces by partitioning the set of points and then uses a hill-climbing technique to refine the partitions. To identify modes  , all data points are taken as starting points and their location is updated through a sequence of hill climbing step. The latter approach was chosen in this paper because it avoids representing the high-dimensional feature space. Only those data points that have a density exceeding the noise threshold before beginning the hill-climbing are assigned to a cluster center. Assignment to a cluster center is achieved using hillclimbing on the same density landscape. Otherwise  , the attributes in the non-stale set are selected as being influential on the score. We usually settle at a maximum within 15–25 iterations: Figure 3shows that Jα quickly grows and stabilizes with successive iterations. The hill-climbing approach is fast and practical. For the following discussion  , we assume medium or large nonindexed images and unrestricted variables. Earlier authors have considered instead using hill-climbing approaches to adjust the parameters of a graph-walk 14. In this paper we propose the use of learned re-ranking schemes to improve performance of a lazy graph walk. 1for the robot is generated between the two node positions. If a local miminum is reached  , A * search is invoked  , beginning at the point at which hill climbing got stuck see Fig. In many cases the contact positions had to be heavily adjusted to fulfill reachability. The dotted lines indicate the path each contact took in 3D space during the iterated refinement and hill climbing steps. Since our method has only 3 parameters  , we calculated their optimal setting with a simple coordinate-level hill climbing search method. For the baseline method the association score between the document and any candidate mentioned is always equal to 1.0. Tuning Interrelated Knobs: We may know of fast procedures to tune a set of interrelated knobs. We can now focus on these type-II knobs  , and perform hill climbing to obtain a potentially better knob configuration. Note  , that this maximization is a special case of the maximization of the posterior 3  , just that the likelihood becomes a constant. This can be done by hill climbing as well. We run preliminary experiments on a small scale system to validate that the theoretical results hold. Applying a hill-climbing strategy for workload intensity along the stress vectors  , we are able to reach the stress goal. That figure shows the percentage of times an attribute was selected by a N =4 hill climbing search. Figure 2suggests that we do not have such a " large enough " database. Hill climbing has the potential to get stuck in a local minimum or freeze  , so stopping heuristics are required. This allows us to randomly walk around ¦  , without reducing the goodness of our current solution. Table 8compares results for some fixed level arrays reported in 22 . Simulated annealing consistently does as well or better than hill climbing  , so we report only those results for the next two tables. These observations support Joachim's experience that the VC-dimension of many text Train  , c = −1 Test  , c = −1 "money-fx.lf" "money-fx.af" We then perform a hill-climbing search in the hierarchy graph starting from that pair. If this simple test fails  , we randomly sample the cache and identify a pair in the sample whose distance is closest to the required one. In order to comprehend the behavior of hill climbing under different combinations of search strategies  , we first study the search space for configuration similarity. Figure 2shows b 12 variables and thus does not necessarily guarantee an optimal path in the shortest path sense. Remolina and Kuipers 13  ,  151 present a formalization of the SSH framework as a non-monotonic logical theory. A gateway is a boundary between qualitatively different regions of the environment: in the basic SSH  , the boundary between trajectory-following and hill-climbing applicability. This is the criterion used in the examples in Figures Each gateway has two directions  , inward and outward. For the refinement step  , we apply a greedy hill climbing procedure explained in Sec. We stop coarsening the mesh before it degenerates and then apply a random initialization of contacts. This energy could be employed for hill climbing or long jumping  , or converted to vertical motion in a " pole vaulting " mode. In reality  , the hopper may be able to store substantial additional energy due to its horizontal motion. Accepting bad moves corresponds to perform what is called a hill climbing: on the other side of the hill there may exist a better solution. Then  , the method Proceedings of the 17th International Conference on Very Large Data Bases acceptAction uses Prob  , which is a boolean function that returns true with a probability that depends on temp and the costs of the compared states  , usually e ~~s'~cost~s~cost~~temP. This commanded velocity profile resulted in the vehicle's front wheels reaching the top of the hill at approximately 4.1 s. A time-lapse sequence of the motion with and without SBMPC is shown in Figure 12. All the other runs got stuck in an infeasible local maximum. In the experiments for this problem  , only 8 out of 480 single start statistical hill-climbing runs 6 hours on one Sparc 20 per run converged to a feasible solution-that is approximately 1.7%. Section II describes the dynamic model used in this research  , which was developed in 5 and emphasizes important model features that enable it to be used for motion planning in general and the steep hill climbing problem in particular. Second  , it constructs a complete representation of the paths at the place  , and hence of the dstates and possible turn actions. The hill climbing search strategy modifies the position of one fixel at a time until arriving at a fixel configuration achieving simultaneous contact and providing force closure with the feature tuple. In this section  , we describe a heuristic search strategy for finding a fixel configuration for a particular feature tuple. Deletion of tuples is performed symmetrically  , from the leaves to the root  , updating each concerned summary to take into account tuple deletion. The so-called hill-climbing search method locally optimize the summary hierarchy such that the tree is an estimated structure built from past observations and refined every time a new tuple is inserted. The β values are tuned via hill climbing based on the hybrid NDCG values of the final ranking lists merged from different rankers. In CS-DAC  , several rankers are trained simultaneously  , and each ranking function f * k see Equation 3 is optimized using the CS- DAC loss function and hybrid labels. Since both energy functions can be locally minimized by preserving the overlap  , a definite hill climbing is involved. The non-overlapping modules corresponding to the initial configuration lie inside the loop while those corresponding to the final Configuration lie outside the loop. It should be noted that local optimizing techniques  , such as hill climbing  , cannot be used here to find the global optimum  , due to the presence of local extrema. Like a random search  , a global optimum will be produced in the limit as ng-wo. Despite the great deal of motion planning research  , not much work has been done directly on the area of pushing planning. However  , it has a few limitations  , such as the fact that it is based on a hill climbing search  , which seem to make it unsuitable for our domain. surface are iden tifiedand counted as rocks for inclusion in the roughness assessment. In effect  , targets that differ from the ground 'The F uzzy Bversability Index also depends on the wheel design and traction mechanism of the robot which determine its hill clim bing and rok climbing capabilities. Surprisingly  , although ensemble selection overfits with small data  , reliably picking a single good model is even harder—making ensemble selection more valuable. Despite previous refinements to avoid overfitting the data used for ensemble hill- climbing 3   , our experiments show that ensemble selection is still prone to overfitting when the hillclimb set is small. While 10 uses a feature space grid to assist in the search for maxima  , 4 parses the table of data points for each hill climbing step. Note that hill-climbing strategies are currently the only ones that are compatible with LLA  , because statistical goodness-offit tests χ 2  require the compared models to be nested. Forward selection starts with a simple model usually all variables independent and iteratively adds terms accepting more complex hypotheses  , so long as there is sufficient evidence to accept new hypotheses. The method applies a " hill-climbing " strategy that makes use of a 3-D playing area measuring   , as visualised in the illustrations discussed above. To obtain these values  , we apply a procedure for identifying the threshold values that lead to the highest classification accuracy from a particular training set. In the latter case the hill-climbing procedure has been ineffective in escaping a poor local optimum. This is also the case for zoo and hepatitis  , and for mushroom  , where even a much larger data set includes misleading instances if a small support threshold is chosen. The average width and height of the facets generated by the three methods were about the same  , except that random-occasionally chose some much wider facets. In this technique  , the " bad quality " clusters the ones that violate the size bound are discarded Step FC7 and is replaced  , if possible  , by better quality clusters. The system performs the path search in an octree space  , and uses a hybrid search technique that combines hypothesize and test  , hill climbing  , and A ' This paper discusses some of the issues related to fast 3-D motion planning  , and presents such a system being developed at NRS. 12where it can be seen that despite random initialization  , our approach is capable to synthesize point contact grasps that comply to different reachability constraints. At this moment  , we have selected a value for all type-I and type-III knobs of S. Recall that some type-I knobs are actually converted from type-II ones  , which are ordered discrete or continuous. Thus  , the system does not adopt a purely agglomerative or divisive approach  , but rather uses both kind of operators for the construction of the tree. One can check whether the fitness function for the satellite docking problem exhibits this property by performing a large number of statistical hillclimbing runs 6. It can be noticed that climbing hills are not very well localised and that sometimes rocks are wrongly classified as steps down. The right image shows some small acceptable rocks on the right  , a 1 m to 20 crn deep from left to right step down at 5 m  , and a 45" hill at 10 m. Obstacle detection is quite reliable. This set of items is a complete description of what the mobile robot can see during its runs. We make the hypothesis that two or more of these situations cannot overlap e.g. , a small rock on the right side while climbing a big hill. Moreover  , it is worth noticing that  , since the search strategy and the application context are independent from each other  , it is possible to easily re-use and experiment strategies developed in other disciplines  , e.g. The procedure commences with initial support and confidence threshold values  , describing a current location   in the base plane of the playing area. Fig- ure 13shows the average characteristics of the faceted interfaces generated by these methods. The number of blocks remains constant throughout the hill climbing trial. A potential transformation is made by selecting one of the sets belonging to Ë and then replacing a random point in this -set by a random point not in the -set. The soft cardinalities a measure of set cardinality that considers inter-element similarities in the set of the two sets of stems and their intersection are used to compute the similarity of two given short text fragments. Several measurements were made to ascertain the quality of the various selection techniques  , as seen in Figure 1. All of the design and selection of the distance measures was done using hill-climbing on the development set  , and only after this exploration was All of the design and selection of the distance measures was done using hill-climbing on the development set  , and only after this exploration was In Figure 1we see both development and test set results for answer selection experiments involving a sample of the distance measures with which we experimented. By extracting the switching points from the model  , we are able to compute the stress vectors that yield a bottleneck change. Due to the absence of the training corpus  , the tuning of all parameters was performed on the testing data using a brute-force hill-climbing approach. As an exception  , the Probabilistic Translation Model was evaluated on the same representation that was used by Xu et.al.19. However  , the conventional G A applications generate a random initial population without using any expert knowledge. In other search engines such as Hill-Climbing  , it is clear that starting from a good location can significantly improve chances for convergence to an optimal solution in a much shorter time. Further  , we will replace the exponential moving average with an more efficient stochastic gradient hill climbing strategy. Future work will improve our distributed approach by optimizing floating point parameters of central pattern generators instead of discrete action or set-points in gaittables . This way it can significantly increase the number of prob­ lems for which a solution can be found. In this paper we present a randomized and hill-climbing technique which starts with an initial priority scheme and optimizes this by swapping two randomly chosen robots. In order for dead reckoning to be useful for mobile robots in real-world environments  , some means is necessary for correcting the position errors that accumulate over time. The presented data is taken from the above experiment and for the bunny object. For this  , we consider how many hill climbing steps the approach requires at each level and how many grasps need to be compared in each of these steps. 3represents the largest possible output power for one side of the vehicle  , which is 51 W. Generally speaking  , the torque limit constraint 5 is what causes deceleration when climbing a steep hill  , while the power constraint 6 limits the speed of the vehicle while traveling on either horizontal or sloped terrains. The square symbol in Fig. A particular classifier configuration can be evaluated over a set of over 10000 images with several lights per image by a few hundred computers in under a second.  The knowledge base is enriched by learning from user behaviors  , such that the retrieval performance can be enhanced in a hill-climbing manner. Therefore  , the results retrieved based on it are more relevant to the query than those retrieved by the CBR systems  , which rely on low-level features only.  The LGM provides a solid and generic foundation for multimedia retrieval  , which can be extended towards a number of directions. The knowledge base is enriched by learning from user behaviors  , such that the retrieval performance can be enhanced in a hill-climbing manner. The problems remaining are those of stability and reliability. The control problem can be problem of getting stuck in a local optimum which other Proceedings of the 17th International Conference on Very Large Data Bases hill climbing problems are faced with. These parameters can be divided into two kinds: the weights on the classes of words  , like people or locations  , and the thresholds for deciding if enough of the content is novel. We opted for a hill-climbing approach to find effective parameters for the system. In general  , the quality of solutions increases with density. In order to test this observation we ran experiments with the four variations of hill climbing 2 variable selection  2 value selection mechanisms using query sets of 6 and 15 variables over datasets of I000 uniformly distributed rectangles with densities of 0.1 and 1. Following six trajectories for each of ten rooms  , we observe that  , provided GSL is accurate  , the JUKF could repeatedly and reliably track the position and orientation of both vehicles. The WSJ  , FT  , SJMN  , and LA collections are used for testing whether the parameters optimized on AP can be used consistently on other collections. Therefore  , a simple coordinate-level hill climbing search is used to optimize mean average precision by starting at the full independence parameter setting λT = 1  , λO = λU = 0. An example mean average precision surface for the GOV2 collection using the full dependence model plotted over the simplex λT + λO + λU = 1 is shown in Figure 2. In this case it is advisable to choose the optimum slope which requires the nummum energy consumption. However  , in some cases it is important to evaluate the energy required per ascending distance  , which we denote by cost of climbing COC  , such as when presented with different paths to the peak of a hill. The relocalization subsystem then used hill-climbing to find the best match between these two grids and output the estimated error. Next the encoders were reset  , so the robot viewed the new location as the origin  , and a second evidence grid was built. During these experiments  , transient changes were present  , in the form of people moving past the robot as it constructed these evidence grids. A hill-climbing gradient ascent technique described independently by Sanderson 9 and Jarvis 4 is to compute the criterion function  , move the lens  , recompute the Criterion function  , and look at the sign of the difference of the criterion. This section describes a control strategy for automatically focusing on a point in a static scene. As can be seen  , the energy function corresponding to the optimal assignment metric yields ibetter results than the overlap metric in all cases. In general  , the fitness of the composite operator is adjusted as  By adjusting the operator fitness  , we balance the exploration of new search space and the exploitation of promising solutions found by the hill-climbing algo- rithm. This scheme is called parent replacement. Anyway  , the C parameter tuning is a very time and labor intensive work so that we need some automatic hill-climbing parameter calibration given enough computing power. For the feature sets  , combining the full text terms  , gene entities and MeSH terms is effective but even the combinations of two of them work reasonably well. We needed to index most of the content  , so indexing the content with partial noise was preferred to the one where some content blocks are unrecognized. The result was quite similar to the hill climbing heuristic  , but it skipped many important blocks in some of the cases. The ultimate goal of this work is the development of 3D machines that can cross rugged  , natural andl manmade terrains. The heading is then modified so that the robot moves towards the stronger reading. In order to maintain a heading close to the centre of the chemical plume the robot employs a hill-climbing strategy in which the robot turns to take sensor readings to the left and right of its current heading. In such situations  , the cost to the destination can be computed without using equation 3 and the recursive computation terminates. In extensive experiments it has been proven to be very effective even for large teams of robots and using two different dec au pled path planning techniques. For forward selection  , the generation of candidate alternatives to a current model relies on the addition of edges  , because graphical models are completely defined by their edges or two-factor terms. The method searches for the weights that correspond to the best projection of data in the ddimensional space according to S&D. To find a meaningful weighting of a specific set of d dimensions   , Dim  , for a given set of must-link and cannot-link constraints  , further referred to as S&D  , our approach performs hill climbing. This phase follows a hill climbing strategy   , that is  , in each iteration  , a new partition is computed from the previous one by performing a set of modifications movements of vertices between communities. The goal of this phase is to refine the partition received from the previous phase. Given ℐ −   , instead of exhaustively considering all possible element subsets of ℐ −   , we apply a hill-climbing method to search for a local optimum  , starting from a random -facet interface ℐ . This is in line with the idea of avoiding large overlap between facets Section 4.2. To increase the chance of forming a good solution we repeat the random walk or trial a number of times  , each time beginning with a random initial feasible solution. Since EIL for M CICM where the limiting campaign has high effectiveness property or for COICM in general are submodular and monotone  , the hill climbing approach provides a 1 − 1/e ap- proximation 10  , 36 for these problems. We leave a more extensive evaluation including such heuristics as future work. However the substantial time required and perhaps the complexity of implementing such methods has led to the widespread use of simpler heuristics  , such as hill-climbing 8 and greedy methods. If the size of the test suite is the overriding concern  , simulated annealing or tabu search often yields the best results . We shall examine normalized vectors to see if it helps for an easier parameter tuning. The small number of queries in the testing dataset precluded the use of any statistical significance tests. Rather  , it selects a successor at random  , and moves to that successor provided that there is an improvement of MP C. The computation usually halts when we have not been able to choose a better successor after a fixed number of attempts. Stochastic hill climbing does not examine all successors before deciding how to move. The first is how to utilize initial expert knowledge for a better and faster search routine. Given that the Meet space is unlikely to be convex  , there is no guarantee that this greedy hill climbing approach will find a global optimum  , but  , as we will show  , it tends to reliably find good solutions for our particular problem. Each single dimensional optimization problem is solved using a simple line search. For this reason  , we discriminatively train our model to directly maximize the evaluation metric under consider- ation 14  , 15  , 25. Under the experiment's conditions  , the maximum speed on smooth level ground was 4 2 c d s or approximately 2.5 body lengths per second. The goal was to apply SBMPC to the hill climbing problem in a computationally efficient manner. The related problems of traversing mud and high  , stiff vegetation are also of interest with the main issue being a technique for effective characterization of the vehicle-ground interaction. However  , one recursive coarsening step already improves results considerably over mere hill climbing on the original mesh at level 0. We observe a general trend showing that grasp quality is increased and variance reduced as the number of levels is increased. Since the experiment in the previous section shows that more levels in general lead to better expected grasp quality  , we have to investigate how the average and worst case complexity relate to the number of levels. Turbulence in the airflow produces a fluctuating chemical concentration at the robot. All of the timings in this section were done on a 120MHz Pentium PC running Linux  , and the code was compiled using the gcc compiler with optimisation turned on  , This figure illustrates clearly the usefulness of hill-climbing  , with the effect being most noticeable for larger hulls. The data-points plotted are times in ps for a complete distance calculation . Second  , we explore how ensemble selection behaves with varying amounts of training data available for the critical forward selection step. For large document clusters  , it has been found to yield good results in practice  , i.e. , the local optimum found yields good conceptual clusters 4. is a hill-climbing procedure and is prone to getting stuck at a local optimum finding the global optimum is NP-complete. The presentation emphasizes the importance of using a closed-loop model i.e. , one that includes the motor speed controllers to reduce the uncertainty of the tire/ground interaction  , the inclusion of the motor limitations  , and the ability of the model to predict deceleration when climbing a steep hill. This section describes the dynamic model of a skid-steered wheeled vehicle that was developed and experimentally verified in 8. This differs from the simple-minded approach above  , where only a single starting pose is used for hill-climbing search  , and which hence might fail to produce the global maximum and hence the best map. If the samples are spaced reasonably densely which is easily done with only a few dozen samples  , one can guarantee that the global maximum of the likelihood function can be found. We produce five queries with 9 variables  , and five with 12  , and for each query we generate 500 random solutions in a dataset of 1 ,000 uniformly distributed rectangles with density 0.5 density is defined as the sum of all rectangle areas divided by the workspace. The idea of considering both similarity and cost is motivated in Section 4.2.   , pagelinks.sql  , categorylinks.sql  , and redirect.sql  , which provide all the relevant data including the hyperlinks between articles  , categories of articles   , and the category system. As the goal function to be optimized in hill-climbing  , ℐ is considered better if the facets of ℐ have both smaller pair-wise similarities and smaller navigational costs than that of ℐ line 14. These latter effects probably account for the increase in average time per operation for the hill-climbing version to around 250-300ns; the difference in the code for these two methods is tiny. Some of this discrepancy will be due to the cost of the additional machine operations  , and on a modern small computer some of the time will be due to cache misses and pipeline flushes. When a local maximum is reached with a stepsize of 0.125 feet and 0.125 degrees  , the search is stopped and the resulting maximum is output as the transformation between the two evidence grids. The hill-climbing stepsize is initially set to 1.0 feet in translation  , degrees in rotation and is halved when a local maximum is reached  , in order to more precisely locate this maximum. Besides the discrete design variables  , the size of the search space is further increased by six continuously varying parameters defining the position and orientation of the space shuttle with respect to the satellite. In conclusion  , the TBD problem for the satellite docking operation is characterized by: a very large search space a high computation cost for evaluating the fitness of a a very small fraction of feasible designs a small probability of reaching these feasible designs through statistical hill-climbing. The speed limitations are expected to be particularly important when planning minimum time paths on undulating terrain. Given the vertex We can ensure that all of the vertices of the simplex found by GJK are surface points of the TCSO: when first added to the simplex vertex set we can do this by always generating them by opposing support vertices  , and at the next time step we can check the TC-space vertices that have remained in the simplex set by hill-climbing until we do find extrema1 vertices. The final solution to the optimization problem is a setting of the parameters w and a pruning threshold that is a local maximum for the Meet metric. Figure 4shows the average similarity of25 queries in each set retrieved over the two datasets every 50 seconds using a SUN Ultrasparc 2  , 200 MHz  , with 256MB of RAM. Figure 7a presents the performance of the predictive hill climbing approachPHCA and the degree centralityDegi  heuristic under various amounts of missing information for the case where the limiting campaign L is started with 30% delay. ratio of the number of the nodes saved using the respective method to the number of nodes that would be saved by the greedy method were we to have complete data Λ  , Σ  , Ξ. The expected log-likelihood 14 i s maximized using EM  , a popular niethod for hill climbing in likelihood space for problems with latent variables 2. Each new map is obtained by executing two steps: an E-step  , where the expectations of the unknown correspondences Ecij and Eci , are calculated for the n-th map eln  , and an M-step  , where a new maxinium likelihood map is computed under these ex- pectations. Finally  , note that we have assumed here that the coordinates of the object vertices are available on There is a catch though: whereas in visualisation we usually view from single directions  , in simulation we are likely to want to keep track of distances between many pairs of objects lo . We can ensure that all of the vertices of the simplex found by GJK are surface points of the TCSO: when first added to the simplex vertex set we can do this by always generating them by opposing support vertices  , and at the next time step we can check the TC-space vertices that have remained in the simplex set by hill-climbing until we do find extrema1 vertices. When the objects interpenetrate the origin of TCspace slips into the TCSO  , and GJK discovers a simplex almost certainly a tetrahedron containing the origin and within the TCSO. Correspondingly  , the cost of the outer query block can vary significantly depending on the sort order it needs to guarantee on the tuples produced. The cost of evaluating inner query block can vary significantly depending on the parameter sort order guaranteed by the outer query block. For a given nested query block  , several execution plans are possible  , each having its own required parameter sort order and cost. Further  , more than one query block can be nested under the same parent query block. However   , we have chosen to re-arrange bytes by the sort order of prefixes read right to left. Traditionally  , BWT rearranges bytes in a block by the sort order of all its suffixes. Results for such queries are shown in column TLC-O for the second group of queries q1-q2. 'Push Sort in Join': Pushing Sort into a Join applies to single block join queries. This approach avoids generation of unwanted sort orders and corresponding plans. Now  , the compatible combinations of plans and the effective parameter sort order they require from the parent block are as shown in Figure 5. This Sort should also simplify the Group operation that follows and associates to each researcher the number of projects it belongs to. In block B'Res  , a Sort operation is added to order the researchers according to their key number. The sort continuous in this manner until the list of items is fully sorted in ascending order after the lg m th phase. Similarly  , the second phase of bitonic sort involves merging each even-indexed 2- item block with the 2-item block immediately following it  , producing a list where consecutive 4-item blocks are sorted in alternating directions. While generating the plans for the nested blocks we consider only those plans that require a parameter sort order no stronger than the one guaranteed by the outer block. For each sort order  , we optimize the outer query block and then all the nested blocks. In this section we propose additional techniques for exploiting the sort order of correlation bindings by retaining the state of the inner query execution across multiple bindings of the correlation variables. Correspondingly  , the cost of the outer parent query block can vary significantly depending on the sort order it needs to guarantee on the tuples produced. Then we sort the set of average intensities in ascending order and a rank is assigned to each block. In our case  , we utilize 3×3 block of each frame for ordinal signature extraction. The BWT rearranges characters in a block by the sort order of the suffixes of these characters. Offsets are limited to a maximum value called the " window size " . To reduce the number of candidate plans we can adopt a heuristic of considering only the physical operators that requires the strongest parameter sort order less than the guaranteed sort order. However  , if the parameter sort order guaranteed by the parent block is weaker e.g. , null  , then only the plain table scan is a possible candidate. Depending on the delay condition  , HERB either simultaneously released the block no delay or waited until its head was fully turned and then released the block delay  , Fig- ure 2. The human-robot interactions lasted approximately 2 minutes and 20 seconds  , though the particular amount of time varied by how long the participant took to sort the block. Finally  , the block size for AIX is 2KB  , with Starburst assuming 4KB pages  , so each Starburst I/O actually requires two AIX I/OS.' For sorting  , Starburst does not use the global buffer pool  , relying instead on a separate sort buffer; we configured its sort buffer size to be lOOKI to provide a comparable amount of space for sorting as for regular I/O. A cost-based optimizer can consider the various interesting sort orders and decide on the overall best plan. For the table in Figure 3  , one might imagine that IP Address was used as a predictor for Client ID to some benefit because each user had a preferential computer   , shown below. This is a powerful effect: all prior sort orders are used to break ties this is because stable sort was performed for each block. However  , note that a sort-merge anti-join cannot be used if the correlated query block is part of a procedure or function where as NISR can be used in this case. A sort-merge anti-join implementation if present and used would perform exactly same as NISR and hence we have not consider it here explicitly. If this heuristic is adopted in the above example  , when the parameter sort order guaranteed from the parent block is {p 1 } only the state retaining scan is considered and the plain table scan is dropped. An approximated block matrix is generated when we then sort the eigenvectors and rearrange the eigenvector components accordingly before calculating the eigenprojector. Thus we do not need to set the number of clusters ex ante. Figure 8shows an example of this technique in action. Since the tuples within each block are sorted by timestamp  , a merge sort is employed to retrieve the original order of tuples across the different blocks in the run. In this approach we first traverse all the blocks nested under a given query block and identify the set of all interesting parameter sort orders. We describe this approach in subsequent subsections. Vo and Vo also showed that usage of multiple predictors for breaking ties in sort order often improves compression. This is logically equivalent to applying the permutation to all the tokens in the second block before running RadixZip over it. For queries where other factors dominate the cost  , like join q2  , the speedup is relatively small. In single block selection type queries x19 both TLC-D and TLC-O contribute by removing the blocking factor of DE and Sort. The rewrite applies only to single block selection queries. 'Push Sort in Select': We tested the efficiency of our rewrite that pushes Sorts into Selects  , as described in Section 5.2. The run block size is the buffer size for external Instead of sorting the records in the data buffer directly  , we sort a set of pointers pointing to the records. We consider LB to be the elementary block and we attempt to discuss the possibilities of fault tolerance in this program. This is the well known straight insertion sort. Since the matrices are hermitian  , the blocks are symmetric but different in color. We have implemented block nested-loop and hybrid hash variants. Anti-Semijoin For an anti-semijoin El I ? ,  , E2 all common implementation alternatives like sort merge  , hash  , and nested-loops come into account. This produces a list where consecutive 2-item blocks are sorted  , in alternating directions. Further assume query block q 2 nested under the same parent as q 1 has two plans pq 3 and pq 4 requiring sorts p 1   , p 2  and null respectively. Participants were also told that HERB's head would move and that HERB may provide suggestions about how to sort the blocks  , but that the final sorting method was up to them. They were instructed to take the block from HERB's hand once HERB had extended the block to them. When m is a power of 2  , bitonic sort lends itself to a very straight-forward non-recursive implementation based on the above description. For instance   , during the 4-merge phase phase 2 in the figure all compare-and-swaps performed within the first 4-item block are ascending  , whereas they are descending for the second 4-item block. Further  , the cost of the plan for the outer query block can vary significantly based on the sort order it needs to guarantee on the parameters. Note that the best parameter ordering for each query in the function body can be different and also there can be multiple functions invoked from the same outer query block. The necessary conditions to bundle operators within a block are: same degrees of parallelism and same partitioning strategies. In the PQEP shown in Figure 2c   , the largest block is formed by the sort  , projection proj  , group  , and hash-join hj ,i , operators having a DOP of 5. For illustration  , we will use the following block of variable-width tokens: Figure 5.1 shows the output of both BWT and RadixZip Transform run on this input. The minimum amount of main memory needed by Sort/Merge is three disk block buffers  , because in the sort phase  , two input buffers and one output buffer are needed. Given the fact that b/k blocks are needed in the fist phase  , and k blocks are needed in the second phase of the join  , the challenge is to find the value for k  , where the memory consumption maxb/k ,k is minimal : It is unfair for one sort to allocate extra memory it cannot use while others are waiting; l a sort whose performance is not very sensitive to memory should yield to sorts whose performance is more affected by memory space; l large sorts should not block small sorts indefinitely   , while small sorts should not prevent large sorts from getting a reasonable amount of mem- ory; l when all other conditions are the same  , older sorts should have priority over younger sorts. Specifically  , the following fairness considerations are reflected in our policy: l a sort should not allocate more memory than needed. To eliminate unnecessary data traversal  , when generating data blocks  , we sort token-topic pairs w di   , z di  according to w di 's position in the shuffled vocabulary  , ensuring that all tokens belonging to the same model slice are actually contiguous in the data block see Figure 1 . If suffixes provide a good context for characters  , this creates regions of locally low entropy  , which can be exploited by various back-end compressors. Besides SIMDization  , implementing bitonic sort efficiently on the SPEs also require unrolling loops and avoiding branches as much as possible. There is a change in the shuffles performed  , because the compare-and-swap direction is reversed for the second 4-item block. The final permutation 41352 represents the sort order of the five tokens using last byte most significant order  , and can be used as input to future calls to permute. Lemma 3.2. permute and its inverse are Ob time operations   , where b is the number of bytes in the block. Then the Hilbert value ranges delineated by successive pairs of end marker values in the sorted list have the prop erty that they are fully contained within one block at each level of each participating tree. We sort the full set Of 6Qj F values and delete any duplicates. The bottom-up approach can be understood by the following signature of the Optimizer method. In order to avoid optimization of subexpressions for sort orders not of interest the bottom-up approach first optimizes the inner most query block producing a set of plans each corresponding to an interesting order. In the logical query DAG LQDAG  , due to the sharing of common subexpressions  , the mapping of parameters to the level of the query block that binds it cannot be fixed statically for each logical equivalence node. Therefore if any sort order needs to be guaranteed on the output of the Apply operator an enforcer plan is generated. In each ordering we consider the first 5 blocks  , and for each block we calculate the maximum similarity to the 5 blocks on both the next and previous page. For each page  , we sort all blocks on the page in four different orders: from top to bottom  , from bottom to top  , from left to right  , and from right to left. Plan operators that work in a set-oriented fashion e.g. , sort  , might also be content with this simple open-next-close protocol  , which  , however  , may restrict the flexibility of their implementation. All " real " plan operators within a block access their relevant information via the opennext-close interface of the LAS cf. To understand this property  , consider the paradigm used by previous skyline evaluation techniques  , such as Block Nested Loops 4 and Sort-First Skyline 9 . An additional interesting property of the new lattice-based skyline computation paradigm is that the performance of LS is independent of the underlying data distribution. A cost-based optimizer can consider the various options available and decide on the overall best plan. This approach combines the benefits of both the top-down exhaustive approach and the bottom-up approach. Our last example see Figure 8 shows  , among other interesting features  , how one can push a Group that materializes the relationship between researchers and projects. A 6-axis force-torque sensor in the robot's hand identifies when the participant has grasped the block to begin the transfer phase of the handover. In this task  , a robot called HERB hands colored blocks to participants  , who sort those blocks into one of two colored boxes according to their personal preference. Our memory adjustment policy aims to improve overall system performance  , that is  , throughput and average response time  , but it also takes into account fairness considerations. The output of a single block FLWOR statement in XQuery can be ordered by either the binding/document order as specified in the FOR clauses or the value order as specified in the OR- DERBY clause. An outcome of our technique is that the Ordering Specification O-Spec of a collection and for that matter the SORT operation that produced it is a superset of the potential order that can be expressed by XQuery. The drawback of this approach is that it requires significant changes to the structure of any existing Volcano-style optimizer due to the need for propagating multiple plans for the same expression and then combining them suitably. Inference of " bounded disorder " appears to be relevant when considering how order properties get propagated through block-nested-loop joins  , and could be exploited to reduce the cost of certain plan operators. We also are interested in generalizing this work to infer " bounded disorder " : unordered relations whose disorder can be measured as the number of passes of a bubble sort required to make the relation ordered. At query time  , when OSCAR begins to scan a new run of blocks  , it uses the latest value returned by the r- UDF to only read from a corresponding fraction of the blocks in this new run. The size of the shared pool  , which is used by Oracle to store session information such as sort areas and triggers  , was set to 20MB and the size of the log buffer to 4MB to minimise the influence of Oracle internals on the measurements. The database buffer was set to 500 blocks with a database block size of 4 kbytes which resulted in an average buffer hit ratio of 98.5%. To the best of our knowledge  , the state-retention techniques and optimization of multi-branch  , multi-level correlated queries considering parameter sort orders have not been proposed or implemented earlier. Database systems such as Microsoft SQL Server consider sorted correlation bindings and the expected number of times a query block is evaluated with the aim of efficiently caching the inner query results when duplicates are present and to appropriately estimate the cost of nested query blocks. We use the log-likelihood LL and the Kolmogorov-Smirnov distance KS-distance 8 to evaluate the goodness-of-fit of and . The KS-distance as defined below In general  , a better fit corresponds to a bigger LL and/or a smaller KS-distance. The criterion used to1 detect this phenomena comes from the Kolmogorov-Smirnov KS test 13. Mitosis is essential because  , after some training  , there can be nodes that try to single-handedly model two distinctly different clusters. A more difficult bias usually causes a greater proportion of features to fail KS. We have thus demonstrated how the Kolmogorov- Smirnov Test may be used in identifying the proportion of features which are significantly different within two data samples. The tasks compared the result 'click' distributions where the length of the summary was manipulated. 3 These judgements were analysed with the two-sample Kolmogorov-Smirnov test KS test to determine whether two given samples follow the same distribution 15. D is the maximum vertical deviation as computed by the KS test. An * indicates that the Kolmogorov- Smirnov test did not confirm a significant di↵erence p > 0.05 between the indicated bin and the fourth bin. In all cases  , the PL hypothesis provides a p-value much lower than 0.1 our choice of the significance level of the KS-test. We use the Kolmogorov- Smirnov test KS  , whose p-values are shown in the last column of Table 3. The HEC utilizes the Kolmogorov-Smirnov KS test to determine the compactness of a data cluster 13  , and decide if a node should be divided mitosis to better model what might be two different clusters. Perhaps the best example of a  It also permits nodes which can represent topographical cues to be freely added and/or removed. Moreover  , two-sample Kolmogorov-Smirnov KS test of the samples in the two groups indicates that the difference of the two groups is statistically significant . The figures also clearly indicate that the density curve for Rel:SameReviewer is more concentrated around zero than Rel:DifferentReviewer for all three categories. Their methods automatically estimate the scaling parameter s  , by selecting the fit that minimizes the Kolmogorov-Smirnov KS D − statistic. Given the retrieval measurements taken for a particular query set and length  , we determined whether the retrieval effectiveness followed a power law distribution by applying the statistical methods by Clauset et al 3. To answer RQ1  , for each action ID we split the observed times in two context groups  , which correspond to different sets of previous user interactions  , and run the two-sample twosided Kolmogorov-Smirnov KS test 14 to determine whether the observed times were drawn from the same distribution. Experiment 1. Tague and Nelson 16 validated whether the performance of their generated queries was similar to real queries across the points of the precision-recall graph using the Kolmogorov-Smirnov KS Test. In order to establish replicative validity of a query model we need to determine whether the generated queries from the model are representative of the corresponding manual queries. It reaches a maximum MRR of 0.879 when trained with 6 data sources and then saturates  , retaining almost the same MRR for higher number of training data sources used. Figure 2 clearly shows that the Kolmogorov-Smirnov KS-test-based approach achieves much higher MRR than the other 4 approaches for all number of labelled data sources used in training. Users were asked in the post-task questionnaire which summary made the users want to know more about the underlying document . Similar to the Mann-Whitney test  , it does not assume normal distributions of the population and works well on samples with unequal sizes. We also considered the two-sample Kolmogorov -Smirnov KS Test 6  , a non-parametric test that tests if the two samples are drawn from the same distribution by comparing the cumulative distribution functions CDF of the two samples. The KS test is slightly more powerful than the Mann-Whitney's U test in the sense that it cares only about the relative distribution of the data and the result does not change due to transformations applied to the data. Our experiments on numeric data show that the Kolmogorov-Smirnov test achieves the highest label prediction accuracy of the various statistical hypothesis tests. While this difference is visually apparent  , we also ensure it is statistically significant using two methods: 1 the two-sample Kolmogorov-Smirnov KS test  , and 2 a permutation test  , to verify that the two samples are drawn from different probability distributions. In both cases  , suspended and deviant users are visibly characterized by different distributions: suspended users tend to have higher deviance scores than deviant not suspended users. If you assume that the two samples are drawn from distributions with the same shape  , then it can be viewed as a comparison of the medians of the two samples. The null hypothesis states that the observed times were drawn from the same distribution  , which means that there is no context bias effect. The exception to this trend is Mammography   , which reports zero correlation categorically  , as within each test either all or none of the features fail the KS test except for some MCAR trials for which failure occurred totally at random.