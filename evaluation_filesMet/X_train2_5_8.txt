We answer this question quantitatively in Section 6. However  , the key issue is doing this efficiently for practical cases. This optimization is performed first by noticing that the exponential loss En+m writes: The search of the ranking feature ft and its associated weight αt are carried out by directly minimizing the exponential loss  , En+m. Stack Search Maximizing Eq. The window provides us with a safety frame that guides the search in a promising direction. iv The large volume of ESI needed to be handled has also been known to lead to suboptimal performance with traditional IR solutions that may need to search hundreds or thousands of individual search indexes when performing an investigative search. 1 also indicate an exponential increase in the number of web services over the last three years. We tackle i using heuristic search -a well known technique for dealing with combinatorial search spaces. an exhaustive search is not practical for high number of input attributes. The search technique needs to be combined with an estimator that can quantify the predictive ability of a subset of attributes. As with any program synthesis technique which fundamentally involve search over exponential spaces  , the cost of our technique is also worst case exponential in the size of the DSL. In our experiment  , the search workload under the fixed workload scheme is set to be 2500 50 generations with 50 individuals in each generation  and is stipulated by workload function w = ϕ 2 in The time complexity may now become exponential with respect to ϕ as long as the workload function is an exponential function w.r.t ϕ. As in relational databases  , where the problem of large search space is mainly caused by join series  , in OODBMS the search space of a query is exponential according to the length of path expressions. The RAND-WALK agent impkments a completely randomized search strategy  , which has been shown to have a search complexity that is exponential in the number of state-action pairs in the system 2  , lo. With the exponential growth of information on the Web  , search engine has become an indispensable tool for Web users to seek their desired information. However  , it is never Copyright is held by the International World Wide Web Conference Committee IW3C2. However  , the problem of finding optimal plans remains a difficult one. For example  , our Mergesort branch policy still leaves an exponential search for worst-case executions. As we hypothesized  , the rate parameter of the exponential in Eq. A query task classification system was also employed  , based on 32 words indicative of home page search such as 'home' or 'homepage'. Watchpoint descriptions begin with a list of module names. These search based methods work only for low-dimensional systems because their time/space complexity is exponential in the dimension of the explored set. For a given sample data set  , the number of possible model structures which may fit the data is exponential in the number of variables ' . In our experience of applying Pex on real-world code bases  , we identify that Pex cannot explore the entire program due to exponential path-exploration space. Existing DSE tools alleviate path explosion using search strategies and heuristics that guide the search toward interesting paths while pruning the search space. We also embedded the collision detection method within a search routine to generate collision-free paths. Practically  , it is impossible to search all subgraphs that appear in the database. This reduces the computational complexity from 0  2 ~  to oN~ or from exponential computational time to polynomial computational time  121. Frequent closed itemsets search space is exponential to |I| i.e. To solve the problems optimally  , it requires an exponential search. Since the space is exponential in the number of attributes   , heuristic search techniques can be used. Zweig and Chang 43 found that the use of Model M exponential n-gram language model with personalization features improved the speech recognition performance on Bing voice search. The current Web is largely document-centric hypertext. Unlike the univariate approach  , the tuning of covariance matrix Q has an exponential search space  , since we need to simultaneously set all diagonal elements. 26 introduces a way to empirically search for an exponential model for the documents. the size of the search space increases in a strong exponential manner as the number of input attributes grows  141  , i.e. These conditions are easily checked  , but the exponential number of partitions m must be fairly large to allow decryption renders ex- haustive search impossible. In the worst case  , the search for all possible alliances in order to not miss any solution to the original problem reintroduces exponential complexity. The search of the ranking feature ft and its associated weight αt are carried out by directly minimizing the exponential loss  , En+m. When dealing with a human figure  , the notion of naturalness will come into consideration. Finding locally optimal solutions in this respect would be a logical approach and is the subject of current research. With about 32 degree of freedom DOfs to be determined for each frame  , there is the potential of exponential compl exity evaluating such a high dimensional search space. Theoretically  , the number of paths is exponential in the user-assigned search depth. Further advances in compositional techniques 26  , pruning redundant paths 7  , and heuristics search 9 ,40 are needed. Using an exponential distribution to accomplish a blending of time and language model Eq. From Table 1  , we can see that the search space for optimizing a path expression is exponential to the path length. The heuristic-search has the exponential computational complexity at the worst case. Because of the size of the graph  , this requires exponential time to solve using standard graph search techniques. However  , Grimson lo has shown that in the gencpal case  , where spurious m e a surements can arise  , the amount of search needed to find the hest interpretation is still exponential. A distributed e-library is perhaps best explained as a huge  , global database  , where search engines or directory services act as the indexes to information see  , Figure 11. To tame this exponential growth  , we use a beam search heuristic: in each iteration  , we save only the best β number of ungrounded rules and pass them to the next iteration. However  , for most practical problems  , solutions are easier to find and such search is not neces- sary. The organization of this paper is described as follows . Even though this bmte-force approach  , unlike the other work mentioned above  , guarantees optimality and completeness  , it k n o t practical for larger scale problems because of its computational complexity  , which is exponential in the number of moving droplets. As each evaluated state in the search requires execution of a collision detection method  , an efficient method will effectively reduce the magnitude of the base of the exponential relationship  , significantly improving the time performance of the search. This is especially important  , since the search space is exponential and the number of MDS patterns present in the data may also be very large. Due to its exponential complexity  , exhaustive search is only feasible for very simple queries and is implemented in few research DBMSs  , mainly for performance comparison purposes. Despite the exponential growth of Web content  , we believe the relevance of content returned by search engines will improve as query options will become more flexible. If the query optimizer can immediately find the profitable nary operators to apply on a number of collections  , the search space will be largely reduced since those collections linked by the nary operator can be considered as one single collection. An exhaustive search method that evaluates all the possible  i 0 values can require a total of r n combinations which is exponential with n and can require a large amount of calculation time. As the exponential growth of web pages and online documents continues  , there is an increasing need for retrieval systems that are capable of dealing with a large collection of documents and at the same time narrowing the scope of the search results not only relevant documents but also relevant passages or even direct answers. They adjust an exponential discount model to the expected quality of a search experience  , based on the session information. In modern query optimizer architectures FV94  , FG94  , different components are driven by different search strategies; thus  , it would be useful to have a special combination of strategies for optimizing path expressions . Early signs of such trends are visible with Google and Microsoft providing Twitter based search results for real-time events  , and exponential growth of tools like Yelp and Foursquare. The restricted search space has still an exponential size with respect to dimensionality  , which makes enumeration impossible for higher dimensionalities. In section 4 we show that for common scenarios there is significant benefit to nevertheless search for the best cost minimal reformulation. Unfortunately  , it is well known that the generation of the reachability tree takes exponential time for the general case. They use this model to generate a set of weights for terms from past queries  , terms from intermediate ranked lists and terms from clicked documents  , yielding an alternative representation of the last query in a session. If the moving direction keeps the same in the iterations  , the step increases faster than an exponential function and is given by iteration the search span at the moving direction  , a is the Fig. The approach to searching these huge spaces has been to apply heuristics to effectively reduce the extent of the space. Understandably  , model refinement implies exponential enhancement in the search space where the solution should be found. A simple chemical data set of 300 molecules can require many hours to mine when the user specifies a low support threshold. This occurs because a worst-case Mergesort execution must alternate between the two sides of a critical conditional  , but our generator can only capture that worst-case paths are always permitted to take either branch. This suggests that using the m most recent queries as the the search context for generating recommendations will likely introduce off-topic information  , causing recommendations that seem out of place. For taking the rank into consideration  , an exponential decay function with half-life α = 7 is proposed by Ziegler et al. Even then  , the exhaustive search is lirmted in the range and resolution of the weights considered  , and often has to be approximated by either gradient-descent or decomposmon techniques. The complexity of the planner is exponential on the number of joints  , and is of the order of Mn2nu   , where A4 is the discretization of the rectangular grid. A major challenge in substructure mining is that the search space is exponential with respect to the data set  , forcing runtimes to be quite long. To find out the best model structure from this huge space  , an efficient search strategy is highly demanded. Heuristic search aspires to solve this problem efficiently by utilizing background knowledge encoded in a heuristic function. After fitting a combination of exponential and Weibull models to their data  , they report that roughly 10% of inter-modification intervals are 10 days or less and roughly 72% are 100 days or less. To put this into perspective  , even for the simple snowflake example with 12 nodes  , the size of the lattice is 1024 and the size of the game tree is 1024 factorial the amount of time required to search the game tree  , an astronomically large number. In order to prevent this exponential increase of the planning time for queries with many patterns  , we use a greedy query optimizer when the number of patterns in the query is greater than a fixed number. Then the document scores and their new ranks are transformed using exponential function and logarithmic function respectively. In order to deal with configuration similarity under limited time  , Papadias et al. In Section 5 we present a technique based on analyzing the properties of ideal queries  , and using those observations to prune the option search space. In this work  , we take advantage of the advancement in speech recognition  , to explore a high-quality transcribed query log  , but do not delve into speech recognition aspects. Specifically  , it was shown empirically that the score distributions on a per query basis may be fitted using an exponential distribution for the set of non-relevant documents and a normal distribution for the set of relevant documents. In order to avoid this situation  , most researchers 1623 focus on a special case where all images/frames contain exactly the same set of labeled objects. Along the line of similar studies  , the statistics suggest an exponential growth of pages on the WWW. In general  , in the worst case we would need to look at all possible subsets of triples an exponential search space even for the simplest queries. OPTIMIZED uses memoization to avoid this exponential explosion: it never expands a rule more than once per query. During the past decade colleges and universities have witnessed an exponential growth in digital information available for teaching and learning. The only approach that could be employed is systematic search  17 18  , which due to the worst case exponential cost is not guaranteed to terminate within reasonable time. We first show that the score distributions for a given query may be modeled using an exponential distribution for the set of non-relevant documents and a normal distribution for the set of relevant documents. In these conditions   , the interpretation tree approach seems impracticable except for very small maps. However  , the discretized equations of motion can be formulated in such way that most of the operations can be precomputed. With a case-base on the order of ten cases  , we were able to solve a set of ASG tasks which otherwise require exponential time because of the spatial properties involved. Figure 3shows the scalability of All-Significant-Pairs and LiveSet-Driven with respect to various gradient thresholds . This is a very important issue since if the rules were applied in an unordered and exhaustive manner there would be the problem of exponential explosion of the search space. By making objects a part of the domain model  , SPPL planner avoids unnecessary grounding and symmetries  , and the search space is reduced by an exponential factor as a result. The occurrence of sub-itemsets in the search space is a threat when answer completeness is required. sheet approach all require user examination to discard unintended mappings 8  with extra effort devoted to search for mappings not automatically generated missed mappings. Allowing disconnected sub-ensembles would imply an exponential search through all subsets of the total ensemble  , and distributing information between the members of these subsets would require significant multi-hop messaging. T Query arrival rate described by an exponential distribution with mean 1/λ  , T = λ. ts Seek plus latency access time  , ms/postings list  , ts = 4 throughout. To allow larger distances to increase backtracking capability and avoid the exponential explosion  , a maximum number of markings is allowed at each level. The second challenge is that the MDS's frequency threshold cannot be set as high as it is in frequent subsequence mining. The straightforward exhaustive search is apparently infeasible to this problem  , especially for highdimensional datasets. The perplexity of tweet d is given by the exponential of the log likelihood normalized by the number of words in a tweet. In contrast  , the proposed approach in this paper leverages the exponential character of the probabilistic quadtree to dramatically reduce the state space  , which also benefits the Fig. In this section  , we show how to normalize a tRDF database — later  , in Section 6  , we will show experimentally that normalization plays a big part in evaluating queries efficiently at the expense of a small increase in the storage space. Our analytical model has these features:  Pages have finite lifetime following an exponential distribution Section 5.1. If the size of d is p the number of alternatives then after n steps there are pn possible target configurations  , so the search space is exponential. Although abstract action models capture the world dynamics compactly  , using them for planning is challenging: the state space in relational domains is exponential in the number of objects  , the search space of action sequences is huge  , and reasoning about actions is aggravated by the their stochasticity. The key feature of the prophet graph  , is that we can use it to compute the solution to the query without having to refer to the original graph G. Though PRO-HEAPS still has exponential computational complexity in the worst case  , in practice it is able to execute queries in real time as shown in our Section 4. The ontology building experience in my Grid suggests the need of automated tools that support the ontology curator in his work  , especially now with the exponential increase of the number of bioinformatics services. Because NDCG focuses on ranking for top pairs  , it is extensively used to measure and compare the performances of rankers or search engines. Our approach differs in three ways: our method for finding the internal grasp force can be carried on efficiently during the computation of the robot dynamics 9; we use a penalty-based optimization rather than a potentially exponential search; and we deal directly with the frictional constraints  , which requires knowing or estimating only the coefficient of kinetic friction between the fin ers and the grasped object. 0 Motion prediction. Several variants coexists; among them the Fourier Transform for discrete signals and the Fast Fourier Transform which is also for discrete signals but has a complexity of On · ln n instead of On 2  for the discrete Fourier Transform. This can be calculated in JavaScript. The Fourier coefficients are used as features for the classification. By applying the Fast Fourier Transformation FFT to the ZMP reference   , the ZMP equations can be solved in frequency domain. These feature vectors are used to train a SOM of music segments. Fast Fourier Transform FFT has been applied to get the Fourier transform for each short period of time. Most data visualizations  , or other uses of audio data begin by calculating a discrete Fourier transform by means of a Fast Fourier Transform. This section describes an important when there is an acceleration or deceleration  , the amplitude is greater than a threshold. The Fourier spectrum is normalized by the DC component  , i.e. We modeled FFTs in two steps which are considered separately by the database. In 10 the authors use the Fast Fourier Transform to solve the problem of pattern similarity search. 4shows the beating heart motion along z axis with its interpolation function and the frequency spectrum calculated from off-line fast fourier transform. The Fourier spectrum calculation is proportional to the square of the voltage input signal. First one  , we transform the data into Frequency domain utilizing the Fast Fourier Transform FFT  , obtain the derivative using the Fourier spectral method  , and perform inverse FFT to find the derivative in real time domain. This is followed by a Fast Fourier Transformation FFT across the segments for a selected set of frequency spectra to obtain Fourier coefficients modeling the dynamics. The Servo thread is an interrupt service routine ISR which The windows are grouped in two sections: operator windows green softkeys and expert windows blue softkeys. Second one  , numerically calculate the derivative using the finite difference method. Since the temporal data from 'gentle interaction' trials were made of many blobs  , while temporal data from 'strong interaction' trials were mainly made of peaks  , we decided to focus on the Fourier spectrum also called frequency spectrum  which a would express these differences: for gentle interaction  , there would be higher amplitudes for lower frequencies while for strong interaction  , there would be higher amplitudes for higher frequencies. A fast-Fourier transform was performed on this signal in order to analyze the frequencies involved and the results can be seen in figure 12. The use of the fast Fourier transform and the necessity to iterate to obtain the required solution preclude this method from being used in real time control. Periodically  , the fast Fourier transform FFT yields a signal spectrum: But the bcst way is to determine TI and T2 directly in DSP from input data array xn. The impulse was effected by tapping on the finger with a light and stiff object. The first method is to take the fast Fourier transform FFT of the impulse response for Table 2: Characteristic frequencies for link 2 a given impulse command. Fast Fourier Transform. Each segment  , the entire interval when the sensor is in contact with the object  , is transferred to the frequency domain using Fast Fourier Transform. Since it is hard to pick up the signals during contact phase  , we cannot use the Fast Fourier Transformation FFT technique which converts the signal from time-domain to frequencydomain . As na¨ıvena¨ıve implementations that evaluate the KDE at every input point individually can be inefficient on large datasets  , implementations based on Fast Fourier Transformation FFT have been proposed. To ensure the FFT functioned appropriately  , the data was limited to a range which covered only an integer number of cycles. 1for an example spectrogram. Then the inverse FFT returns the resulted CoM trajectory into time domain. A Fast Fourier Transform FFT based method WiaS employed to compute the robot's C-space. In this method th'e C-space is respresented as the convolution of the robot and workspace bitmaps 19. We identify this noise elements by high frequency and low-power spectrum in the frequenc domain transformed by the fast Fourier transform YFFT. The signal detection operates on a power signal; a Fast Fourier Transform FFT is being done which trans­ forms the signal in time domain into frequency domain. We assume that the torque sensor output is composed of various harmonic waves whose frequencies are unknown. Combining the 256 coefficients for the 17 frequency bands results in a 4352-dimensional vector representing a 5-second segment of music. It is often easier to recognize patterns in an audio signal when samples are converted to a frequency domain spectrogram using the Fast Fourier Transform FFT 3  , see Fig. The vibration modes of the flexible beam are identified by the Fast Fourier Transform FFT  , and illustrated in Fig. For example  , our Space Physics application 14 requires the FFT Fast Fourier Transform to be applied on large vector windows and we use OS-Split and OS- Join to implement an FFT-specific stream partitioning strategy. The Discrete Cosine Transform DCT is a real valued version of Fast Fourier Transform FFT and transforms time domain signals into coefficients of frequency component. By exploiting a characteristic that high frequency components are generally less important than low frequency components  , DCT is widely used for data compression like JPEG or MPEG. Figure 2shows the impulse expressed as a change in the wavelength of light reflected by an FBG cell and its fast Fourier transform FFT. In an early attempt  , Anuta l  used cross-correlation to search for corresponding features between registered images; later he introduced the idea of using fast Fourier transform. Similar attempts   , using the sum of absolute differences  , were also reported in the early stages of research on this topic. The statistic behaviors for each indicator were determined computing the mean and standard deviation. The one-dimensional Fast Fourier Transform is then applied to this array. Step 2: Since the primary task is to maintain visibility of the target  , the acceptable observer locations are marked. After removing this noise data from the data  , the remaining elements are transformed into the time domain by using the inverse FFT. The resulting frequency spectra are plotted for pitch and roll in Fig. These features are usually generated based on mel-frequency cepstral coefficients MFCCs 7 by applying Fast Fourier transforms to the signal. The sharp pixel proportion is the fraction of all pixels that are sharp. To obtain features  , we calculated the power of the segment of 1 second following the term onset using the fast Fourier transform and applying log-transformation to normalize the signal. However  , it can still be used in open-loop control and other closed-loop control strategies. In 8  , it is shown that the Fast Fourier Transform can be used to efficiently obtain a C-space representation from the static obs1 ,acles and robot geornetry. To better understand the nature of the VelociRoACH oscillations as a function of the stride frequency  , we used Python 3 to compute the fast Fourier transform of each run  , first passed through a Hann window  , and then averaged across repeated trials. Used features. The twenty-tree indicators are : 2 indicators of instant energy  , 3 obtained by fast Fourier transform FFT  , 16 from the computation of mean power frequency MPF and  , others resulting from the energy spectrum of each component derived from the wavelet decomposition of the normalized EMG. The photographs are transformed from spatial domain to frequency domain by a Fast Fourier Transform  , and the pixels whose values surpass a threshold are considered as sharp pixels we use a threshold value of 2  , following 4. The used features are Root Mean Square RMS computed on time domain; Pitch computed using Fast Fourier Transform frequency domain; Pitch computed using Haar Discrete Wavelet Transform timefrequency domain; Flux frequency domain; RollOff frequency domain; Centroid frequency domain; Zero-crossing rate ZCR time domain. They use the Discrete Fourier Transform DFT to map a time sequence to the frequency domain  , drop all but the first few frequencies  , and then use the remaining ones to index the sequence using a R*-tree 3 structure. Shannon Entropy is defined as To answer this question  , we calculate the Shannon Entropy of each user from the distribution of categories across their sessions. Shannon entropy: Shannon entropy 27 allows to estimate the average minimum number of bits needed to encode a string of symbols in binary form if log base is 2 based on the alphabet size and the frequency of symbols. FE- NN2 is based on the fast implementation scheme and the approximate pignistic Shannon entropy. We then calculate the Shannon Entropy Shannon et al.  the autocorrelation of the signal. Applying the Shannon Entropy equation directly will be misleading. Hence  , the optimum wavelet tree represents the maximum entropy contained in the image and thereby its information content. Shannon Entropy is shown on the left  , min-Entropy in the middle and Rényi Entropy on the right. In information theory  , entropy measures the disorder or uncertainty associated with a discrete  , random variable  , i.e. More specifically  , we compute two entropy-based features for the EDA and EMG-CS data: Shannon entropy and permutation entropy. We consider a set of objects described by boolean variables . In this section  , we analyze the characteristics of categories on Pinterest and Twitter. Entropy is being popularly applied as a measurement in many fields of science including biology  , mechanics  , economics  , etc. Higher entropy means a more uniform distribution across beer types  , i.e. The Theil uncertainty coefficient measures the entropy decrease rate of the consequent due to the antecedent . There are numerous metrics that are applicable such as informationbased metrics that result in the optimization of Shannon entropy  , mutual information  , etc. This indicates the proposed fast implementation scheme works well  , both in equivalent combination scheme and the use of approximate pignistic Shannon entropy. Shannon entropy in the past has been successfully used as a regularizing principle in optical image reconstruction problems. So he has there by advanced information theory remarkably . We choose the Shannon entropy as the opthising functional. In above  , K fuzzy evidence structures are used for illustration . Information theory deals with assessing and defining the amount of information in a message 32 . However  , the LZ method shows a more intense correlation since our model has considered the conditional situations. The results are shown in Table 3   , which indicate that an individual's NST@Self shows an obvious positive correlation with both shannon entropy and LZ  , i.e. Shannon proposed to measure the amount of uncertainty or entropy in a distribution. Shannon entropy. To answer this question  , we calculate the Shannon Entropy of each user from the distribution of categories across their sessions. Our task is to predict user engagement solely on the basis of inexpensive  , easy-to-acquire user interaction signals. Hypothesis 1 -Tweeters with higher diversity have higher brokerage opportunities. Having computed the topical distribution of each individual tweet  , we can now estimate an entire profile's topical diversity and do so by using the Shannon diversity theorem entropy: Topical Diversity. Uncertainties/entropies of the two distributions can be computed by Shannon entropy: Let Y denote posterior changed probabilities after certain information is known: Y = y1  , y2  , . In general  , the optimization problem 17 can be locally solved using numerical gradient-descent methods. In this section  , we present the least information theory LIT to quantify meaning semantics in probability distribution changes. Moreover  , the MI can be represented via Shannon entropy  , which is a quantity of measuring uncertainty of random variables  , given as follows It is straightforward that the MI between two variables is 0 iff the two variables are statistically independent. By varying the resistor R we can vary the weight given to the regularizing entropy term relative to the minimization of the square of the error. Another approach is to apply the Kolmogorov complexity that measures the signal complexity by its minimum description length  , that in the limit tends to the Shannon Entropy measure. Similarly  , the weighted permutation entropy scores did not exhibit a significant difference over the latency conditions  , for permutations of order With respect to the EDA data  , the obtained Shannon entropy scores did not change significantly across the latency conditions χ 2 3 = 3.40  , p > .05. For example  , using gray level histogram  , a checker-board b/w pattern of 2x2 squares will have the same entropy as one with 4x4 squares covering an equal area although the latter contains more information. Various other theorists introduced the concept of Entropy to general systems. Thus  , the Shannon Entropy forms a type of lower bound on the dimensionality of the index space. We made use of Spearman's rho 8  , which measures the monotonic consistency between two variables   , to test whether NST@Self stays in line with modelfree methods. The rationale for this choice  , as well as the underlying mathematics  , is described in detail later in this article. This basic unit of objective information  , the bit  , was more formally related to thermodynamics by Szilard. Finally  , to address the varying number of checkins per user  , we compute the Shannon Entropy of the per user checkin frequency. Yet  , we turn to a decomposition-like scheme  , where a product result of fuzzy evidence structures is treated as a fuzzy like focal with mass 1  , and it is further decomposed into a crisp evidence structure in the same manner as 3. Moreover   , pignistic Shannon entropy is computed based on the derived crisp evidence structure. Shannon adopted the same log measure when he established the average information-transmitting capacity of a discrete channel  , which he called the entropy  , by analogy with formulae in thermodynamics. Finally  , there might be months that are more olfactory pleasant than others. Specifically  , Let X be a |W | × C matrix such that x w ,c is the number of times term w appears in messages generated by node c. Towards understanding how unevenly each term is distributed among nodes  , let G be a vector of |W | weights where g w is equal to 1 plus term w's Shannon information entropy 1. We measure the compressibility of the data using zero order Shannon entropy H on the deltas d which assumes deltas are independent and generated with the same probability distribution  , where pi is the probability of delta i in the data: It also reduces the delta sizes as compared to URL ordering  , with approximately 71.9% of the deltas having the value one for this ordering. Query session := <query  , context> clicked document* Each session contains one query  , its corresponding context and a set of documents which the user clicked on or labeled which we will call clicked documents. Mutual information is a measure of the statistical dependency between two random variables based on Shannon' s entropy and it is defined as the following: Thus probabilistic correlations among query terms  , contextual elements and document terms can be established based on the query logs  , as illustrated in Figure 1. In the field of information science  , Shannon has defined information as the degree of entropy. In t h e 1940's  , Shannon resolved the problem of measuring information by defining Entropy as a measure of the uncertainty of transmission of information: where as is the space of information signals transmitted 12  , 51. On both datasets  , the feature weight shows that powerful users tend to express a more varied range of emotions. Using these interpretations  , it would be possible to relate this information measure to the conventional Shannon-Hartley entropy measure. We calculate these metrics for both the fitted model and the actual data  , and compare the results. Indeed  , training a classifier on the Shannon entropy of a user's distribution of NRC categories achieved good performance on FOLLOWERS and KLOUT  , with accuracies of 65.36% and 62.38% respectively both significant at p < 0.0001. In the information theory  , the concept of entropy developed by Shannon measures the extent to which a system is organized or disorganized. In this section  , we compare DIR to the informationtheoretic measures traditionally used to evaluate rule interestingness see table 1for formulas:  the Shannon conditional entropy 9  , which measures the deviation from equilibrium;  the mutual information 12  , the Theil uncertainty 23 22  , the J-measure 21  , and the Gini index 2 12  , which measure the deviation from independence. The outputs of our computational methodology are two  , inter-related  , user typologies: 1 a course-grained view of the user population segmented into use diffusion adopter categories and 2 a fine-grained view of the same population segmented along the same two dimensions but using more detailed measures for variety and frequency. Furthermore  , since NST@Self actually measures an individual's aspiration for variety  , we compared two model-free methods widely adopted in information theory: shannon 37  , which calculates the conditional entropy. The information-theoretic measures commonly used to evaluate rule interestingness are the Shannon conditional entropy 9  , the average mutual information 12 often simply called mutual information  , the Theil uncertainty coefficient 23 22  , the J-measure 21  , and the Gini index 2 12 cf. Several well studied codes like the Huffman and Shannon- Fano codes achieve 1 + HD bits/tuple asymptotically  , using a dictionary that maps values in D to codewords. LIF and LIB*TF  , which have an emphasis on term frequency  , achieved significantly better recall scores. Overall  , LIB*LIF had a strong performance across the data collections. Methods with the LIB quantity  , especially LIB  , LIB+LIF  , and LIB*LIF  , were effective when the evaluation emphasis was on within-cluster internal accuracy  , e.g. Compared to TF*IDF  , LIB*LIF  , LIB+LIF  , and LIB performed significantly better in purity  , rand index  , and precision whereas LIF and LIB*TF achieved significantly better scores in recall. While LIB and LIB+LIF did well in terms of rand index  , LIF and LIB*TF were competitive in recall. Similar to IDF  , LIB was designed to weight terms according to their discriminative powers or specificity in terms of Sparck Jones 15. LIB+LIF: To weight a term  , we simply add LIB and LIF together by treating them as two separate pieces of information. By modeling binary term occurrences in a document vs. in any random document from the collection  , LIB integrates the document frequency DF component in the quantity. With the NY Times corpus  , LIB*LIF continued to dominate best scores and performed significantly better than TF*IDF in terms of purity  , rand index  , and precision Table 5. The two are related quantities with different focuses. While we have demonstrated superior effectiveness of the proposed methods  , the main contribution is not about improvement over TF*IDF. Whereas LIF well supported recall  , LIB*LIF was overall the best method in the experiments and consistently outperformed TF*IDF by a significant margin  , particularly in terms of purity  , precision  , and rand index. The LIB*LIF scheme is similar in spirit to TF*IDF. In most experiments  , the proposed methods  , especially LIB*LIF fusion   , significantly outperformed TF*IDF in terms of several evaluation metrics. This is very consistent with WebKB and RCV1 results . LIF  , on the other hand  , models term frequency/probability distributions and can be seen as a new approach to TF normalization . The other methods such as LIF and LIB*TF emphasize term frequency in each document and  , with the ability to associate one document to another by assigning term weights in a less discriminative manner  , were able to achieve better recalls. In light of TF*IDF  , we reason that combining the two will potentiate each quantity's strength for term weighting. In each set of experiments presented here  , best scores in each metric are highlighted in bold whereas italic values are those better than TF*IDF baseline scores. We derive two basic quanti-ties  , namely LI Binary LIB and LI Frequency LIF  , which can be used separately or combined to represent documents.