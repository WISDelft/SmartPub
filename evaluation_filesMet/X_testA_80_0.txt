Then    , two paralleled embedding layers are set up in the same embedding space    , one for the affirmative context and the other for the negated context    , followed by their loss functions.   , discriminative word sequences found within the input tweets that are common throughout the training instances. Hence    , the input sentence matrix is augmented with an additional set of rows from the word type embeddings . So    , when tackling the phrase-level sentiment classification    , we form a sentence matrix S as follows: for each token in a tweet    , we have to look up its corresponding word embedding in the word matrix W    , and the embedding for one of the two word types.