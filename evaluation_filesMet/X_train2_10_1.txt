Motivated by financial and statistical applications e.g. Another strength of our approach is that it is a relatively simple and efficient way of incorporating time into statistical relational models. For example  , hyperlinked web pages are more work Koller  , personal communication. Autocorrelation is a statistical dependency between the values of the same variable on related entities  , which is a nearly ubiquitous characteristic of relational datasets. In this paper  , we proposed three classification models accounting for non-stationary autocorrelation in relational data. To date  , work on statistical relational models has focused primarily on static snapshots of relational datasets even though most relational domains have temporal dynamics that are important to model. Although there has been some work modeling domains with time-varying attributes  , to our knowledge this is the first model that exploits information in dynamic relationships between entities to improve prediction. In addition  , the shrinkage approach could easily be incorporated into other statistical relational models that use global autocorrelation and collective inference. Relational autocorrelation  , a statistical dependency among values of the same variable on related en- tities 7  , is a nearly ubiquitous phenomenon in relational datasets. Promising research directions include: 1 using patterns e.g. Access rights may be granted and revoked on views just as though they were ordinary tables. These sizes are then used to determine the CPU  , IO and communication requirements of relational operations such as joins. However  , this work has focused primarily on modeling static relational data. This explanation applies to continuous and discrete variables and essentially any test of conditional independence. The goal of this work is to improve attribute prediction in dynamic domains by incorporating the influence of timevarying links into statistical relational models. Indeed  , the results we report for LGMs using only the class labels and the link information achieve nearly the same level of performance reported by relational models in the recent literature. The relational operations join  , restrict and project as well as statistical summaries of tables may be used to define a view. This paper presents the Kylin Ontology Generator KOG  , an autonomous system that builds a rich ontology by combining Wikipedia infoboxes with WordNet using statistical-relational learning. One motivation for modeling time-varying links is the identification of influential relationships in the data. This information is necessary to derive accurate relational statistics that are needed by the relational optimizer to accurately estimate the cost of the query workload. This theory b part of a unitled approach to data modelling that integrates relational database theory  , system theory  , and multivariate statistical modelling tech- niques. In this work  , we propose the Time Varying Relational Classifier TVRC framework—a novel approach to incorporating temporal dependencies into statistical relational models. Researchers always use tables to concisely display their latest experimental results or statistical data. Autocorrelation is a statistical dependence between the values of the same variable on related entities  , which is a nearly ubiquitous characteristic of relational datasets. Whereas in the CONTROL condition 20% of the adjectives chosen belonged to the machine category  , 20% to the humanized one and 60% to the relational one. Instead of storing the data in a relational database  , we have proposed to collect Statistical Linked Data reusing the RDF Data Cube Vocabulary QB and to transform OLAP into SPARQL queries 14. We chose statistical data  , because 1 there is clear need to integrate the data and 2 although the data sets are covering semantically similar topics  , standardization usually does not cover the object properties  , only the code lists themselves  , if at all. Each infobox template is treated as a class  , and the slots of the template are considered as attributes/slots. They are  , however  , at a disadvantage in interactivity  , graphical presentation and popularity of the computational language. Regarding the multiple adjective choice  , even if not supported by statistical significance  , we observe that children in the OAT condition chose no machine category adjectives  , 30% of the chosen adjectives belonged to the humanized category and 70% to the relational one. This work has demonstrated that incorporating the characteristics of related instances into statistical models improves the accuracy of attribute predictions. For example  , hyperlinked web pages are more likely to share the same topic than randomly selected pages 23  , and movies made by the same studio are more likely to have similar box-office returns than randomly selected movies 6. IE can only be employed if sensory information is available that is relevant to a relation  , deductive reasoning can only derive a small subset of all statements that are true in a domain and relational machine learning is only applicable if the data contains relevant statistical structure. Although there are probably a number of heuristic ways to combine sensory information and the knowledge base with machine learning  , it is not straightforward to come up with consistent probabilistic models. In this paper  , we intend to give an empirical argument in favor of creating a specialised OLAP engine for analytical queries on Statistical Linked Data. Recent work has only just begun to incorporate temporal information into statistical relational models. Some initial work has focused on transforming temporal-varying links and objects into static aggregated features 19 and other work has focused on modeling the temporal dynamics of time-varying attributes in static link structures 13. Our initial investigation has shown that modeling the interaction among links and attributes will likely improve model generalization and interpretability. This allows the model to consider a wider range of dependencies to reduce bias while limiting potential increases in variance and promises to unleash the full power of statistical relational models. On the other hand  , there are existing computational engines without scalability or fragmentation problems and with a well-defined computational algebra  , for example  , OLAP 7  , 8  , Statistical 12 and Relational engines. On the other hand  , DataScope is flexible to browse various relational database contents based on different schemas and ad-hoc ranking functions. Yet  , there is little work on evaluating and optimising analytical queries on RDF data 4 ,5 . Thii attribute enables DBLEARN to output such statistical statements as 8% of all students majoring in Sociology are Asians. Two areas for further investigation are: the use of probabilistic dependencies as constrainta  , and the way in which they interact; and the concept of the degree to This theory b part of a unitled approach to data modelling that integrates relational database theory  , system theory  , and multivariate statistical modelling tech- niques. While our use case has been motivated by statistical data  , a lot of Linked Data sources share this data model structure  , since many of them are derived from relational databases. In this paper we have combined information extraction  , deductive reasoning and relational machine learning to integrate all sources of available information in a modular way. For example  , pairs of brokers working at the same branch are more likely to share the same fraud status than randomly selected pairs of brokers. The language of non-recursive first-order logic formulas has a direct mapping to SQL and relational algebra  , which can be used as well for the purposes of our discussion  , e.g. Disjoint learning ignores the unlabeled instances in the graph during learning see Figure 1b This is because collective inference methods are better able to exploit relational autocorrelation  , which refers to a statistical dependency between the values of the same variable on related instances in the graph. The Comet methodology is inspired by previous work in which statistical learning methods are used to develop cost models of complex user-defined functions UDFs—see 13  , 15—and of remote autonomous database systems in the multidatabase setting 19  , 26. Topic model performance is often measured by perplexity of test data as a function of statistical word frequencies  , ignoring word order. We used as our backend retrieval system the IBM DB2 Net Search Extender  , which allows convenient combination of relational and fulltext queries. Relational machine learning attempts to capture exactly these statistical dependencies between statements and in the following we will present an approach that is suitable to also integrate sensory information and a knowledge base. We also propose a way to estimate the result sizes of SPARQL queries with only very few statistical information. In FJS97   , a statistical approach is used for reconstructing base lineage data from summary data in the presence of certain constraints . This paper presents a new approach to modeling relational data with time-varying link structure. This is because collective inference methods are better able to exploit relational autocorrelation  , which refers to a statistical dependency between the values of the same variable on related instances in the graph. The characteristics of such domains form a good match with our method: i links between documents suggest relational representation and ask for techniques being able to navigate such structures; " flat " file domain representation is inadequate in such domains; ii the noise in available data sources suggests statistical rather than deterministic approaches  , and iii often extreme sparsity in such domains requires a focused feature generation and their careful selection with a discriminative model  , which allows modeling of complex  , possibly deep  , but local regularities rather than attempting to build a full probabilistic model of the entire domain. To address the shortcomings of this conventional approach   , we described in this paper statistics on views in Microsoft SQL Server  , which provide the optimizer with statistical information on the result of scalar or relational expressions. Even if privacy and confidentiality are in place  , to be practical  , outsourced data services should allow sufficiently expressive client queries e.g. This is important because today's outsourced data services are fundamentally insecure and vulnerable to illicit behavior  , because they do not handle all three dimensions consistently and there exists a strong relationship between such assurances: e.g. Therefore  , we can conclude that attribute partitioning is important to a SDS. To support the integration of traditional Semantic Web techniques and machine learning-based  , statistical inferencing  , we developed an approach to create and work with data mining models in SPARQL. The goal of this paper is to combine the strengths of all three approaches modularly  , in the sense that each step can be optimized independently. Contributions of this paper are centered around four analytical query approaches listed in the following – We compare the performance of traditional relational approaches RDBMS / ROLAP and of using a triple store and an RDF representation closely resembling the tabular structure OLAP4LD-SSB. In addition  , we will cast the model in a more principled graphical model framework  , formulating it as a latent variable model where the summary " influence " weights between pairs of nodes are hidden variables that change over time and affect the statistical dependencies between attribute values of incident nodes. If there is a significant influence effect then we expect the attribute values in t + 1 will depend on the link structure in t. On the other hand  , if there is a significant homophily effect then we expect the link structure in t + 1 will depend on the attributes in t. If either influence or homophily effects are present in the data  , the data will exhibit relational autocorrelation at any given time step t. Relational autocorrelation refers to a statistical dependency between values of the same variable on related objects—it involves a set of related instance pairs  , a variable X defined on the nodes in the pairs  , and it corresponds to the correlation between the values of X on pairs of related instances. Attribute partitioning HAMM79 is another term for a transposed file scheme within a relational database  , As stated in BORA62  , such schemes are useful in statistical database systems because although the relations often contain many attributes  , usually only a few are referenced in any one query  , Additionally  , attribute partitioning is useful in compression schemes that depend on physical adjacency of identical values EGGEBO  , EGGEBl  , TURN79. We now present our overall approach called SemanticTyper combining the approaches to textual and numeric data. First  , we used the data extracted from DBpedia consisting of the 52 numeric & textual data properties of the City class to test our proposed overall approach SemanticTyper. It is evident from experimental results that our approach has much higher label prediction accuracy and is much more scalable in terms of training time than existing systems. Experiments on three real-world datasets demonstrate the effectiveness of our model. Noting that our work provides a framework which can be fit for any personalized ranking method  , we plan to generalize it to other pairwise methods in the future. Genetic programming GP is a means of automatically generating computer programs by employing operations inspired by biological evolution 6. The term "Genetic Programming" was first introduced by Koza 12 and it enables a computer to do useful things by automatic programming. However  , whether the balance can be achieved by genetic programming used by GenProg has still been unknown so far. The problems all shared a common set of primitives. GGGP is an extension of genetic programming. The core of this engine is a machine learning technique called Genetic Programming GP. Given a problem  , the basic idea behind genetic programming 18 is to generate increasingly better solutions of the given problem by applying a number of genetic operators to the current population . Compared to random search  , genetic programming used by GenProg can be regard as efficient only when the benefit in terms of early finding a valid patches with fewer number of patch trials  , brought by genetic programming  , has the ability of balancing the cost of fitness evaluations  , caused by genetic programming itself. Communication fitness for controller of Figure  93503 for a mobile robot via genetic programming with automatically defined functions  , Table 5. In Section 2  , we provide background information on term-weighting components and genetic programming. l   , who used genetic programming to evolve control programs for modular robots consisting of sliding-style modules 2  , 81. al. Several program repair approaches assume the existence of program specification. First  , the initial population is generated  , and then genetic operators  , such as Genetic programming GP is a means of automatically generating computer programs by employing operations inspired by biological evolution 6. One of the key problems of genetic programming is that it is a nondeterministic procedure. These primitives were d e signed to aid genetic programming in finding a solution and either encapsulated problem specific information or low-level information that was thought to be helpful for obtaining a solution. In this paper  , however  , we plan to further investigate whether genetic programming used by GenProg has the better performance over random search  , when the actual evolutionary search has started to work. Determining which information to add was the result of parallel attempts to examine the unsuccessful results produced by the genetic programming and attempts to hand code problem solutions. We defer discussing the possible reason to Section 6. In this paper  , we try to investigate the two questions via the performance comparison between genetic programming and random search. One novel part of our work is that we use a Genetic Programming GP based technique called ARRANGER Automatic geneRation of RANking functions by GEnetic pRogramming to discover ranking functions automatically Fan 2003a. Ranking functions usually could not work consistently well under all situations. We compared EAGLE with its batch learning counterpart. Other researchers used classifier systems 17  or genetic programming paradigm 3  to approach the path planning problem. RQ2 is designed to answer the question. proposed GenProg  , an automatic patch generation technique based on genetic programming. In Section 3  , we present our Combined Component Approach for similarity calculation. Individuals in the new generation are produced based on those in the current one. 19  select ranking functions using genetic programming   , maximizing the average precision on the training data. GP maintains a population of individual programs. Individuals in a new generation are produced based on those in the previous one. The entity resolution ER problem see 14 ,3  for surveys shares many similarities with link discovery. An individual represents a tentative solution for the target problem. We are not surprised for this experimental results. 17  propose matching ads with a function generated by learning the impact of individual features using genetic programming. This approach randomly mutates buggy programs to generate several program variants that are possible patch candidates. The 'Initial Repair' heading reports timing information for the genetic programming phase and does not include the time for repair minimization. for a mobile robot via genetic programming with automatically defined functions  , Table 5. collision avoidance as well as helping achieve the overall task. 7  proposed a new approach to automatically generate term weighting strategies for different contexts  , based on genetic programming GP. Interested readers can reference that paper or  The details of our system and methodology for Genetic Programming GP are discussed in our Robust track paper. Generate an initial population of random compositions of the functions and terminals of the problem solutions. GP is expansion of GA in order to treat structural representation. As our time and human resources were limited for taking two tasks simultaneously  , in this task we only concentrate on testing our ranking function discovery technique  , ARRANGER Automatic Rendering of RANking functions by GEnetic pRogramming Fan 2003a  , Fan 2003b  , which uses Genetic Programming GP to discover the " optimal " ranking functions for various information needs. Given the problem  , RQ1 asks whether genetic programming used by GenProg works well to benefit the generation of valid patches. Although promising results have been shown in their work  , the problem of whether the promising results are caused by genetic programming or just because the used mutation operations are very effective is still not be addressed. Genetic Programming GP 14 is a Machine Learning ML technique that helps finding good answers to a given problem where the search space is very large and when there is more than one objective to be accomplished. Also  , the work in 24  applies Genetic Programming to learn ranking functions that select the most appropriate ads. The experimental results show that the matching function outperforms the best method in 21 in finding relevant ads. We also compared our method with genetic programming based repair techniques. Genetic programming approaches support more complex repairs but rely on heuristics and hence lack these important properties. In Genetic Programming  , a large number of individuals  , called a population  , are maintained at each generation. Genetic Programming searches for the " optimal " solution by evolving the population generation after generation. Our first approach extends a state-of-the-art tag recommender based on Genetic Programming to include novelty and diversity metrics both as attributes and in the objective function 1. Genetic Programming searches for an " optimal " solution by evolving the population generation after generation. Koza applied GP Genetic Programming to automatic acquisition of subsum tion architecture to perform wall-following behavior  ?2. Given that genetic programming is non-deterministic  , all results presented below are the means of 5 runs. Each experiment was ran on a single thread of a server running JDK1.7 on Ubuntu 10.0.4 and was allocated maximally 2GB of RAM. We also employed GenProg to repair the bugs in Coreutils. Learning approaches based on genetic programming have been most frequently used to learn link specifications 5 ,15 ,17. In addition  , it usually requires a large training data set to detect accurate solutions. Another genetic programming-based approach to link discovery is implemented in the SILK framework 15. Genetic ProgrammingGP is the method of learning and inference using this tree-based representation". Since an appropriate stopping rule is hard to find for the Genetic Programming approach  , overtraining is inevitable unless protecting rules are set. Finally  , we applied data mining DM techniques based on grammar-guided genetic programming GGGP to create reference models useful for defining population groups. The average time required by SEMFIX for each repair is less than 100 seconds. This confirms that if the repair expression does not exist in other places of the program  , genetic programming based approaches have rather low chance of synthesizing the repair. There has also been work on synthesizing programs that meet a given specification. These functions are discovered using genetic programming GP and a state-of-the-art classifier optimumpath forest OPF 3  , 4. We use genetic programming to evolve program variants until one is found that both retains required functionality and also avoids the defect in question. A framework for tackling this problem based on Genetic Programming has been proposed and tested. In this paper we presented EAGLE  , an active learning approach for genetic programming that can learn highly accurate link specifications. The following experiments were run by connecting FX- PAL'S genetic programming system to a modular robot simulator  , built by J. Kubica and S. Vassilvitskii. Active learning approaches based on genetic programming adopt a comitteebased setting to active learning. As the planning motion  , we give this system vertical movement and one step walk. Sims studied on co-evolution of motion controller and morphology of rirtual creatures 3. All the experiments were conducted on a Core 2 Quad 2.83GHz CPU  , 3GB memory computer with Ubuntu 10.04 OS. The classifier uses these similarity functions to decide whether or not citations belong to a same author. To this purpose we have proposed randomized procedures based on genetic programming or simulated annealing 8  , 9. Subsequently  , we give some insight in active learning and then present the active learning model that underlies our work. GP is a machine learning technique inspired by biological evolution to find solutions optimized for certain problem characteristics. The main inconvenient of this approach is that it is not deterministic. Using an error situation obtained with the sampled parameters  , a fitness unction based on the allowed recovery criteria can be defined. We used strongly typed genetic programming The specific primitives added for each problem are discussed with setup of the the initial population  , results of crossover and mutation  , and subtrees created during mutation respectively . The three most common and most important methods are: Genetic programming applies a number of different possible conditions to the best solutions to create the next generation of solutions. The goal of grammarguided genetic programming is to solve the closure problem 7. External validity is concerned with generalization. Other approaches based on genetic programming e.g. Although they also used genetic programming  , their evaluation was limited to small programs such as bubble sorting and triangle classification  , while our evaluation includes real bugs in open source software. The 'Time' column reports the wall-clock average time required for a trial that produced a primary repair. Since an appropriate stopping rule is hard to find for the Genetic Programming approach  , over-training is inevitable unless protecting rules are set. With this system  , we simulate motion generation hierarchically for six legged locomotion robot using Genetic Programming. Here  , the mappings are discovered by using a genetic programming approach whose fitness function is set to a PFM. Supervised batch learning approaches for learning such classifiers must rely on large amounts of labeled data to achieve a high accuracy. This paper has reported our initial experiments aimed at investigating whether evolutionary programming  , and genetic programming in particular can evolve multiple robot controllers that utilise communication to improve their ability to collectively perform a task. Answer for RQ1: In our experiment  , for most programs 23/24  , random search used by RSRepair performs better in terms of requiring fewer patch trials to search a valid patch than genetic programming used by GenProg  , regardless of whether genetic programming really starts to work see Figure 1 or not. For the representation problem  , GenProg represents each candidate patch as the Abstract Syntax Tree AST of the patched program. They doubted that the promising results may not be brought by genetic programming used by GenProg  , because the patch search problem can be easy when random search would have likely yielded similar results. With the hypothesis that some missed important functionalities may occur in another position in the same program  , GenProg attempts to automatically repair defective program with genetic programming 38. Then  , in this subsection we plan to investigate to what extent genetic programming used by GenProg worsens the repair efficiency over random search used by RSRepair. " Genetic programming GP is a computational method inspired by biological evolution  , which discovers computer programs tailored to a particular task 19. Our classification approach combines a genetic programming GP framework  , which is used to define suitable reference similarity functions   , with the Optimum-Path Forest OPF classifier  , a graph-based approach that uses GP-based edge weights to assign input references to the correct authors. As we can see  , Genetic Programming takes a so-called stochastic search approach  , intelligently  , extensively  , and " randomly " searching for the optimal point in the entire solution space. To give proper answers for these questions  , we propose a new approach to content-targeted advertising based on Genetic Programming GP. We show how the discovery of link specifications can consequently be modeled as a genetic programming problem. sKDD transforms the original numerical temporal sequences into symbolic sequences  , defines a symbolic isokinetics distance SID that can be used to compare symbolic isokinetics sequences   , and provides a method  , SYRMO  , for creating symbolic isokinetics reference models using grammar-guided genetic programming. We developed a genetic programming approach to finding consensus structural motifs in a set of RNA sequences known to be functionally related. Both GenProg and Par use the same fault localization technique to locate faulty statements  , and genetic programming to guide the patch search  , but differ in the concrete mutation operations. The results show that genetic programming finds matching functions that significantly improve the matching compared to the best method without page side expansion reported in 18. Their approach relies on formal specifications  , which our approach does not require. Recent work has addressed this drawback by relying on active learning  , which was shown in 15 to reduce the amount of labeled data needed for learning link specifications. For example   , the approach presented in 5 relies on large amounts of training data to detect accurate link specification using genetic programming. In this paper we have introduced a new approach based on the combination of term weighting components  , extracted from well-known information retrieval ranking formulas  , using genetic programming. Genetic Programming has been widely used and approved to be effective in solving optimization problems  , such as financial forecasting  , engineering design  , data mining  , and operations management. Genetic Programming shows its sharp edge in solving such kind of problems  , since its internal tree structure representation for " individuals " can be perfectly used for describing ranking functions. Section 2 of the paper gives an overview of the I4 Intelligent Interpretation of Isokinetics Information system  , of which this research is part. A follow-up work 13 proposes a method to learn impact of individual features using genetic programming to produce a matching function. Guided by genetic programming  , GenProg has the ability to repair programs without any specification  , and GenProg is commonly considered to open a new research area of general automated program repair 26  , 20  , although there also exists earlier e.g. Construct validity threats concern the appropriateness of the evaluation measurement. GP makes it possible to solve complex problems for which conventional methods can not find an answer easily. In a follow-up work 7 the authors propose a method to learn impact of individual features using genetic programming to produce a matching function.   , but none of these strategies reaches the level of applicability and the speed of execution of random testing. Our technique takes as input a program  , a set of successful positive testcases that encode required program behavior  , and a failing negative testcase that demonstrates a defect. In order to answer these questions  , we choose ARRANGER – a Genetic Programming-based discovery engine 910 to perform the ranking function tuning. Genetic Programming has been widely used and proved to be effective in solving optimization problems  , such as financial forecasting  , engineering design  , data mining  , and operations management 119. As we have formalized link specifications as trees  , we can use Genetic Programming GP to solve the problem of finding the most appropriate complex link specification for a given pair of knowledge bases. This approach is yet a batch learning approach and it consequently suffers of drawbacks of all batch learning approaches as it requires a very large number of human annotations to learn link specifications of a quality comparable to that of EAGLE. The robot modules we consider are the TeleCube modules currently being developed at Xerox PARC 13 and shown in Figure 1 . For example  , the genetic programming approach used in 7 has been shown to achieve high accuracies when supplied with more than 1000 positive examples. Still  , none of the active learning approaches for LD presented in previous work made use of the similarity of unlabeled link candidates to improve the convergence of curious classifiers. For example  , the approach presented in 8 relies on large amounts of training data to detect accurate link specification using genetic programming. Several other strategies for input generation have been proposed symbolic execution combined with constraint solving 30  , 18  , direct setting of object fields 5  , genetic programming 29  , etc. 15 proposes an approach based on the Cauchy-Schwarz inequality that allows discarding a large number of superfluous comparisons. Particularly  , we investigate an inductive learning method – Genetic Programming GP – for the discovery of better fused similarity functions to be used in the classifiers  , and explore how this combination can be used to improve classification effectiveness . Genetic Programming takes a so-called stochastic search approach  , intelligently  , extensively  , and " randomly " searching for the optimal point in the entire solution space. Another approach to contextual advertising is to reduce it to the problem of sponsored search advertising by extracting phrases from the page and matching them with the bid phrase of the ads. In addition  , gradient primitives   , shown to be effective for communication in modular robots We also gave the genetic programming runs additional primitives for each problem. The best computer program that appeared in any generation  , the best-so-far solution  , is designated as the result of genetic programming Koza 19921. In addition  , similar to other search-based software engineering SBSE 15  , 14 approaches  , genetic programming often suffers from the computationally expensive cost caused by fitness evaluation  , a necessary activity used to distinguish between better and worse solutions. That is  , compared to random search  , genetic programming does not bring benefits in term of fewer NCP in this case to balance the cost caused by fitness evaluations. 26  introduced the idea of program repair using genetic programming  , where existing parts of code are used to patch faults in other parts of code and patching is restricted to those parts that are relevant to the fault. Out of the 90 buggy programs  , with a test suite size of 50 — SEMFIX repaired 48 buggy programs while genetic programming repaired only 16. This is the major motivation to choose GP for the ranking function discovery task. Based on the plaintext collection  , our ARRANGER engine  , a Genetic Programming GP based ranking function discovery system  , is used to discover the " optimal " ranking functions for the topic distillation task. The function is represented as a tree composed of arithmetic operators and the log function as internal nodes  , and different numerical features of the query and ad terms as leafs. The results show that genetic programming finds matching functions that significantly improve the matching compared to the best method without page side expansion reported in 8. With flexible GP operators and structural motif representations  , our new method is able to identify general RNA secondary motifs. We choose not to record the genetic programming operations performed to obtain the variant as an edit script because such operations often overlap and the resulting script is quite long. The isolation of the search strategies from the search space makes the solution compatible with that of Valduriez891 and thus applicable to more general database programming languages which can be deductive or object-oriented Lanzelotte901. Yet  , so far  , none of these approaches has made use of the correlation between the unlabeled data items while computing the set of most informative items. Furthermore  , affected by GenProg  , Par also uses genetic programming to guide the patch search in the way like GenProg. The fact that it has been successfully applied to similar problems  , has motivated us to use it as a basis for discovering good similarity functions for record replica identification. This approach captures the novelty and diversity of a list of recommended tags implicitly  , by introducing metrics that assess the semantic distance between different tags diversity and the inverse of the popularity of the tag in the application novelty. AutoFix-E 37 can repair programs but requires for the contracts in terms of pre-and post-conditions. Running test cases typically dominated GenProg's runtime " 22  , which is also suitable for RSRepair  , so we use the measurement of NTCE to compare the repair efficiency between GenProg and RSRepair  , which is also consistent with traditional test case prioritization techniques aiming at early finding software bugs with fewer NTCE. Short titles may mislead the results  , specially generic titles such as Genetic Programming  , then we add the publication venue title to this type of query. That is  , RSRepair immediately discards one candidate patch once the patched program fails to pass some test case. After that  , general automated program repair has gone from being entirely unheard of to having its own multi-paper sessions  , such as " Program Repair " session in ICSE 2013  , in many top tier conferences 20  , and many researchers justify the advantage of their techniques  , such as Par and SemFix  , via the comparison with GenProg. We conducted a set of experiments aiming to evaluate the proposed disambiguation system in comparison with stateof-the-art methods on two well-known datasets. To the best of our knowledge  , the problem of discovering accurate link specifications has only been addressed in very recent literature by a small number of approaches: The SILK framework 14  now implements a batch learning approach to discovery link specifications based on genetic programming which is similar to the approach presented in 6. Several approaches that combine genetic programming and active learning have been developed over the course of the last couple of years and shown to achieve high F-measures on the deduplication see e.g. The evaluation has shown that the numerical and symbolic reference models generated from isokinetics tests on top-competition sportsmen and women are  , in the expert's opinion  , similar. Our method is similar to these methods as we directly optimize the IR evaluation measure i.e. BSBM supposes a realistic web application where the users can browse products and reviews. BSBM generates a query mix based on 12 queries template and 40 predicates. We randomly generated 100 different query mix of the " explore " use-case of BSBM. Each dataset has its own community of 50 clients running BSBM queries. On the BSBM dataset  , the performance of all systems is comparable for small dataset sizes  , but RW-TR scales better to large dataset sizes  , for the largest BSBM dataset it is on average up to 10 times faster than Sesame and up to 25 times faster than Virtuoso. This behavior promotes the local cache. Two synthetic datasets generated using RDF benchmark generators BSBM 2 and SP2B 3 were used for scalability evaluation. We extend the BSBM by trust assessments. The BSBM executes a mix of 12 SPARQL queries over generated sets of RDF data; the datasets are scalable to different sizes based on a scaling factor. The queries are in line with the BSBM mix of SPARQL queries and with the BSBM e-commerce use case that considers products as well as offers and reviews for these products. Due to space limitations   , we do not present our queries in detail; we refer the reader to the tSPARQL specification instead. The sp2b uses bibliographic data from dblp 12 as its test data set  , while the bsbm benchmark considers eCommerce as its subject area. This is normal because the cache has a limited size and the temporal locality of the cache reduce its utility. We note that BSBM datasets consist of a large number of star substructures with depth of 1 and the schema graph is small with 10 nodes and 8 edges resulting in low connectivity. We compare the native SQL queries N  , which are specified in the BSBM benchmark with the ones resulting from the translation of SPARQL queries generated by Morph. 5 BSBM is currently focused on SPARQL queries  , therefore we plan to develop a set of representative SPARQL/Update operations to cover all features of our approach. We found that for the BSBM dataset/queries the average execution time stays approximately the same  , while the geometric mean slightly increases. The Berlin SPARQL Benchmark 17 BSBM also generates fulltext content and person names. Figure 6 shows the results of these evaluations. For more details of the evaluation framework please refer to 15 ,16. Therefore  , 5 entries in the profile is sometimes not enough to compute a good similarity. The experimental results in Table 5show that exploiting the emergent relational schema even in this very preliminary implementation already improves the performance of Virtuoso on a number of BSBM Explore queries by up to a factor of 5.8 Q3  , Hot run. The geometric mean does not change dramatically  , because most queries do not touch more data on a larger dataset. Finally  , we present our conclusions and future work in Section 5. We also take into account that resources of BSBM data fall into different classes. We generate about 70 million triples using the BSBM generator  , and 0.18 million owl:sameAs statements following the aforementioned method. Our extension  , available from the project website  , reads the named graphs-based datasets  , generates a consumer-specific trust value for each named graph  , and creates an assessments graph. Figure 6shows the distribution of queries over clients. The size of table productfeatureproduct is significantly bigger than the table product 280K rows vs 5M rows. For the LUBM dataset/queries the geometric mean stays approximately the same  , whilst the average execution time decreases. We use an evaluation framework that extends BSBM 2 to set up the experiment environment. We generate co-reference for each class separately to make sure that resources are only equivalent to those of the same class. In Section 4 we describe our evaluation using the BSBM synthetic benchmark  , and three positive experiences of applying our approach in real case projects. Figure 4bshows that the number of calls answered by caches are proportional with the size of the cache. As in the previous experimentation  , we run a new experimentation with 2 different BSBM datasets of 1M hosted on the same LDF server with 2 different URLs. Two set of queries are used to perform two tasks: building a type summary and calculating some bibliometrics-based summary.  BSBM SQL 4 contains a join between two tables product and producttypeproduct and three subqueries  , two of them are used as OR operators. The flow of BSBM queries simulates a real user interacting with a web application. The SP 2 Bench and BSBM were not considered for our RDF fulltext benchmark simply due to the fact of their very recent publication. The BSBM benchmark 1 is built around an e-commerce use case  , and its data generator supports the creation of arbitrarily large datasets using the number of products as scale factor. Although not included here  , we also evaluated those queries using D2R 0.8.1 with the –fast option enabled. The measured total time for a run includes everything from query optimization until the result set is fully traversed  , but the decoding of the results is not forced. During query execution the engine determines trust values with the simple  , provenance-based trust function introduced before. Enriching these benchmarks with real world fulltext content and fulltext queries is very much in our favor. Additionally  , a subset of the realworld data collection Biocyc 1 that consists of 1763 databases describing the genome and metabolic pathways of a single organism was used. Most surprisingly  , the RDFa data that dominates WebDataCommons and even DBpedia is more than 90% regular. For this setting  , the chart in Figure 9b depicts the average times to execute the BSBM query mix; furthermore  , the chart puts the measures in relation to the times obtained for our engine with a trust value cache in the previous experiment. We run an experimentation with 2 different BSBM datasets of 1M  , hosted on the same LDF server with 2 differents URLs. The BSBM SPARQL queries are designed in such a way that they contain different types of queries and operators  , including SELECT/CONTRUCT/DESCRIBE  , OPTIONAL  , UNION. To understand this behaviour better  , we analyzed the query plans generated by the RDBMS. For BSBM we executed the same ten generated queries from each category  , computed the category average and reported the average and geometric mean over all categories. The Social Intelligence BenchMark SIB 11  is an RDF benchmark that introduces the S3G2 Scalable Structure-correlated Social Graph Generator for generating social graphs that contain certain structural correlations. In the same spirit  , the corresponding SQL queries also consider various properties such as low selectivity  , high selectivity  , inner join  , left outer join  , and union among many others. All the triples including the owl:sameAs statements are distributed over 20 SPARQL endpoints which are deployed on 10 remote virtual machines having 2GB memory each. To measure the impact of this extension on query execution times we compare the results of executing our extended version of the BSBM with ARQ and with our tSPARQL query engine. As the chart illustrates  , determing trust values during query execution dominates the query execution time. The data generator is able to generate datasets with different sizes containing entities normally involved in the domain e.g. To eliminate the effects of determining trust values in our engine we precompute the trust values for all triples in the queried dataset and store them in a cache. Both benchmarks allow for the creation of arbitrary sized data sets  , although the number of attributes for any given class is lower than the numbers found in the ssa. While the BSBM benchmark is considered as a standard way of evaluating RDB2RDF approaches  , given the fact that it is very comprehensive  , we were also interested in analysing real-world queries from projects that we had access to  , and where there were issues with respect to the performance of the SPARQL to SQL query rewriting approach. The BSBM benchmark 5  focuses on the e-commerce domain and provides a data generation tool and a set of twelve SPARQL queries together with their corresponding SQL queries generated by hand. In all the cases  , we compare the queries generated by D2R Server with –fast enabled with the queries generated by Morph with subquery and self-join elimination enabled. The main result is that the multi-probe LSH method is much more space efficient than the basic LSH and entropybased LSH methods to achieve various search quality levels and it is more time efficient than the entropy-based LSH method. The entropy-based LSH method is likely to probe previously visited buckets  , whereas the multi-probe LSH method always visits new buckets. The results show that the multi-probe LSH method is significantly more space efficient than the basic LSH method. This section provides a brief overview of LSH functions  , the basic LSH indexing method and a recently proposed entropy-based LSH indexing method. Table 2 shows the average results of the basic LSH  , entropybased LSH and multi-probe LSH methods using 100 random queries with the image dataset and the audio dataset. To achieve over 0.9 recall  , the multi-probe LSH method reduces the number of hash tables of the basic LSH method by a factor of 14 to 18 while achieving similar time efficiencies. The space efficiency implication is dramatic. Although the multi-probe LSH method can use the LSH forest method to represent its hash table data structure to exploit its self-tuning features  , our implementation in this paper uses the basic LSH data structure for simplicity. We have experimented with different number of hash tables L for all three LSH methods and different number of probes T i.e. The basic idea of locality sensitive hashing LSH is to use hash functions that map similar objects into the same hash buckets with high probability. Our experimental results show that the multi-probe LSH method is much more space efficient than the basic LSH and entropy-based LSH methods to achieve desired search accuracy and query time. It also shows that the multi-probe method is better than the entropy-based LSH method by a significant factor. It shows that for most recall values  , the multi-probe LSH method reduces the number of hash tables required by the basic LSH method by an order of magnitude. A comparison of multi-probe LSH and other indexing techniques would also be helpful. For example  , 25 introduced multi-probe LSH methods that reduce the space requirement of the basic LSH method. We have also shown that although both multi-probe and entropy-based LSH methods trade time for space  , the multiprobe LSH method is much more time efficient when both approaches use the same number of hash tables. In all cases  , the multi-probe LSH method has similar query time to the basic LSH method. In comparison with the entropy-based LSH method  , multi-probe LSH reduces the space requirement by a factor of 5 to 8 and uses less query time  , while achieving the same search quality. To compare the two approaches in detail  , we are interested in answering two questions. We have developed two probing sequences for the multiprobe LSH method. LSH is a promising method for approximate K-NN search in high dimensional spaces. Since the entropy-based and multi-probe LSH methods require less memory than the basic LSH method  , we will be able to compare the in-memory indexing behaviors of all three approaches. For the image dataset  , the Table 2: Search performance comparison of different LSH methods: multi-probe LSH is most efficient in terms of space usage and time while achieving the same recall score as other LSH methods. By picking the probing sequence carefully  , it also requires checking far fewer buckets than entropy-based LSH. Instead of generating perturbed queries  , our method computes a non-overlapped bucket sequence  , according to the probability of containing similar objects. ever developed a LSHLocality Sensitive Hashing based method1  to perform calligraphic character recognition. higher Max F 1 score than ANDD-LSH-Jacc  , and both outperform Charikar's random projection method. We emphasize that our focus in this paper is on improving the space and time efficiency of LSH  , already established as an attractive technique for high-dimensional similarity search. We see that our method strictly out-performs LSH: we achieve significantly higher recall at similar scan rate. We use LSH for offline K-NNG construction by building an LSH index with multiple hash tables and then running a K-NN query for each object. We make the following optimizations to the original LSH method to better suit the K-NNG construction task: We use plain LSH 13  rather than the more recent Multi- Probing LSH 17 in this evaluation as the latter is mainly to reduce space cost  , but could slightly raise scan rate to achieve the same recall. For each dataset  , the table reports the query time  , the error ratio and the number of hash tables required  , to achieve three different search quality recall values. multi-probe LSH method reduces the number of hash tables required by the entropy-based approach by a factor of 7.0  , 5.5  , and 6.0 respectively for the three recall values  , while reducing the query time by half. The entropy-based LSH method generates randomly perturbed objects and use LSH functions to hash them to buckets  , whereas the multi-probe LSH method uses a carefully derived probing sequence based on the hash values of the query object. Our experiments show that the multi-probe LSH method can use ten times fewer number of probes than the entropy-based approach to achieve the same search quality. However  , this method does not use task-specific objective function for learning the metric; more importantly  , it does not learn the bit vector representation directly. Experimental studies show that this basic LSH method needs over a hundred 13 and sometimes several hundred hash tables 6 to achieve good search accuracy for high-dimensional datasets. The multi-probe LSH method proposed in this paper is inspired by but quite different from the entropybased LSH method. Finally  , we give the recognition result based on the searching results. It is a big step for calligraphic character recognition. We have implemented the entropy-based LSH indexing method. The default probing method for multi-probe LSH is querydirected probing. Intuitively  , increases as the increase of   , while decreases as the increase of . Locality Sensitive Hashing LSH 13  is a promising method for approximate K- NN search. Besides the random projections of generating binary code methods  , several machine learning methods are developed recently. The basic method uses a family of locality-sensitive hash functions to hash nearby objects in the high-dimensional space into the same bucket. We found that although the entropybased method can reduce the space requirement of the basic LSH method  , significant improvements are possible. The results in Table 2also show that the multi-probe LSH method is substantially more space and time efficient than the entropy-based approach. The basic LSH indexing method 17 only checks the buckets to which the query object is hashed and usually requires a large number of hash tables hundreds to achieve good search quality. Our results indicate that 2GB memory will be able to hold a multi-probe LSH index for 60 million image data objects  , since the multiprobe method is very space efficient. Our evaluation shows that the multi-probe LSH method substantially improves over the basic and entropy-based LSH methods in both space and time efficiency. However  , due to the limitation of random projection  , LSH usually needs a quite long hash code and hundreds of hash tables to guarantee good retrieval performance. We compare our new method to previously proposed LSH methods – a detailed comparison with other indexing techniques is outside the scope of this work. Ideally  , we would like to examine the buckets with the highest success probabilities. The two datasets are: Image Data: The image dataset is obtained from Stanford's WebBase project 24  , which contains images crawled from the web. Also note that the space cost of LSH is much higher than ours as tens of hash tables are needed  , and the computational cost to construct those hash tables are not considered in the com- parison. However  , since our dataset sizes in the experiments are chosen to fit the index data structure of each of the three methods basic  , entropybased and multi-probe into main memory  , we have not experimented the multi-probe LSH indexing method with a 60-million image dataset. For even larger datasets  , an out-of-core implementation of the multi-probe LSH method may be worth investigating. We have experimented with different parameter values for the LSH methods and picked the ones that give best performance . Spectral hashing SH 36  uses spectral graph partitioning strategy for hash function learning where the graph is constructed based on the similarity between data points. First  , when using the same number of hash tables  , how many probes does the multiprobe LSH method need  , compared with the entropy-based approach ? Locality sensitive hashing LSH  , introduced by Indyk and Motwani  , is the best-known indexing method for ANN search. Acknowledgments. This paper presents the multi-probe LSH indexing method for high-dimensional similarity search  , which uses carefully derived probing sequences to probe multiple hash buckets in a systematic way. Then the LSH-based method will be used to have a quick similarity search. Thus  , we utilize LSH to increase such probability. Note that one can always apply binary LSH on top of a metric learning method like NCA or LMNN to construct bit vectors. The dataset sizes are chosen such that the index data structure of the basic LSH method can entirely fit into the main memory. Figure 10shows that the search quality is not so sensitive to different K values. As we will show  , our method has better performance characteristics for retrieval and sketching under some common conditions. By probing multiple buckets in each hash table  , the method requires far fewer hash tables than previously proposed LSH methods. Our results show that the query-directed probing sequence is far superior to the simple  , step-wise sequence. These machine learning methods usually learn much more compact codes than LSH since they are more complicated. One of the well-known uni-modal hashing method is Locality Sensitive Hashing LSH 2  , which uses random projections to obtain the hash functions. On the other hand  , when the same amount of main memory is used by the multi-probe LSH indexing data structures  , it can deal with about 60- million images to achieve the same search quality. An interesting avenue for future work would be the development of a principled method for selecting a variable number of bits per dimension that does not rely on either a projection-specific measure of hyperplane informativeness e.g. The intention of the method is to trade time for space requirements. This is because that using the LSH-based method for similarity searching greatly reduced the time of  was about 0.004 second in our experiment  , which is very time-consuming in Yu's because it calculate the skeleton similarity between the input calligraphic character and all the candidates in the huge CCD. This method does not make use of data to learn the representation. Although LSH can be applied on the projected data using a metric learned via NCA or LMNN  , any such independent two stage method will be sub-optimal in getting a good bit vector representation. To achieve high search accuracy  , the LSH method needs to use multiple hash tables to produce a good candidate set. Locality Sensitive Hashing LSH 7 constitutes an established method for hashing items of a high-dimensional space in such a way that similar items i.e. Figure 8 shows some recognition results of five different calligraphic styles using our LSH-based method. Except for the LSH and KLSH method which do not need training samples  , for the unsupervised methods i.e. As we know  , most calligraphic characters in CCD were written in ancient times  , most common people can't recognize them without the help of experts  , so we invited experts to help us build CCD. In our system  , we use a standard Jaccard-based hashing method to find similar news articles. The probability that the two hash values match is the same as the Jaccard similarity of the two k-gram vectors . Since the similarity functions that our learning method optimizes for are cosine and Jaccard  , we apply the corresponding LSH schemes when generating signatures. Each perturbation vector is directly applied to the hash values of the query object  , thus avoiding the overhead of point perturbation and hash value computations associated with the entropy-based LSH method. For the entropybased LSH method  , the perturbation distance Rp = 0.04 for the image dataset and Rp = 4.0 for the audio dataset. A sensitivity question is whether this approach generates a larger candidate set than the other approaches or not. Baselines: We compare our method to two state-of-theart FSD models as follows. For methods SH and STH  , although these methods try to preserve the similarity between documents in their learned hashing codes  , they do not utilize the supervised information contained in tags. Figure 1shows how the multi-probe LSH method works. Since each hash table entry consumes about 16 bytes in our implementation   , 2 gigabytes of main memory can hold the index data structure of the basic LSH method for about 4-million images to achieve a 0.93 recall. In future we plan to make more comparison of our image representation and other descriptors  , such as SIFT and HOG. In addition  , dissimilar items are associated with the same hash values with a very low probability p 2 . In addition  , the construction of the index data structure should be quick and it should deal with various sequences of insertions and deletions conveniently. The key idea is to hash the points using several hash functions so as to ensure that  , for each function  , the probability of collision is much higher for objects which are close to each other than for those which are far apart. Then we run another three sets of experiments for MV-DNN. As pointed out by Charikar 5   , the min-wise independent permutations method used in Shingling is in fact a particular case of a locality sensitive hashing LSH scheme introduced by Indyk and Motwani 12. Also  , our method performs well in recognition rate and show robustness in different calligraphic styles. The SpotSigs matcher can easily be generalized toward more generic similarity search in metric spaces  , whenever there is an effective means of bounding the similarity of two documents by a single property such as document or signature length. For our proposed approach  , for both Apps and News data sets  , we first run three sets of experiments to train single-view DNN models  , each of which corresponds to a dimension reduction method in Section 6 SV-TopK ,SV-Kmeans and SV-LSH. We will design a sequence of perturbation vectors such that each vector in this sequence maps to a unique set of hash values so that we never probe a hash bucket more than once. In the test stage  , we use 2000 random samples as queries and the rest samples as the database set to evaluate the retrieval performance. Collaborative Tagging systems have become quite popular in recent years.