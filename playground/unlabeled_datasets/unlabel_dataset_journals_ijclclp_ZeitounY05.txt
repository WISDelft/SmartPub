The Formosan Language Archive at Academia Sinica 2 , Taipei, is part of the Language * Institute of Linguistics, Academia Sinica, Taipei, Taiwan E-Mail: {hsez, harryyu}@gate.sinica.edu.tw 1 Earlier drafts of this manuscript were presented in different occasions and among others, at The Fourth Workshop on Asian Language Resources, March 25, 2004, Sanya, Hainan Island, China and at the Tuesday Seminar at the Institute of Linguistics, University of Hawai'i at Mānoa on Feb.1, 2005.
We are thankful to all the participants for their helpful suggestions and comments.We are also grateful to two reviewers for their insightful comments that helped us revise an earlier version of this manuscript.
2 The Formosan Language Archive is located at: http://formosan.sinica.edu.tw/.
The project work team (headed by E. Zeitoun) includes/included* the following assistants and members.
@BULLET Language analysis: E. Zeitoun, Hui-chuan Lin, *Tien-hsin Hsin (Rukai) Tai-hwa Chu, E. Zeitoun (Saisiyat) Yu-ting Yeh, E. Zeitoun *Cui-wei Lin (Amis) Jia-jing Hua, E. Zeitoun (Paiwan) transcribed as faithfully as possible 4 .
The Formosan Language Corpora provide different types of search systems --sentence-based, paragraph-based, concordance-based, keyword-based, affix-based and lexical category-based --and preserve the original work recorded by earlier scholars by providing two kinds of display, cf., " original data " and " re-edited data, " which can be viewed separately or conjointly.
The geographic search system permits users to determine the geographical distribution of each language/dialect.
It is hoped that in the future, we will be able to further develop this system so that it will be possible to observe the expansion/decrease of a particular linguistic community over the last hundred years.
Another goal is to provide mappings of phonemes, lexical items (arranged in different semantic fields) and grammatical words to allow users to see the distribution of cognates within the Formosan languages and identify areal features.
The search system for the four bibliographical databases allows access to the latest information in publications about Formosan languages pertaining to linguistics, language teaching, literature and music.
The display of the Archive will not be further discussed in this paper, as it has been reported in more detail elsewhere (see Zeitoun et al.
[2003]).
Table 1.
Digitized texts in Chinese and English, as of June 2005 Language [2003] 2) E. Zeitoun & Hui-chuan Lin [1999][2000][2001][2002][2003][2004]21 7000 1200 65MB Maga Tien-hsin Hsin [2002] 24 3945 419 50MB 1) E. Zeitoun [1993][1994][1995][1996][1997][1998][1999][2000][2001]12 11281 899 60MB Tona 2) E. Zeitoun [2003][2004]8 3400 500 35MB Labuan E. Zeitoun [2003] The goal of the present paper is to discuss the linguistic analysis approach adopted in the Formosan Language Corpora and the processing programs that have been developed for it.
Indeed, the digitization of various Formosan languages and dialects has posed numerous challenges on both the linguistic and computational levels.
We have had to develop not only a uniform annotation system to account for language variation and typology but also processing tools for annotating the growing corpus and retrieving and displaying the data from/on the Internet.
This paper is organized as follows.
In section 2, we discuss problems related to the transcription of different corpora.
In section 3, we deal with annotation rules and standards.
In section 4, we turn to the notion of text structure.
In section 5, we discuss problems related to analytic and programming consistency.
Conclusions are drawn in section 6. 
In this section, we first deal with the orthographic system adopted in the Archive and then discuss IPA conversions from one operating environment (Word) to another (Web).
We first outline the phonemic inventory of the Formosan languages.
We then provide an overview of the diverse writing systems that have been used to transcribe the Formosan languages.
Finally, we deal with the problems raised by these writing systems, and explain our preference for using IPA for standardized transcription.
The Formosan languages exhibit fairly simple phonemic inventory systems consisting usually of no more than twenty consonants and four vowels, which typically include a series of voiceless and voiced stops: /p, t, k, q, /, b, d, g/; an affricate: /ts/; fricatives: /s, z/; a series of nasals: /m, n, N/; liquids: /l, r/; and four vowels: /a, i, u, ´/.
Of course, there is great variation among these languages which has arisen through phonological changes.
They will not be detailed in the present paper.
Most noticeably, Paiwan has developed a series of palatals: /c, Ô, ¥/; Rukai, Paiwan and Puyuma exhibit a partial/full series of retroflexes: /ˇ, Í, Ò/.
Atayal, Seediq, Bunun, Paiwan and Thao distinguish between velar and pharyngeal sounds, while Amis differentiates glottal and epi-glottal sounds [Li 1999].
A few languages such as Squliq Atayal, Tsou, Maga Rukai and Saisiyat have developed more complex vocalic systems.
All the consonants and vowels found in the Formosan languages are given in  <Equation_0> The basic syllable structure in most languages is CVC, though both Rukai and Tsou now exhibit a CV syllable structure.
Consonant clusters occur in only a few languages (e.g., Tsou, Maga Rukai, Thao and Atayal).
Stress is usually non-phonemic.
Different writing systems (alphabetic, syllabic and logographic) have been adopted to transcribe the Formosan languages during the past four hundred years.
Four stages can be distinguished that reflect the history of Taiwan.
The last of them is the most complex.
Dutch colonization (1629-1661): The Roman alphabet was first used in Taiwan in the 17th century by Dutch missionaries to record Siraya and Favorlang.
They devised a Romanization system based on the Dutch spelling, which at the time had not yet been standardized.
Chinese colonization (1661-1895): With the colonization of Taiwan by the Chinese, many land contracts, songs, place or family names and reports were transcribed with Chinese characters.
The phonetic value of these Chinese characters is somewhat complex, sometimes referring to Mandarin Chinese and at other times to the Minnan pronunciation.
Japanese colonization (1895-1945): From 1895 to 1945, Taiwan was a Japanese colony.
Aboriginal children were enrolled in schools (up to the age of 12) and learnt Japanese, so they were able, in later years, to transcribe their own language in katakana.
Post-1945: With the arrival of the Nationalist Chinese under the leadership of Chiang Kai-shek, the Chinese government imposed Mandarin Chinese as the sole official language.
The Zhuyin fuhao system more popularly known as Bopomofo, was introduced and used in textbooks, dictionaries etc.
At one time, it was also used to transcribe the Formosan languages (e.g., the Bible, songs and textbooks).
Bopomofo consists of 37 symbols derived from Chinese characters, and some of these symbols were slightly altered to convey sounds recorded in the Formosan languages that are not found in Chinese.
Different writing systems (all Romanized) were devised by the Catholic and the Protestant Church and used during the same period.
The lack of adherence to common principles had the unfortunate consequence of producing different writing systems for different tribes.
Diacritics were introduced: in Amis, for instance, ^ is used to represent a glottal stop.
In 1991, Prof. Li Jen-kuei [Li 1992] was asked by the Ministry of Education of Taiwan to devise writing systems for the Formosan languages and proposed different solutions (e.g., replacing IPA symbols such as N with a capital letter N or with two symbols, ng).
In 2002, linguists were asked by the Council of [Taiwan] Indigenous Peoples, Executive Yuan, to work in collaboration with each tribe according to their individual expertise and finalize the orthographic system(s) of all the aboriginal languages of Taiwan.
This has also led to a variety of Romanized systems that try to improve on the Romanization systems of the Catholic and Protestant Church while taking into account Li's [Li 1992] recommendations.
We will not discuss problems with earlier writing systems (the Dutch-based transcription system and the use of Chinese characters and symbols) as these have been addressed elsewhere (see, for instance, Adelaar [1999] and Rau [1995]).
We will, rather, focus on the inconsistencies in the Romanization systems, devised either by missionaries or linguists.
The various Romanization systems devised by missionaries were not usually based on a phonemic representation of the language being transcribed.
This had, in many cases, an unfortunate consequence: a relevant phonemic contrast was not represented while other non-distinctive features were taken into account.
Early et al.
[2003:15] showed, for instance, that in Paiwan the orthography used by two Swiss Catholic missionaries did not distinguish between the two phonemes /tj/ and /dj/ but represented both phonemes with a single graph, cf., tj.
Li [1992:21] also noted that an orthographic system was devised for Paiwan whereby a distinction between /T/ and /s/ was made, but such a contrast does not exist in that language.
No common principles have been applied to the Formosan languages nor have they been consistently adopted among linguists.
In Amis, for instance, d represents the lateral fricative /¬/, but in all the other Formosan languages, it refers to a dental /d/.
Blust (see Blust [2003]) transcribes [T] as c (to show phonological change, PAN *C > Thao T), while most other linguists transcribe [T] as th.
Table 3provides a comparison of the various symbols used to transcribe the Formosan languages along with their IPA equivalents.
<Equation_1> To overcome the problem of non-standardization in the current writing systems, we decided to record or re-edit texts in IPA, a recommended standard used in many Archive projects (e.g., the Rosetta Project).
However, to preserve the integrity of earlier recorded data, we keep intact original materials recorded with certain Romanization systems and produce new versions of these based on our own standardized format.
It became necessary for us to make changes in our corpus, as we were including more and more languages.
The first languages we started to digitize and to annotate were Rukai, Atayal and Tsou.
The commonly accepted use of c in Formosan linguistics as a replacement for [ts] seemed at the time the best choice 6 , as there are consonant clusters in three of these languages, cf., Maga Rukai, Squliq Atayal and Tsou.
However, the introduction of Paiwan, in which there is a distinction between palatalized and non-palatalized sounds, forced us to change our writing policy though, as c is the standard IPA symbol used to represent a palatal stop.
We thus changed the earlier c to ts, to distinguish the affricate [ts] from the palatal [c].
Other changes may be needed in the future as we include more languages, but we plan to keep them to a minimum.
To convert IPA symbols from Word documents (in which texts are typed) to the Web, we make use of the Unicode encoding system, which offers the possibility of displaying symbols uniformly across browser platforms.
In Unicode, each IPA character is assigned a standardized encoding number so as to avoid using the same code for two different symbols.
In theory, Unicode represents the best way to display IPA characters on the Web.
In practice, it requires an initial configuration.
Displaying IPA symbols on certain platforms is sometimes difficult as will be shown below (Webster [2002]).
This section discusses how we use IPA in our two working environments (Word and the Web) and how we convert IPA symbols into a computer-readable form.
Three things are required to convert IPA symbols from word processing documents into 3.
A Unicode-compliant application (e.g., Microsoft Word or Internet Explorer).
All the texts included in the Formosan Language Corpora contain different kinds of information: metadata information, utterance identifications, orthographic transcriptions, interlinear word-glosses and free translations.
Specific IPA symbols are introduced in the files whenever necessary.
We make use of the Unicode-compliant font SILDoulosIPA, made available through SIL.
The data is typed as follows: (Strictly speaking, a Word document is not an ASCII text file, as it may contain formatting code (e.g., indenting, italics, etc.)
and IPA symbols, which are challenging for computer processing.
It is thus necessary to convert these phonetic symbols into computer-readable forms.
Thus the interoperability can be achieved on another application or platform.
A macro can be used to transliterate IPA symbols as decimal numeric entities.
For instance, the D character is rendered by the HTML code &#240.
Each IPA symbol is automatically converted into its corresponding numeric reference entity throughout a document.
When this operation is finished, we import these alphanumeric characters into the textual database.
Once the database has been established, the query operation can be performed as desired.
To display IPA symbols in Web pages, some preliminary work must be done by the user, i.e., his/her computer must be configured with a Unicode IPA font and a Unicode-compliant browser for viewing IPA symbols on the Web.
Internet Explorer automatically views web pages encoded with UTF-8, an encoding standard, provided that an appropriate font is In order to display Unicode IPA Web pages, we declare that the HTML page is using: (2) <head> <meta http-equiv="Content-Type" content="text/html; charset=utf-8"> ... </head> Then, we need to either specify the name of the font locally, e.g., Lucida Sans Unicode as: (Our database keeps track of all the graphs and symbols in each corpus.
In other words, not only the Romanization systems but also the numeric reference entities for IPA symbols are stored.
This means that when a query is issued from the user's machine, the request is then sent to the server application, which sends the query command to the back-end database, producing a query result that satisfies the initial criteria.
The result is then sent back to the server program, which finally produces the HTML output for the user.
Our web application is oriented to both browsing and searching the corpus.
Either method displays the HTML output, including the IPA codes (if any), and finally displays it in the client browser.
If the client computer has the appropriate font installed, e.g., Lucida Sans Unicode, then the IPA symbols are guaranteed to be displayed correctly; if not, the user's web browser will display "????"
or empty boxes .
As briefly outlined above (see section 1), the Formosan Language Archive not only permits the browsing of texts, but also allows for searching based on (i) keywords, (ii) list of affixes and (iii) lexical categories.
While the search through affixes and lexical categories is rather simple, as the user browses a separate database 7 , keyword search is one of the most important features of the Formosan Language Archive.
The search can be made by typing a word in any of the Formosan languages included in the corpora, its Chinese or English translation or glosses.
Of interest for us is searching performed by typing a word in a specific Formosan language.
Since each corpus includes IPA symbols, the type of search must also handle these.
The two applications we are using, Microsoft Word and Internet Explorer, do not allow the automatic insertion of Unicode IPA.
However, it is easier to insert manually IPA symbols in Word than in Internet Explorer.
The insertion of IPA symbols will first be discussed here with respect to these two environments.
We will then explain how we devised a keyboard mapping mechanism that allows the insertion of IPA symbols on the Web.
In Microsoft Word (e.g., 2000/XP), there are several ways to insert a Unicode IPA symbol.
The first is the well-known Insert…Symbol menu command.
After Insert…Symbol is chosen, a Unicode font is then selected, the pull-down list on the right displays all of the Unicode code points (such as " IPA Extension " ) included in that font.
The second method consists of using the AutoCorrect feature, which is designed to replace mnemonic abbreviations with their Unicode IPA equivalents.
This method is handy, but a constraint is placed on codes.
They must all begin and end with a non-alphabetic character (see Webster [2002]).
A third method consists of inserting IPA symbols using the find/replace function.
It is extremely difficult, if not impossible to insert Unicode IPA symbols when using Web browsers like Internet Explorer.
Such symbols, if inserted, usually become empty boxes in the field.
To display such symbols, we decided to design a keyboard in which all occurring IPA symbols (so far, 15) along with their numeric equivalents could be displayed (Figure 1).
When the user clicks on one of the IPA buttons, the reference code is inserted into the field automatically, and the code is enclosed by " less than " and " greater than " marks (e.g., <660>).
The reason for not inserting the typical reference entity (e.g., &#660;) directly is that the ampersand character is significant for Web processing.
When the field data is posted onto the server, the Web application can manipulate it due to its computer-readability.
In the server, each of the posted IPA symbols is converted back into the standard entity (e.g., $#660;).
During this process, we can guarantee that the search string is kept undistorted when sent to the server.
It should be noted, however, that a few IPA symbols are able to appear AS IS in the field.
Even so, these symbols would be urlencoded 8 into unexpected character strings which would be hard for the program to parse.
When we started digitizing data on the Formosan languages and were confronted with the insertion of IPA symbols on the Web, we found the above method most acceptable.
The sole limitation is that users must have installed Unicode IPA symbols beforehand to take advantage of this type of input mechanism.
Figure 1.
IPA Keyboard Mapping 
The use of language codes is necessary when constructing the ontology of different Formosan language families included in the corpora.
Our coding system is actually based on the latest version of Ethnologue, which was developed by the Summer Institute of Linguistics and is available on the Internet (e.g., DRU for Rukai and BNN for Bunun).
As the SIL website does not provide abbreviated names for dialects.
We use a two-letter code based on the dialect name itself (e.g., Mn from Mantauran Rukai).
Thus, the language and the dialect codes form distinct entries in our database.
The codes used for the Formosan languages (along with the dialects they include) that are being archived are shown in The Formosan languages are morphosyntactically heterogenous, and though the literature on a number of Formosan languages is now much more abundant than it used to be, many grammatical phenomena have yet to be clarified or need to be further investigated.
This poses a challenge for the analysis of each Formosan language corpus that we deal with, as will be explained below.
As pointed out by Zeitoun et al.
[2003], each text is annotated based on linguistic annotation standards.
The transcription of a text in the original language is divided into utterances, sentences and clauses.
Words are glossed, and sentences are given free translations.
Glosses (or tagset) can be provided at two different levels: the word level (stems) and at the morphemic level (roots and affixes).
The major difference between these two types of annotations lies in the fact that glosses at the word level might provide only a vague interpretation of a word and render its word formation opaque.
In the texts that have been collected for the Formosan languages (e.g., Tung [1964], Li [1975]), we find that this interpretation is most often context-based (i.e., subject to the context of the whole sentence).
At the morphemic level, on the other hand, roots and affixes as well as morphological alternations must be identified and further analyzed.
Since we started our research in 2001, we have applied a morphemic analysis to annotate all the texts that have been recorded or re-analyzed by ourselves.
This method has many advantages in spite of its shortcomings (see below).
First, the linguist can annotate the corpus consistently, i.e., words are not " contextually " glossed but their " core " meaning is sought.
Second, it helps to determine the distribution and meaning of nearly each affix, thus allowing construction of an affix database.
Third, it deepens one's understanding of the grammar of a specific language, making it easier to identify major lexical and syntactic categories (also included in a database).
The first corpus was annotated in 2001 and focused on only one dialect of Rukai, Mantauran.
Over the past four years, as different languages have been annotated, we have been obliged to add more abbreviations to our original list, taking into account morphosyntactic distinctions that exist in these languages.
This does not pose a problem, as far as linguistic analysis is concerned, because we know that the Formosan languages exhibit much typological variation.
As our abbreviation list was discussed in Zeitoun et al.
[2003], we will only deal in this section with problems that have arisen due to inclusion of more languages in our corpora.
The addition of new abbreviations has had two different consequences: (i) the use of particular glosses for a single language, and (ii) the insertion of new symbols to distinguish different types of affixes.
We will discuss these two consequences in turn below.
Some of these abbreviations are (so far) only used for one language.
In Atayal, for instance, there is a distinction between the immediate progressive and remote progressive (cf., nyux vs. cyux).
As progressive auxiliary verbs have grammaticalized from earlier existential verbs that still co-occur productively in this language, the same immediate/remote distinction is also found in these existential verbs.
This dichotomy has been reported in Seediq, a language from which collections of texts ready for digitization have not yet been retrieved.
Atayal is, thus, the only language in our corpora that makes use of these four abbreviations.
Other abbreviations, e.g., AF, PF, Red and LocNmz, are much more common and widely spread cross-linguistically.
One of the most important changes we have had to make has been the insertion of brackets <>, commonly used to delimit infixes and recommended by the Max Planck Institute, Leipzig 9 .
Initially, that symbol was not used in our glosses because in the languages that we were annotating (Rukai, Tsou, Atayal and Saisiyat), two infixes barely co-occur simultaneously.
In Saisiyat, for instance, though the combination S<om><in>B´t [beat<AF><Perf>beat] 'beat' is grammatically correct, it was not found in our corpus.
Originally, if we had a word like SomB´t 'beat' to annotate, we would use hyphens to show its word formation, cf., S-om-B´t [beat-AF-beat], following a common practice among Formosanists.
The introduction of two new languages, Bunun and Paiwan, forced us to use brackets instead, as the occurrence of two infixes in these languages is quite productive.
Our newest abbreviation list is shown in Table 5.
Abbreviations are given both in English and in Chinese, as one of the major goals of the Formosan Language Archive is to build a multilingual corpora in which the original orthography and Chinese-English translations co-exist.
Our annotation system is not without shortcomings which we are well aware of.
First, though our morphemic analysis allows for the development of different (re)search tools (e.g., keywords, list of affixes and lexical categories), the reading of a word without a whole translation of the sentence is nearly impossible for someone not familiar with Formosan languages.
To cope with this problem, lists of lexical categories have been made for each corpus that allow the user to search for a word, to determine its word formation, to check for related words and to understand its meaning.
Second, morphemic analysis can be performed only if a language is well understood by the analyst.
Though the project leader trained for many years aboriginal assistants in linguistic analysis, and is supervising the development of each corpus to help make the consistency rate higher through the use of the same terminology, it has become clear that to overcome analytical problems, the participation of more language specialists in the development of each different corpus is crucial.
Third, while users can cross-reference rather easily both " original " and " linguistically re-annotated/re-edited " data files, our system can not display the phonetic/phonemic transcriptions of languages, as in the case of Maga Rukai for instance, where morphophonemic alternations render systematic morphemic analysis opaque.
This inoperability of our system results from the fact that only a few languages exhibit such dense internal variation so that it is hard to generalize a program for the whole corpora.
But this limitation has been solved by adding columns pertaining to morphophonemic alternations in the databases for lexical categories.
Other shortcomings (e.g., inconsistencies in glosses or " wrong " analyses) have been either remedied through the development of new programs or can be resolved through follow-up revisions and corrections of earlier corpora.
Table 5.
Abbreviations used in the Corpora ABBREVIATION To help with annotation of the corpora, a program called AnnoTool (see Figure 2) has been developed.
It has two main functions: it facilitates the tagging of texts and the translation of the linguistic terminology from English to Chinese or vice versa.
AnnoTool has been designed to work with Microsoft Word.
The user can have both programs running concurrently.
However, it is necessary to arrange the desktop so that the two windows do not overlap each other.
As shown in Figure 3, AnnoTool usually occupies one-third of the screen, and Word two-thirds.
When the user clicks on one of the buttons in AnnoTool, a tag is inserted into the Word document automatically.
This method makes linguistic analysis more efficient and more accurate.
It is more efficient because the linguist can view the on-screen list and stick to pre-defined terminology.
It is more accurate because the likehood of introducing typos is kept to a minimum.
Labels can be translated from English into Chinese, or vice versa.
To do so, the user must first select a single term or an entire line from a document and then switch to AnnoTool and click on English→Chinese (or Chinese→English) in the Translate menu.
Accordingly, the selected sequence in Word can be translated into one of these two languages.
We are conscious that one limitation of AnnoTool is that it has been programmed to handle a specific terminological set.
It does not deal with the literal translation of lexical words or phrases.
Nevertheless, using this tool makes our linguistic analysis easier than it used to be.
Figure 3.
Using AnnoTool with Word For the tagging of major lexical categories, we follow – though with some reservations – the standardization established by CKIP in charge of the Academia Sinica Chinese Corpora.
Not all of the lexical categories devised by CKIP are found in the Formosan languages, and conversely, some lexical categories not listed by CKIP are necessary to describe the Formosan languages, as illustrated in Tables 6 and 7.
The set of lexical categories has been improved since two more languages (Atayal and Saisiyat) other than Rukai 10 were tagged. 
In this section, we deal with linguistic " recognition " of clause/sentence and paragraph boundaries, and the programs that have been developed to obtain from the Internet a parallel alignment of words, glosses and sentences both in Chinese and in English.
Table 6.
A comparison of existing lexical categories in Chinese and in Formosan languages : lexical category found in Rukai or in other Formosan languages As far as linguistic data is concerned, two major factors help in the recognition of clauses/sentences: intonation and syntactic structure.
We transcribe every text based on voice files that are recorded and digitized.
Though we have not taken into account nor have we tried to provide the duration of each word, intonation plays quite an important role in the detection of clause/sentence boundaries.
The analyst's knowledge of the language also helps him/her determine the beginning and the end of a clause vs. that of a sentence.
To give but one example, in Tona Rukai, si 'and' can appear at the end of a sentence or between two nouns or two clauses.
Syntactically speaking, it thus functions as a phrasal or causal coordinator/conjunction.
In terms of discursive practices, it is used to mark a pause.
That pause can be perceived as " long " (as in (5)), in which case the analyst has to treat the clause as a full sentence, or as short (as in (6)), in which case, two clauses will be treated as being coordinated and forming a longer sentence.
In accordance with annotating conventions, the transcription of a text is divided into sentences, which are further segmented into space-delimited words.
There are two types of translations: glosses at the word level and free translations at the sentence level.
Sentences are numbered for reference purposes.
The encoded format of the reference number is xx-xxx-x, where the first part refers to the text id, the second indicates the paragraph id, and the third corresponds to the sentence id 11 .
Each utterance or sentence contributes to the concept of " one block. "
A block thus includes: (i) the reference information, (ii) the original utterance or sentence, (iii) word glosses and (iv) free sentential translations.
The annotated data has a three-level hierarchy.
It includes the "text, " the "word" and the "sentence."
Transcriptions, glosses and translations are associated with one of these three levels.
Metadata is associated at the text level.
The structure is hierarchical in that a text contains sentences and words.
Based on this hierarchical structure, it is easy for a computer to handle a text as an object (see Jacobson et al.
[2000]).
A parse program was written to extract sentence and word objects from each corpus.
The location of each sentence, their translations and other related information are stored in the sentence-level database (Figure 4).
The locations of words, their transcriptions, their word order, Chinese and English word glosses, and punctuation are stored into the word-level database (Figure 5).
The location field, as a primary key, is used to relate one database to another.
Figure 4.
Sentence-level database Figure Figure 6.
XML markup of a linguistic text According to our conventional notations, sentences have been aligned since the first corpus (that for Mantauran Rukai) was initially built on a sentence-by-sentence basis.
Then the Chinese and English translations were appended.
They are clearly distinguishable for distinct line position in the file: We did not have anything.
(Zeitoun and Lin [2003, ex.
013-165-b]) To keep the sentences aligned, our approach maps the linking relationships of the sentence segments and stores or encodes them in a standard format.
In other words, the sentential information is stored in the individual fields of a record or in certain XML node elements.
In the Formosan Language Corpora, each uttered word is space-delimited and owns its bilingual glosses appear below it.
If no gloss is available, then an asterisk * replaces it: Interlinear morpheme-by-morpheme glosses provide most of the information necessary to build a word alignment database.
In the database design, each record is based on a transcribed word.
This lexical unit includes important pieces of information, such as a unique identifier (here called a location), a spelled form, a specific word order, and glosses.
Word order plays a major role in word arrangement.
It allows words (along with their glosses) to be pieced together and to reappear in the same order as in the original format.
Word alignment provides a basis for the extraction of bilingual lexicons.
Using the alignment database, we can get the full index of a particular language.
However, as each word is deliberately cut into pieces corresponding to morphemes rather than being given a literal meaning, it is impractical to put them together in the order of the source language since the result would be incomprehensible gibberish.
That is why we provide a lexical category search, which allows the user to browse the meaning of each word, and a reference to its word formation (see section 3.1).
Our aligning strategy thus consists quite simply of arranging words with their been/are being adopted for other Rukai dialects and other languages whenever necessary. 
There is no translation at a higher level than the sentence, so there is no need for a paragraph-level table.
The free, sentence-level translations can be strung together and arranged in the original order, and they serve as intelligible, if not always smooth or elegant, translations of the whole text.
Morphology plays a crucial role in understanding the Formosan languages, and the morphemic method we have adopted to annotate each corpus has forced us to deal even more carefully with word formation.
The analyst is confronted with two major problems, (i) the incorrect identification of morpheme boundaries, and (ii) the restricted distribution of certain affixes or roots that might render their use and functions opaque.
Blust [Forthcoming] states that " most AN languages can be characterized as agglutinative-synthetic. "
Our assumption is based on the fact that morphemes can either be free or bound and can include roots, function words, clitics and affixes and on the fact that morpheme boundaries are usually clear.
However, morpheme boundaries might also be difficult to identify, and linguists sometimes propose different approaches to analyzing for the same words.
The first problem that has to be settled is whether a word is composed of one or two morphemes.
It happens that in some languages/dialects, certain words are no longer divisible, though historically, an affix could be identified.
That is the case with the word /oponoho 'name of a tribe (Mantauran) or the place they inhabit', which derives from the prefixation of /o-(<swa-from) to ponoho (<ponogo 'name place').
Different analyses from ours are found in the literature, and we must take them into consideration.
In Saisiyat (Chu [2003]), for instance, we analyze /i//ik as a ligature, i.e., a grammatical word that carries no lexical meaning.
These two morphemes occur in complementary distribution and must be glossed slightly differently, /i as 'Lig' and /ik as /i-k 'Lig-Stat'.
The first occurs before dynamic verbs and the second before stative verbs.
Li [1999], on the other hand, has analyzed both morphemes as sometimes bound and sometimes free, and translated them as 'not'.
Some morphemes are invariable.
Because their distribution is very much restricted and their morphophonemic/morphemic alternations are nonexistent, it might be difficult to determine their roots, their origins, their lexical categories.
This is the case with Mantauran Rukai tila!
which translates as 'Leave/Go away' but is actually formed with a first person plural pronoun t(a)-adjoined to what was originally the root ila.
This type of analysis can only be drawn on external evidence, and as mentioned above, necessitates a good understanding of the language being investigated.
Likewise, some affixes are very non-productive, and it might be difficult to determine their meaning.
This is the case with Mantauran Rukai ta/aDa/an´'an´'house warning' (< Da/an´' an´'house'); the meaning of ta/a is still poorly understood.
The major problem that the linguist must be aware of regarding syntactic structures has to do with typological diversity.
For instance, in Mantauran and Labuan Rukai, though subordinate temporal clauses are superficially identical, in the former, the subject is marked by the genitive, and in the latter, it is marked by the nominative.
'Yesterday, after I had eaten, I left.'
From the processing perspective, a hyphen is used as a morpheme boundary and as such provides morphemic information that can be used to parse word tokens (e.g., om-ia-nai 'Dyn.Fin-so-1PE.Nom') without difficulty.
To remedy inconsistencies in transcriptions and glosses, all the words can be extracted from the corpus data to create an index.
This index list (or finderlist) enables the analyst to compare all the words in order to minimize incorrect spelling or glosses.
This program can also output a frequency list of morphemes (Hockey [1998]).
Initially, the design of each database had to take punctuation into account.
We treat a space between two words as a punctuation mark, so every word can be said to have an associated punctuation mark.
Although this mark indicates a boundary between a group of words, in practice it is connected to the preceding word.
Following this approach, we can treat punctuation as a field of the preceding word, as shown in Figure 5(Leech et al.
[1995]).
At a very early stage in the development of the Formosan Language Archive, a program called Chkgloss was designed to verify the rigid structure of the corpus by comparing the number of orthographic words with that of their glosses (see Figure 7).
In each corpus, transcribed words are aligned vertically with their interlinear glosses.
As mentioned above, to guarantee that transcribed words are the same in number as their glosses, an asterisk is used to represent an empty word (whose meaning or morphosyntactic function remains opaque).
It is only after the verification process is completed that a text can undergo whole transformation and be displayed on the Internet.
Figure 7.
A Screenshot of Chkgloss Chkgloss is helpful for identifying errors because it provides the consistency rate between (i) each tagged word and its gloss and (ii) each sentence and its bilingual translation.
In most cases, a corpus has to undergo back-and-forth processing several times before it can be deemed to be valid (Figure 8).
Figure 8.
The workflow of using Chkgloss 
The Formosan Language Archive is a useful tool for conducting research on the Formosan languages.
The multilingual comparable corpora have begun to find their way in linguistic applications and natural language processing.
As far as linguistic applications are concerned, each corpus features well-analyzed data that can serve as a basis for more in-depth studies.
There are a number of advantages in providing word alignment, sentence alignment, linguistic annotations and bilingual translations.
Computer-aided linguistic research is being carried out using tools and techniques that improve the work of the analyst.
Applications that were developed for the Formosan Language Archive include Unicode IPA symbols, AnnoTool, Chkgloss and Indexer.
Drawbacks are inevitable, however.
If suitable electronic text versions had been available, progress would have been more rapid.
Admittedly, a lot of time has been spent on reformatting the legacy data to make it computer readable.
In addition, electronic versions of earlier published materials have to be made from scratch, since there were previously no electronic files (e.g., Li [1975], Tung et al.
[1964], Fey et al.
[1993] etc.).
It is the purpose of our project to collect, analyze and digitize data on many, if not all, of the Formosan languages for which texts are available, but more corpora need to be included to refine the original architecture of the archive.
On the other hand, we also need to think about how to develop new tools, make use of existing tools described in the literature (cf., Szakos et al.
[2004]) and process the voice files for further research (e.g., phonetic and discursive studies) 12 .
We might also be able at some point to conduct an experiment on natural language processing (e.g., corpus-based machine translation). 
