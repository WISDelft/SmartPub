A given word may have multiple meanings, and incorrect word sense recognition may reduce system effectiveness in semantic-oriented applications.
Word sense disambiguation (WSD) identifies the correct sense of polysemous words, and it has emerged as a useful technique for 182 Liang-Chih Yu et al.
many applications, such as machine translation (MT) (Carpuat & Wu, 2007; Chan et al., 2007), lexical substitution (McCarthy, 2002; Dagan et al., 2006), information retrieval (IR) (Agirre et al., 2010), and biomedical applications (Schuemie et al., 2005; Stevenson et al., 2012).
For example, in machine translation, WSD can be used to determine the correct translation for an ambiguous word.
In lexical substitution, it is used to determine whether or not a target word can be replaced by another word (e.g., a near synonym) by determining whether both words share a common sense.
Currently, WSD has been a critical component in the SemEval workshop 1 series (Kilgarriff & Palmer, 2000; Edmonds & Kilgarriff, 2002; Agirre et al., 2009).
Navigli (2009) provides an extensive survey of WSD approaches, investigating various features and machine learning algorithms to address specific tasks.
For example, bag-of-words, n-grams, part-of-speech (POS) tags, and syntactic and semantic information have been used to build WSD systems with machine learning algorithms (Lee & Ng, 2002; Ando, 2006; Tratz et al., 2007; Cai et al., 2007; Agirre & Lopez de Lacalle, 2007; Specia et al., 2007).
Word sense annotated corpora, such as SemCor (Miller et al., 1993), LDC-DSO (Ng & Lee, 1996), Hinoki (Kasahara et al., 2004), and sense annotated corpora constructed with the help of Web users (Chklovski & Mihalcea, 2002) are also useful resources for building WSD systems.
This paper proposes the use of multiple contextual features, including the predicate-argument structure and named entities, to train two commonly used classifiers: Naïve Bayes (NB) and Maximum Entropy (ME) from the OntoNotes corpus, a multilingual corpus of large-scale semantic annotations, including word senses, predicate-argument structure, ontology linking, and coreference (Hovy et al., 2006; Pradhan et al., 2007a).
We then examine whether the two proposed features can improve WSD performance.
The rest of this work is organized as follows.
Section 2 gives a brief description for the OntoNotes Corpus.
Section 3 presents the features used to train classifiers for WSD.
Section 4 summarizes the experimental results.
Conclusions are drawn in Section 5. 
The OntoNotes corpus contains a set of sentences with word senses annotated.
In the word sense inventory, the sense definitions of words are created by manually grouping fine-grained sense distinctions obtained from WordNet (Fellbaum, 1998) and dictionaries into more coarse-grained senses.
There are two reasons for this grouping instead of using WordNet senses directly.
First, people have trouble distinguishing many of the WordNet-level distinctions in real text and make inconsistent choices; thus, the use of coarse-grained senses can improve inter-annotator agreement (ITA) (Palmer et al., 2004;.
Second, improved 183 ITA enables machines to more accurately learn how to perform sense tagging automatically.
Sense grouping in OntoNotes has been calibrated to ensure that ITA averages at least 90%.
Table 1shows the OntoNotes sense tags and definitions for the word arm (noun sense).
Once the sense definitions are created, the sense of words in the sentences can be annotated.
To accomplish this goal, the sentences containing the words in the inventory are retrieved first.
For each target word (i.e., a word in the inventory) in the sentences, its sense is annotated by two annotators, according to its sense definitions in the inventory.
If the two annotators agree on the same sense, then their selection is stored in the corpus.
Otherwise, the sense annotation is double-checked by an adjudicator for final decision.
Recently, the OntoNotes corpus has been used for many applications, including the SemEval-2007 evaluation (Pradhan et al., 2007b), sense merging (Snow et al., 2007), class imbalance problems (Zhu & Hovy, 2007), sense pool verification (Yu et al., 2007;, parsing and named entity recognition (Finkel & Manning, 2009), semantic role labeling (Che et al., 2010), and coreference resolution (Pradhan et al., 2011). 
The features used to build the WSD system include POS tags, local collocations, bag-of-words, named entities, and predicate-argument structure.
These features are extracted from the OntoNotes corpus as follows.
<Equation_0> , relative to the POS tag of the target word.
For instance, the POS sequence of the constituent " …mediator in an attempt to break the… " is " NN NN IN DT TO VB DT " .
Local Collocations: This feature includes single words and multi-word n-grams.
The single words include (W -3 ,  <Equation_1> , relative to the target word W 0 .
Similarly, the multi-word n-grams include (W -2, <Equation_2> ).
For instance, the multi-word n-grams of the above example constituent include {in_an, an_to, to_break, mediator_in_an, in_an_to, an_to_break, to_break_the}.
Bag-of-Words: This feature can be considered a global feature, consisting of 5 words prior to and after the target word, without regard to position.
Liang-Chih Yu et al.
Named Entity: OntoNotes Release 1.0 2 provides 18 types of named entities, such as PERSON, ORGANIZATION, GPE, LOCATION, and PRODUCT.
Predicate-Argument Structure: The predicate-argument structure captures the semantic relations between the predicates and their arguments within a sentence.
Consider the following example sentence.
The argument label Arg0 is usually assigned to the agent, causer, and experiencer, while Arg1 is usually assigned to the patient.
The ArgM-TMP represents a temporal modifier (Babko-Malaya, 2006; Palmer et al., 2005).
The predicate-argument structure of the above sentence is illustrated in Figure 1.
The semantic relations can be either direct or indirect.
A direct relation is used to model a verb-noun (VN), whereas an indirect relation is used to model a noun-noun (NN) relation.
Additionally, an NN-relation can be built from the combination of two VN-relations with the same predicate.
Table 2presents some examples.
For instance, NN1 can be built by combining VN1 and VN2.
Therefore, the two features, VN1 and NN3, can be used to disambiguate the noun arm 3 . 
OntoNotes Release 1.0 was used as the experimental corpus, with a total of 992 words in the sense inventory.
Not all words, however, were polysemous, and some had a small number of sense annotated sentences.
Therefore, we selected 477 polysemous words (247 nouns and 230 verbs) with at least 30 annotated sentences as the test data for the WSD task (see Table 3).
The annotated sentences then were used to train two classifiers, Naïve Bayes (NB) and Maximum Entropy (ME), using the features presented in the previous section.
We first trained the two classifiers using the baseline features, including the POS tag, local collocations, and bag-of-words.
The named entities and predicate-argument structure then were added into both classifiers to determine whether these two features could improve WSD performance.
The baseline classifier used for comparison was implemented using the principle of most frequent sense (MFS), with each word sense distribution retrieved from the OntoNotes corpus.
The evaluation metric was accuracy, defined as the number of correctly identified senses (sentences) divided by the total number of test sentences.
Table 4shows the experimental results with 10-fold cross validation.
The symbols B, PA, and NE in Table 4represent the baseline features, predicate-argument structure, and named entities, respectively.
For comparison of the classifiers, ME outperformed NB for all feature sets.
For comparison of the feature sets, both B+PA and B+PA+NE outperformed B for both NB and ME, indicating that using both predicate-argument structure and named entities can improve performance over using the baseline features alone.
Another observation is that the predicate-argument structure was more sensitive to ME than to NB because the improvement of B+PA over B in ME was greater than that in NB.
Conversely, the named entity was more sensitive to NB.
For more detailed analysis, Tables 5 and 6 list the WSD accuracy for parts of the nouns and verbs in the OntoNotes inventory.
These words were also included in the SemEval-2007 English Lexical Sample Task (Pradhan et al., 2007b).
The " # sense " column lists the number of sense distinctions of a word, and the column " MFS " presents the sense distribution among all senses of the word.
Both the number of sense distinctions and the sense distribution of words may affect WSD performance.
Generally, a large number of sense distinctions with an even distribution may lead to confusion among the classifiers, hence, lower performance.
For example, the noun defense in Table 5has seven senses, and the proportion of the major sense is 0.282, indicating an even distribution (the distribution of the 7 senses is {.14, .18, .19, .08, .04, .28, .09} in the OntoNotes corpus), thus yielding low accuracy.
The verbs go and make in Table 6also have similar results.
Conversely, a small number of sense distinctions with a skewed distribution may have better performance.
For example, in Table 5, the noun rate with a dominant sense of 0.924 yielded high accuracy, as did the verb tell in Table 6.
To further analyze the effect of the sense distribution of words in the whole corpus, we ranked the 247 nouns and 230 verbs in OntoNotes in descending order based on the proportion of their major senses.
Nouns and verbs with major sense proportions within a given range then were grouped together (e.g., >=0.95, 0.90~0.95, 0.85~0.90, …, 0.35~0.4, and <0.35), and their average accuracy was calculated for comparison.
Figures 2~4 present the results of nouns, verbs, and all words, respectively, with accuracy gradually decreasing as the sense becomes more evenly distributed.
Another interesting observation is that, although ME outperformed NB, ME and NB achieved similar performance when the sense distribution became more Word Sense Disambiguation Using Multiple Contextual Features Figure 4.
WSD performance against sense distribution for all words. 
A WSD system was built from the OntoNotes corpus using multiple contextual features to analyze the effect of sense distribution on WSD performance.
Experimental results show that both the predicate-argument structure and named entities improved WSD performance.
In addition, there was a tendency for a skewed sense distribution to yield higher performance than evenly distributed word senses.
Future work will focus on improving WSD performance by investigating more significant features and more effective machine learning algorithms. 
