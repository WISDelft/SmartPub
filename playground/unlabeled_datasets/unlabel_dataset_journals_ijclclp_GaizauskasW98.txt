Information extraction (IE) is a term which has come to be applied to the activityof automatically extracting pre-specified sorts of information from short, natural language texts --typically, but by no means exclusively, newswire articles.
For instance, one might scan business newswire texts for announcements of management succession events (retirements, appointments, promotions, etc.
), extract the names of the participating companies and individuals, the post involved, the vacancy reason, and so on.
Put another way, IE may be seen as the activity of populating a structured information source (or database) from an unstructured, or free text, information source.
This structured database is then used for some other purpose: for searching or analysis using conventional database queries or data-mining techniques; for generating a summary; for constructing indices into the source texts.
Information extraction should not be confused with the more mature Technology of information retrieval (IR), which given a user query selects a (hopefully) relevant subset of documents from a larger set.
The user then browses the selected documents in order to fulfil his or her information need.
Depending on the IR system, the user may be further assisted by the selected documents being relevance ranked or having search terms highlighted in the text to facilitate identifying passages of particular interest.
The contrast between the aims of IE and IR systems can be summed up as: IR retrieves relevant documents from collections, IE extracts relevant information from documents.
The two techniques are therefore complementary, and their use incombination has the potential to create powerful new tools in text processing.
The differences and complementarity of the techniques can be illustrated by means of an example.
The management succession event scenario outlined above was part of the DARPA MUC-6 information system evaluation (see section 2.2.4below).
For this evaluation texts pertaining to management succession were required.
To obtain them, a corpus of Wall Street journal articles was searched using an IR system (eg (5)) with the query shown in Figure 1a).
The query was deliberately not fine-tuned, as it was desired to obtain some proportion of irrelevant texts.
A sample of a relevant text retrieved by this query is shown in Figure 1b).
Such texts were then run through IE systems one of whose principal tasks was to fill in a template whose structure is shown in Figure 1c) to produce results as (partially) shown in 1d); as secondary output the system used here is able to generate a natural language summary of the information in the template as shown in e).
Not only do IE and IR differ in their aims, they differ in the techniques they employ.
These differences arise partly from their difference in aim, but also for historical reasons.
Most work in IE has emerged from research into rule-based systems in computational linguistics and natural language processing, while IR work, where it has not been sui generis has been influenced by information theory, probability theory, and statistics.
Because of the requirement to extract information, IE must pay attention to the structural or syntagmatic properties of texts: `Carnegiehired Mellon' is not the same as`Mellonas`Mellon hired Carnegie' which differs again from`Mellonfrom`Mellon was hired by Carnegie'.
The simplest IR systems treat texts as no more than`bagsthan`bags' of unordered words.
More refined systems allow phrasal matching, proximity searching, and possibly thesaural expansion of query terms.
But these techniques are still not adequate to extract, for example, role players in events and their attributes, as the following example shows: To extract a canonicalised fact such as`Gas`G.
Torretta succeeds N. Andrews as chair-person of BNC Holdings Inc.' from each of these alternative formulations, some level of linguistic analysis is necessary --to cope with grammatical variation (active/passive), lexical variation (`named to' vs. `took the helm'), and cross-sentence phenomena such as anaphora.
The inadequacies of IR techniques for getting at the content of texts, and hence their limitations in satisfying text users information needs, have been long known; indeed almost every paper on IE starts with a cry that IR is inadequate (5;5;5).
But is progress in IE being made?
Are usable systems emerging, or is there a hope that they shortly will?
Our aim in writing this paper is to give positive answers to these questions.
In section 2 we review the history of IE, giving, if not an exhaustive review, at least a broad feeling for the work that hasgone on in the area.
In section 3 we try to give some flavour for the techniques and approaches that have been and are being used in IE systems, concentrating , excusably we trust, on the IE system we have developed and are currently using in a number of research projects.
Then, in section 4 we discuss application areas and applied systems, where IE systems are actually performing real world tasks.
We conclude, in section 5, by discussing some of the challenges facing IE in the future and the boundaries of IE.
Overall we hope to give a reasonable picture of the achievements, limitations, and potential of this exciting new text processing technology. 
IE as an area of research interest in its own right was first surveyed in (5).
Very broadly one can say that the field grew very rapidly from the late 1980's when DARPA, the US defence agency, funded competing research groupsto pursue IE.
However, significant work of relevance was carried out before the DARPA initiative, some of it finding its roots in the 1960s.
In this section we divide the work on IE into three broad categories: early work on template filling (work carried out or under way before the DARPA programme); work carried out in response to the DARPA MUC programme; and recent work on IE outside the DARPA programme.
This division, like any for review purposes, is crude and not too much weight should be placed upon it.
hasn't nam ed a successor to M r. Wright, w ho is expected to begin his new position by the end of the m onth.
Applied work on filling structured records with information from natural language texts appears to have originated in two long-term, research-oriented natural language processing projects.
The Linguistic String Project (5) at New YorkUniversity began in the mid-60's and carried on into the 1980's.
While concerned on the research side largely with the development of a large-scale computational grammar of English, the applications of the work were to do with deriving what Sager called information formats, regularised table-like forms which were, effectively, templates.
These information formats abstracted away from the profusionof natural language forms and permitted a database to be defined against which`factwhich`fact retrieval' (as opposed to document retrieval) could be carried out.
The applications were in the medical domain and concentrated on radiology reports andhospital discharge summaries.
Some limited evaluation was carried out by contrasting the program's behaviour with the results of getting a human clinician tofill in a comparable information format solely on the basis of the information in the discharge summary.
One interesting aspect of this work is that the information formats are not predefined a priori by experts in the field; rather, given a set of texts in a sub-language domain the information formats (the columns or fields in the tables) are induced by using distributional analysis to discover word classes in the domain (e.g.
`film shows clouding', `x-rays indicate metastasis', etc.
permit the definition of a TEST | SHOW | MEDICAL FINDING format).
While inducing templates was abandoned through the 1980's and early 90's as simply too difficult,and the use of predefined, tailored templates created by domain experts adopted instead, there is renewed interest in automatically acquired templates (5).
The second long term project of relevance to the formation of IE as an autonomous area of research was the work on language understanding, and in particular on story comprehension, carried out at Yale University by Roger Schank and his colleagues (5;5;5).
Central to this work was the notion that stories followed certain stereotypical patterns which Schank referred to as scripts.
Knowingthe script, language comprehenders are able to fill in details and make inferential leaps where the information required to make the leap is not present in the text.
Thus a corporate merger, or a management succession event, or a doctor-patient examination all have predictable role-players and sub-events and knowing these permits us to make sense of a text describing any instance of such an event.
The first attempt to build what might be called an IE system using this approach was made by one of Schank's students, Gerald De Jong, who designed and built a system called FRUMP (5).
It used what De Jong called ketchy scripts, a simplified version of the detailed scripts Schank had proposed, to process texts directly from a UPI news wire feed.
De Jong's system employed sketchy scripts for sixty situations to extract information from news stories in domains ranging from earthquakes to labour strikes.
The instantiated scripts were then used to generate summaries of the stories.
His approach relied upon an alternation of predictor and substantiator modules which used, respectively, top-down, expectation-driven processing relying on predictions from the script and bottom-up, data-driven processing based on input from the text.
This general approach has been adopted,in one way or another, by many IE systems since.
De Jong's work is also notable for carrying out a reasonably extensive evaluation: six days of previously unseen news stories were fed in real-time through FRUMP and the results classified as towhether the stories were processed correctly, nearly correctly, wrongly, or were missed.
Following these initial projects, the 1980's saw the first commercial IE systems developed.
The first system to be commercially deployed (to the best of our knowledge) was ATRANS, a system for automatic processing of money transfer messages between banks (5).
ATRANS adopted the Yale script-style approach to text processing, using script-driven predictions to identify actors (originating customer, originating bank, receiving bank, etc.)
in order to fill in a template that was used, after human verification, to initiate automatic money transfers.
Soon after, the Carnegie Group developed and deployed a `fact extraction'system for Reuters called JASPER (5).
JASPER was designed to skim company press releases on PR Newswire and fill in a template containing information aboutcompany earnings and dividends.
These templates were used to produce candidate news stories which were then validated or post-edited by journalists, offering them a significant savings in story preparation time.
A final commercial system initiated in this period was the SCISOR system developed by GE for analysis of corporate mergers and acquisitions (5).
Two other academic research projects from this period should be mentioned.
The first was a system developed by James Cowie to extract regularised descriptions (effectively, templates) of plants fromwild flower guides (5).
Cowie's approach relied upon a domain-specific, handcrafted lexicon of keywords which allowed segments of the source text to be matched with appropriate sectionsof the target template.
Rules pertaining to slots in the template (properties of plants) were then brought to bear on the selected portions of text and the propertyvalues extracted.
The second was a project by G.P.
Zarri to translate automatically French texts dealing with a particular period of French history into a `metalanguage' which captured certain semantic relations pertaining to biographical details that were sought (5).
This metalanguage was organised around case frames for predicates, which can be viewed as small-scale templates: what was to be extracted were the roles in particular historical events, such as the naming to a position of an historical figure by a given body on a particular date at some location.
The approach involved first using a syntactic analyser to establish the text's syntactic structure, and then carrying out semantic parsing in which lexical triggers --keywords in the domain --caused one or more of the case frames for key predicates to be invoked and then instantiated with material identified from thesyntactic analysis, according to rules associated with the slots case frame slots.
In the mid-1980's a number of sites in the US were working on IE from naval messages, in projects sponsored by the US Navy.
In order to understand andcompare their systems' behaviour better, a number of these message understanding (MU) projects decided to work on a set of common messages and then convene tosee how their systems would perform when given some new, unseen messages.
This gathering constituted the first of what has turned into an ongoing series of extremely productive message understanding conferences, or MUCs, which haveserved as key events in driving the field of IE forward (the term`messageterm`message under-standing' is now disappearing in favour of the more descriptively accuratèinformation extraction')(5;5;5;5).
There have been six Message Understanding Conferences to date and a seventh is planned for spring 1998.
The objective of the conferences has been to establish a quantitative evaluation regime for IE or MU systems, which prior to these conferences had been sporadically assessed in an ad hoc fashion, frequentlyon the same data on which they had been trained.
To date, the MUC conferenceshave been sponsored by DARPA and organised by the US Naval Command,Control, and Ocean Surveillance Center RDT\&E Division (NRaD), formerly theNaval Ocean Systems Center, in San Diego, California.
A brief chronology and description of the MUCs is as follows: MUC-1 Held in May 1987 in San Diego.
Six systems participated.
The texts were tactical naval operations reports on ship sightings and engagements.
Twelve training reports were supplied, plus additional messages.
Two unseen messages were distributed at the conference for participants to test their systems on.
There was no task definition and there were no evaluation criteria.
Across these evaluation exercises, the tasks have become progressively more difficult.
Some effort was made to quantify this increase at MUC-5 and the conclusion drawn that there was an order-of-magnitude increase in task complexity on several measures between MUC-2 and MUC-5 (5).
Task complexity measures included text corpus complexity (e.g.
vocabulary size, average sentence length), textcorpus dimensions (e.g.
volume of texts, total number of sentences/words), templatecharacteristics (e.g.
number of object types, number of slots), and difficulty of task (hard to measure, but considered, e.g., number of pages of relevance rules and template fill definitions).
System performance has improved against this backdrop of increasing task complexity, indicating that genuine progress in developing this technology has been made in the past decade.
In sections 2.2.3 and 2.2.4 we describe MUC-5 and MUC-6 in some detail, as the most recent and most sophisticated IE evaluations.
MUC-2 Held in May 1989 in The evaluation metrics have evolved with each MUC.
The starting points for the development of these metrics were the standard IR metrics of recall and precision.
In the information extraction task, recall may be crudely interpreted as a measure of the fraction of the required information that has been correctly extracted and precision as a measure of the fraction of the extracted information that is correct.
The definitions of these measures have been altered from those used in IR (but the names have been retained) to allow for overgeneration in IE where, unlike IR,data not present in the input can be erroneously produced.
Not only have recall and precision measures been redefined for the extraction task, but additional measures have been introduced as well.
Slot fills can be correct, partially correct, or incorrect, but they can also be missing (no fill when there should be), sprious TIPSTER is not an acronym and appears to have been adopted as a name because of the intelligence providing potential of these technologies (cf.
the Oxford Concise Dictionary: tipster n. a person who gives tips, esp.
about betting at horse-races.)
Information Extraction:Beyond Document Retrieval(fill present when it should not be), or non-committal (no fill when the answer key also contains no fill).
These extra categories permit the introduction of measures of overgeneration (fraction of extracted information that is spurious), undergeneration (fraction of information to have been extracted that is missing), and substitution (fraction of the nonspurious extracted information that is not correct).
For MUC-3 and MUC-4 recall and precision were the primary metrics and the others were secondary.
In addition, for MUC-4, van Rijsbergen's combined measure of recall and precision, the F-measure, was used (5).
But for MUC-5, recall and precision were deemed unofficial metrics and a new primary metric called error per response fill was introduced.
This was an attempt to measure the fraction of a system's response that is`wrongis`wrong', i.e.
the fraction of the combined actual and possible responses that were faulty.
It was hoped that this measure would allow developers to focus more directly on the sources of their systems' difficulties, in particular on missing and spurious information which figures directly in the error-based metric, but only indirectly in the recall and precision metrics.
In MUC-6 recall and precision regained their status as official metrics and the metrics were slightly modified so as to eliminate the category of partially correct slot fill.
All of these metrics carried over to three of the four MUC-6 tasks, but only precision and recall metrics were employed for the coreference task and their definitions had to be modified to account for peculiarities of this task (see (5) for more details).
Since at least MUC-3, a text-filtering metric has also been employed to measure how good systems are at separating documents into relevant/nonrelevantcategories.
This measure operates at the level of texts as a whole (are templates generated for a given text when they should be or not) and not at the level of slots.
Task As with MUC-3 and MUC-4, the MUC-5/TIPSTER-I 24-month evaluationrequired systems to extract information from newswire stories.
There were four possible tasks: two domains (joint ventures and microelectronics) and two languages(Japanese and English).
These domain-language pairs are referred to using the acronyms EJV, JJV, EME and JME, in the obvious way.
Participating non-TIPSTER-sponsored systems had to choose one domain and either or both languages; TIPSTER-sponsored systems were intended to operate in all four domain/language pairs.
Most sites did only one task as this proved more than challengingenough .
The EJV task was the most popular, and by common consent the most difficult; most of the following detailed remarks pertain to this task.
The MUC-5 template and fill rules were the most complex to date.
For the first time the template was not a flat data structure, but rather allowed slots to contain pointers to other slots.
Thus the template had anòbject-oriented' feel.
For example, a joint venture was viewed as an object with various slots including its name and status (`existing', `dissolved', etc), but also slots for the participating organisations, each of which was to be filled with a pointer to an organisation object, itself containing slots which in some cases contained pointers to other complex objects.
In all there were 11 objects and 49 slots to be filled in.
Slotswere of four types: set fills (contained one of a given set of alternatives --e.g.
organisation type could be company, person, government or other); string fills (contained a copy of some string from the original text --e.g.
company name); normalised entries (contained data from the text transformed into a canonical form --e.g.
dates, times, monetary amounts); references (pointers to other objects, as described above).
As an indication of the level of detail required to define the extraction task, the fill rules occupied a 45 page document.
Resources There were three sources for the EJV materials: the Wall Street Journal, Lexus/Nexus, and PROMT.
Roughly 2300 training texts were provided andanswer keys were supplied for most of them.
There was a dry run blind test set of 200 articles provided roughly half way through the evaluation, and a final blindtest set of 286 articles.
Official scoring was done for both dry run and final tests by MUC organisers but the scoring program was made available to all sites for use during development.
This program was an extremely sophisticated piece of software which could be run in an entirely automatic mode, or in an interactive modewhere the scorer is queried about the status of what the program judges may be partially correct answers.
The texts ranged in length from just two or three sentences, to several pages.Sentence lengths varied enormously, but some of length greater than seventy wordswere reported.
In some places the texts contained tabular numeric data.
The texts varied between mixed case and all upper case.
All were originally marked up in SGML and contained certain reliably extractable information such as document id, date and source, flagged by SGML markers.
In addition to the training corpora and answer keys, considerable other data resources were supplied.
The methodology and effort required to produce the answer keys were bothnontrivial .
The production of the templates was undertaken by a small team of analysts, equipped with workstations and a software tool to aid in the extraction task.
An elaborate procedure of selecting subsets of the documents to be multiply analysed was adopted in an attempt to ensure consistency in the answer keys.
Of course the fill rules had to be modified as new complexity was uncovered and thisrequired correcting previously created answer keys.
The cost of producing the answer keys alone for MUC-5 and for the preceding TIPSTER extraction trials wasmore than $1 million US.
Table 1shows the best raw score obtained in each of the four tasks discussed above.
One interesting thing to note from these results is that in each domain the Japanese scores were higher.
This observation has prompted discussion of whether in some sense Japanese is an easier language from which to extract information.
For error per response fill, undergeneration, overgeneration, and substitution the lower the score the better; for recall and precision the higher the score the better.
Raw scores need to be interpreted very cautiously.
Statistical studies were done on them (5) and for each task a number of ranks were identified within which raw score differences were claimed to be of no significance.
For EJV there were 7 statistically significant ranks into which 13 systems were placed; in JJV 3 ranks for 5 systems; in EME 5 ranks for 7 systems; and in JME 2 ranks for 4 systems.
Results In MUC-6, rather than a singlèend-to-end' system evaluation as in MUC-5, participants were offered a menu of smaller evaluations from which they could pick and choose, depending on their interests and available resources.
There were four evaluated tasks.
Coreference relations were only marked between certain syntactic classes of expressions (noun phrases and pronouns) and a relatively constrained class of relationships to mark was specified, with clarifications provided with respect to bound anaphors, apposition, predicate nominals, types and tokens, functions and function values, and metonymy.
3.
Template element filling.
This task required the filling of small scale templates wherever they occurred in the texts.
There were only two such template elements, one for organisations and one for persons.
These are illustrated in Figure 1.
4.
Scenario template filling.
The task required the detection of specific relations holding between template elements relevant to a particular information need (in this case corporate management personnel joining and leaving companies) andconstruction of an object-oriented structure recording the entities and details of the relation.
This is illustrated in Figure 1.
The precise specifications of each of these tasks may be found in Appendices C-Fof (5).
Four other evaluations had been considered, but were dropped due to lack of agreement over task definitions and lack of time and money for producing the development and test resources.
These were parse structure evaluation (provide a canonical syntactic analysis of each sentence); predicate-argument structure evaluation (provide a canonical semantic analysis of each sentence); word sense disambiguation (disambiguate the sense of each open class, non-proper name word with respect to some standard lexical resource such as WordNet (5)); and cross-document coreference (determine coreferences between distinct documents).
The demand for this restructuring of the evaluation exercise arose for a number of reasons.
Different participants had different interests and believed effort should be focussed in different areas.
End-to-end systems IE were getting bigger and bigger and many research groups were excluded simply because they could notput the resources together to produce a massive system, where software engineeringissues can soon come to eclipse research issues.
Furthermore, comparison of systems andapproaches had proved extremely difficult because the grain of the evaluation was too large.
Finer scale evaluation, it was believed, would focus and promote more fruitful debate.
However, it can be argued that any subdivision of the end-to-end IE task presupposes a processing approach to the task which may inhibit radically new approaches from emerging.
Resources As with MUC-5, the principal resources supplied by the organisers were annotated development and test corpora and scoring software.
For both the dry run and final evaluations, 100 annotated development texts were provided for each of the four tasks.
For the evaluations themselves there were 30 annotated test texts for the named entity and coreference tasks, and 100 annotated test texts for the scenario template and template element tasks.
These texts were all WallStreet Journal texts, all of them mixed case.
New scoring software was developed for the named entity and coreference tasks, and the MUC-5 scoring software enhanced for the template tasks.
Evaluation In MUC-6 the official evaluation metric reverted to precision and recall from the error-per-response-fill metric used in MUC-5.
These two metrics had shown themselves to be very closely in line in MUC-5 and participants generally preferred precision and recall (perhaps because one tries to maximise these measures, whereas one tries to minimise error-per-response-fill, which caststhe whole exercise in a more negative light).
The two template filling tasks were scored as in previous MUCs, with improvements to the scoring software, but no major departures.
The named entitytask required a new scorer based on comparing SGML-marked up strings, but the standard definitions of recall and precision carry over quite naturally here.
However, in the coreference task, a problem arises which requires that the precision and recall scoring measures be specially adapted.
Clearly, more than twomarkables may corefer, i.e., there may be chains of coreferences, not simply coreferential pairs.
In the case of chains, how to record the chain and how to score systems which fail to discover all the links in the chain become central issues.
See (5) for a full discussion of the definitions of precision and recall for the coreference task.
Results Table 2shows the best raw score obtained in each of the four tasks.
In all but the coreference case the results of the system with the best combined precision and recall score (F-measure) have been displayed (thus, there may be other systems which obtained higher scores on one of the other measures).
Due to differences in the approach to scoring the coreference task and the other tasks, only recall and precision measures were available for coreference, and no satisfactory combined measure could be defined.
Even after doing statistical significance studies it is hard to come to any firm conclusion about the superiority of a given approach, principally because of the varying levels of resources that different sites brought to the task --person-months spent on development, qualifications and backgrounds of the people doing the development, software and hardware resources committed, and so on.
At the conference every site could put up a graph showing a steep line of improvement from the immediately preceding dry run evaluation and claim (especially to their funding bodies !)
that given another few months they could make spectacular gains.
Clearly this improvement has to stop somewhere; but there is no way of telling which approach will level out when and at what level.
Another criticism frequently made of the MUC evaluations is that they lead to copy-cat behaviour, whereby systems tend to converge upon the same approach because any advantage is quickly picked up by others afraid to lag behind in the short term because of funding implications of being seen to be a `loser'.
Each of these criticisms can be at least partially answered.
The first one --that the evaluation results do not let us draw unequivocal conclusions --by observing that imperfect evaluation is better than none at all.
The results can tell us important things; we simply need to be careful in interpreting the results.
The second criticism --that participating sites tend to play safe by copying successfulapproaches --may be true of some sites (perhaps those directly dependent on linked funding), but is certainly not true of all sites, particularly academic ones (section 3.3.1 gives some indication of the wide range of approaches still being entertained).
Besides the rapid transfer of successful technology can hardly be viewed as completely deleterious.
In all the MUC evaluations have provided the IE community resources,evaluation tools, and perhaps above all a sense of identity and a forum for exchange of ideas.
There may come a time when their utility becomes questionable; but they have proved of significant worth to date.
The MUC evaluations are still running, but concurrent with them, either unrelatedlyor in part because of the higher interest in IE they have generated, numerous otherIE projects can be identified.
This list describes some significant European IE projects, but it is almost certainly incomplete given the rapidly expanding nature ofthe field.
Two projects which started in the late 1980's illustrate the use IE systems forprocessing sublanguages --specialised languages that are developed within a restricted area of human activity and which are frequently characterised by extragrammaticality (from the perspective of thèmother' language), idiosyncratic lexical forms, and heavy use of ellipsis (because of the shared world knowledge which the context which gives rise to the sublanguage supplies).
The first of theseis the POETIC (Portable Extendable Traffic Information Collator) system (5) whosefunction was to extract information about road traffic incidents causing traffic congestion from police incident logs and to generate advisory bulletins to be broadcast to motorists.
Police incident logs form a sublanguage in the sense defined above, and the system utilised a special grammar and lexicon, as well as a domain-specific reasoning component to deal with the highly telegraphic and idiosyncratic forms found in the police logs.
The second system was SINTESI (Sistems INtegrato per TESti in Italiano) which processed short texts describing car faults and filled in a template identifying the main fault, chain of causes, chain of effects, car parts involved etc.(5).
Once again, because of the nature of the sublanguage, the approach relied extensively on domain-specific lexical-semantic knowledge (caseframes for relevant objects in the domain).
The Language Engineering (LE) initiatives within the Commission of the European Communities (CEC) Third and Fourth Framework programmes have supported a number of IE projects, several of which are currently underway.
Theseare simply listed with references for the interested reader, as there is not space to describe them, and in some cases, as the projects are just underway, there is yet little published material about them.
The TREE (TRans European Employment) project aims to make information available to job seekers across the European Union by extracting job details from electronic job advertisements and storing them in a database which can be browsed by job seekers in their own language (5;5).
The FACILE (Fast Accurate Categorisation of Information using Language Engineering) project, following on from the COBALT project aims to categorise and filter news stories of interest to stock market traders, using extraction-like techniques (5;5;5).
Finally, at Sheffield we are working on two applications of IE systems within the CEC LE projects: one, AVENTINUS is in the classic IE tradition, seeking information on individuals about security, drugs and crime, andusing classic templates (5;5).
The other, ECRAN, a more research-orientated project, searches movie and financial databases and exploits the notion we mentioned of tuning a lexicon so as to have the right contents, senses and so on to deal with new domains and relations unseen before (5). 
Since IE systems are large, complex software systems usually consisting of many components, classifying them is not an easy task.
Perhaps the most useful aid in this task is a description of the generic IE system provided by J. Hobbs (5).
His description allows newcomers to the field to grasp the principal processing stagesinvolved in IE and provides IE system developers with a standard system description against which to differentiate their own.
While this description was derived as a synthesis of the approaches used in MUC-4 systems, it remains broadly true.
Armed with this general description we then turn to a description of the LaSIE Information Extraction:Beyond Document Retrieval(Large Scale Information Extraction) system which we have developed at Sheffield, using the system we know best to illustrate in more detail the sorts of processing involved in information extraction.
While LaSIE is quite distinct frommany IE systems, it is not difficult to see how it fits Hobbs's general rubric.
Following this moderately detailed description of how one IE system works, we conclude this section with a discussion of some of the general trends that are currently influencing the direction of IE system development.
Hobbs describes the generic IE system as a ``cascade of transducers or modules that at each step add structure and often lose information, hopefully irrelevant, by applying rules that are acquired manually and/or automatically'' ((5), p. 87).
To describe such a system requires identifying the modules, identifying each module's input and output, identifying the form of the rules the modules apply, and specifying how the rules are applied and how they are acquired.
According to Hobbs, a typical IE system consists of a sequence of ten modules: 1.
Text Zoner.
Divides the input text into a set of segments.
2.
Preprocessor.
Converts a text segment into a sequence of sentences, where each sentence is a sequence of lexical items, with associated lexical attributes (e.g.
p art-of-speech).
Of course not all systems exhibit all of these modules, nor do they necessarily perform their processing in exactly this sequence (in particular stages 6 and 7 may occur in the reverse order).
LaSIE System Architecture LaSIE was designed as a general purpose IE research system, initially geared towards, but not solely restricted to, carrying out the tasks specified in MUC-6: named entity recognition, coreference resolution, template element filling, and scenario template filling.
In addition, the system can generate a brief natural language summary of any scenario it has detected in the text.
All of these tasks are carried out by building a single rich model of the text --the discourse model --from which the various results are read off.
The high level structure of LaSIE is illustrated in Figure2.
The system is a pipelined Information Extraction:Beyond Document Retrievalarchitecture which processes a text one sentence at a time and consists of three principal processing stages: lexical preprocessing, parsing plus semantic interpretation, and discourse interpretation.
The overall contributions of these stagesmay be briefly described as follows: lexical preprocessing reads and tokenises the raw input text, tags the tokens with parts-of-speech, performs morphological analysis, performs phrasal matching against lists of proper names; parsing and semantic interpretation builds lexical and phrasal chart edges  in a feature-based formalism then does two pass chart parsing, pass one with a special named entity grammar, pass two with a general grammar, and, after selecting a `best parse', constructs a predicate-argument representation of the current sentence; discourse interpretation adds the information from the predicate-argument representation to a hierarchically structured semantic net which encodes the system's world model, adds additional information presupposed by the input, performs coreference resolution between new and existing instances in the world model, and adds any information consequent upon the new input.
Subsequent to MUC-6, LaSIE was re-engineering at the architectural level to make it function within a language engineering research architecture called GATE --the General Architecture for Text Engineering also developed at Sheffield.
GATE is a software environment that supports researchers who are working in natural languageprocessing and computational linguistics and developers who are producing and delivering language engineering systems (5;5).
It is based on the TIPSTER architecture (5), an object-oriented data model designed to support a broad range ofdocument processing tasks and promoted as a standard for the information retrievaland extraction tasks within the DARPA-sponsored TIPSTER text programme.
The re-engineered LaSIE system functioning within GATE is called VIE (Vanilla IE system).
It was derived from LaSIE by standardising LaSIE module interfaces so that all modules communicated with each other via the GATE document manager (allowing for easy substitution of improved modules with similar functionality --e.g., better part-of-speech taggers, or parsers).
Further details of LaSIE and VIE can be found in (5;5).
2 The processing of the system is best illustrated by means of an example.
We will discuss what processing goes on each of the three principal stages identified above with respect to the small text shown in Figure 1b).
This stage comprises five modules.
1.
Tokenisation.
This module does both text segmentation and tokenisation.
In the example text it distinguishes the document header (everything preceding the <TXT> tag) from the document body, and in longer texts would segment the text into paragraphs.
Tokenisation involves identifying which sequences of characters will be treated as individual tokens --for example, treating SGML tags as single tokens, but separating other punctuation from preceding characters (so <TXT> is a token but In addition we use four lists of trigger words, to tag words which occur inside multi-word proper names, and which reliably permit the class of the proper name to be determined.
For example, `Wing and Prayer Airlines' is almost certainly a company, given the presence of the word Airlines; `Bay of Pigs' almost certainly a location given the word Bay.
This and further aspects of the system's algorithm for proper name recognition are discussed further in (5).
The parsing and semantic interpretation stage of LaSIE is carried out by a single module.
However this stage consists of three substages.
The first substage is parsing with a special named entity grammar.
We use a bottom-up chart parser (5) and a manually constructed context-free grammar of 177 rules pertaining to named entities to recognise multi-word structures which identify organisations, persons, locations, dates, and monetary amounts.
The second substage is parsing with a more general phrasal grammar.
The Same parser mechanism is used, but this time with a grammar of 110 rules Designed to recognise noun phrases, verb phrases, prepositional phrases, adjectival phrases, sentences , and relative clauses.
This grammar was extracted from a large manually annotated corpus of newswire text, the Penn Treebank (5), using a set of programs designed for the purpose (5).
Again, a semantic interpretation is built up during parsing.
For instance the sentence ÏÑ Figure 3 A LaSIE Parse Forest Despite the fact that the parser is complete, i.e.
finds all structural analyses ofits input sentence according to the grammar, it is rare that these analyses contain aunique, spanning parse of the sentence.
Consequently, the final substage of the Parsing module involves selecting a ``best parse'' from the set of partial,fragmentary, and possibly overlapping (and hence incompatible) phrasal analyses which the parser has found.
This is currently done by choosing that sequence of non-overlapping phrases of semantically interpretable categories (sentence, noun phrase, verb phrase and prepositional phrase) which covers the most words and consists of the fewest (hence largest) phrases.
The principal task of the discourse processing module in LaSIE is to integrate the semantic representations of multiple sentences into a single model of the text from which the information required for filling a template may be derived.
The discourse processor works on the semantic representations passed onto it from the parser, though these include a record of the surface text from which they were derived, and in particular permit the order in which entities were introduced to be recovered.
The discourse interpretation stage of LaSIE relies on an underlying`worldunderlying`world model', a declarative knowledge base that both contains general conceptual knowledge and serves as a frame upon which a discourse model for a multi-sentence text is built.
This world model is expressed in the XI knowledge representation language (5) which allows straightforward definition of cross-classification hierarchies, the association of arbitrary attributes with classes or individuals, and the inheritance of these attributes by individuals.
The world model consists of an ontology plus an associated attribute knowledge  Figure 4 A Fragment of the LaSIE World Model and Associated Attribute Knowledge Base The world model described above can be regarded as an empty shell or frame to which the semantic representation of a particular text is added, populatingit with the instances mentioned in the text.
The world model which results is then a model specialised for the world as described by the current text; we refer to this specialised model as the discourse model.
Figure 5 A Fragment of the LaSIE Discourse Model  Discourse processing proceeds in four substages for each new sentence representation passed on from the parser.
First, the semantic representation producedby the parser is processed by adding its instances, together with their attributes, to the discourse Information Extraction:Beyond Document Retrievalmodel which has been constructed so far for the text.
Instances which have their semantic class specified in the input (via unary predicates) are added directly to the discourse model, beneath their class in the ontological hierarchy (e.g.
firm(e24)).
Attributes -binary predicates in which the first argument is always an instance identifier --are added to the attribute-value structure associated with instance identifiers occurring within them, provided the class of the instance is known.
In the second stage, presuppositions are expanded, leading to further information being added to or removed from the model.
In the current example, this has two effects.
First, it permits missing semantic class information for instances to be derived from type restrictions on attribute arguments.
For instance,an attr_of attribute associated with the node in the ontology corresponding to the title attribute, records that this attribute holds only of entities of type post_holder.
Thus, given the input fact title(e23,executive VP) but no input fact specifying the class of e23, it becomes possible to attach the instance e23 beneath the correct class in the ontology.
Second, the semantic types of verbal roles are used to hypothesise entities which fulfil those roles, if they are not present, or have not been discovered, in the input.
In this case the fact that`Donaldthat`Donald Wright' is the logical object of thèwas named' event has not been determined by the parser, as the intervening phrasè46 years old' was not properly parsed, hence preventing theparser from identifying`Donaldfying`Donald Wright' as the surface subject/logical object of the passive verb phrase.
Thus, a person e26 is added to the model to play this role.
In a similar fashion e25, an organisation, is added to the model to play the role of the logical subject of the naming event.
The third stage involves comparing all new instances (those introduced by thissentence) with previously existing instances to determine whether any pair can be merged into a single instance, representing a coreference in the text.
The algorithm takes into account considerations such as the instances' textual proximity and the consistency of their semantic classes and attributes.
For the current example the coreference algorithm leads to the merging of e26 and e21 --that is, `Donald Wright' is recognised as the logical object of the naming event --and e25 is merged with e24 --that is, `this brokerage firm' is identified as the logical subjectof the naming event.
Subsequently these merged entities are merged with e20 --that the brokerage firm doing the naming is identified as`Burns as`Burns Fry'.
The reader is referred to (5) for further details, and an evaluation, of the coreference algorithm.
The final stage of discourse processing is consequence expansion.
This stage is intended to allow any inferences to be drawn which can now be made given the addition to the discourse model of the information in the current sentence.
Its primary use in LaSIE is to allow inference rules associated with template objects and slots to infer values for these objects and slots from information now present in the discourse model.
After all sentences in a text have been processed, the template will have beenfilled to the best of the system's abilities.
The template is then written out in whatever form is required.
IE is not an isolated activity and is being influenced by and is in turn influencing other activities in natural language processing and computational linguistics.
In this section we look briefly at three trends that can be seen in the recent development of IE: the movement towards shallower processing (or towards what might be called anàppropriateanàppropriate' level of processing for the task), the movement away from handcrafted rule sets towards automatically acquired rule sets, and the movement towards coupling together relatively independent modules.
Of course these trends are not entirely independent.
They are all part of a general move towards a more empirically oriented approach to NLP that has emerged for a host of reasons, including the availability of large scale electronic corpora, frustration with theoretical developments that seemed to be losing touch with the reality of the data, and the drive towards applications.
Given the pragmatic constraints imposed by the IE task --the relatively limited understanding required --many developers of IE systems have, in recent years, opted for engineering solutions that de-emphasize the substantial body of theoreticalwork both in computational syntax and semantics and in knowledge representation and reasoning.
This de-emphasis is perhaps most dramatically illustrated by SRI who abandoned, quite consciously, the theoretically motivated TACITUS system after MUC-3 (1991) in favour of the pragmatically motivated FASTUS system which they have used for MUC-4 (1992) through MUC-6 (1995).
TACITUS (5) attempted a full syntactic analysis, using a large scale grammar of English, performed semantic interpretation to produce first-order predicate calculus representations, and then used abductive reasoning to interpret the semantic representations of individual sentences in the context of a schema pertaining to the scenario of interest.
FASTUS (5), by contrast, uses a cascade of finite-state transducers that successively tokenise, recognise names, recognise phrases, recognise template patterns, and then combine or merge partially filled templates to generate the final template.
SRI have been keen to stress that this change in direction has not happened because they concluded that the TACITUS approach was faulty, but because they believed it was inappropriate for the task.
TACITUS did text understanding, FASTUS information extraction, the latter, on their view, a much simpler task that does not require the theoretical and computational sophistication of TACITUS.
The chief gain from the switch has been speed (from 36 hours to 12 minutes for 100 texts between MUC-3 and MUC-4) and to some extent ease ofporting to new domains.
Though performance results, in terms of combined precision and recall, are not strictly comparable between MUCs, it is worth noting that FASTUS scores surpassed TACITUS scores by about 16% between MUC-3 and Muc-4, mostly due to increased recall.
SRI have not been alone in moving away from a more powerful, linguistically motivated approach towards a more restricted, task-specific, engineering-driven approach.
Recent IE systems developed by General Electric, Mitre Corporation, New York University and SRA have all come to be considered exemplars of a `shallow' processing approach to IE which promises, if not better recall and precision, at least faster, more portable systems.
This movement away from the more theoretically motivated work of the 1980's has engendered considerable debate (and rhetoric) about`shallowabout`shallow' versus`deepversus`deep' approaches to information extraction.
This debate is ongoing and the underlying distinction, while reflecting important insights, needs to be analysed, as it can lead to distortion and over-simplification.
In particular, it is important to distinguish at least two ways in which processing in an IE system can be shallower or deeper.
The processing in an IE system can be divided coarsely into two parts: the syntactic portion that works on single sentences of the input and the discourse-level portion that integrates information from the syntactic analyses of multiple sentences.
The former typically includes tokenisation, part-of-speech tagging, phrasal pattern matching or parsing and produces a regularised form which may be anything from a partially filled template to a full logical form.The latter takes whatever regularised form has been produced by the former and, perhaps using more general knowledge of domain, attempts to integrate information from the individual sentence representations into a larger scale structure which ultimately either is, or serves to provide, the information for the final template.
Thus, processing in an IE system can be shallower or deeper depending on the shallowness or depth of processing in each of these two processing stages.
First, the syntactic analysis the system performs can be more or less thorough.
At one extreme there are systems which employ formally weak mechanisms (finite-state pattern matchers) to apply domain-specific lexically-triggered patterns; at the other extreme there are systems which employ formally stronger mechanisms (complete parsers for context-free or even more expressive formalisms) to apply general grammars of natural language.
Examples of the former include the SRI FASTUS system, Mitre's Alembic} system (5), and the SRA (5) and NYU (5) MUC-6 systems; examples of the latter include the TACITUS system mentioned above, the Proteus system (5), and the PIE system (5).
Systems like LaSIE and the BBN PLUM system (5) which use a domain independent grammar, but only attempt fragmentary parsing, fall somewhere in the middle.
Second, the discourse or multi-sentence level processing can be more or less general.
Thus, the semantic representation derived from the syntactic analysis can be expressed in a more or less general formalism and manipulated by more or lessgeneral algorithms which attempt to integrate it into a more or less general model of the text and domain.
There may or may not be any attempt to use declaratively represented world and domain knowledge to help in resolving ambiguities of attachment, word sense, quantifier scope, and coreference, or to support inference-driven template filling.
At one extreme there are information extraction systems which produce semantic representations that are fragments of the target template for just those sentences that yield template relevant information and then merge these using ad hoc heuristics to produce the final template (e.g.
FASTUS and the SRA MUC-6 system); at the other extreme there are systems that use abductive theorem provers and axiomatisations of the domain to compute the least cost explanation of the first order logic expressions derived from every sentence in the input and then generate the template from the resulting underlying logical model (e.g.
TACITUS).
In between lie systems that translate their input into some sort of template-independent predicate-argument notation and use some amount of declaratively represented information about the domain to assist in doingcoreference and inference driven template filling.
LaSIE falls into this camp as do the NYU MUC-6 system and the MITRE Alembic system.
Early successful systems like JASPER (see section 2.1 above), depended on very Complex hand-crafted templates, made up by analysts.
However, the IE movement has grown by exploiting, and joining, the recent trend towards a more empirical and text-based computational linguistics, that is to say by putting less emphasis on linguistic theory and trying to derive structures and various levels of linguistic generalisation from the large volumes of text data that machines can now manipulate.
A conspicuous success has been part-of-speech taggers, systems that assign one and only one part-of-speech symbol to a word in a running text and do so onthe basis (usually) of statistical generalisations across very large bodies of text.
Recent research has shown that a number of quite independent modules of analysisof this kind can be built up independently from data, usually very large electronic texts, rather than coming from either intuition or some dependence on other parts of a linguistic theory.
These independent modules, each with reasonably high levels of performance in blind tests, include part-of-speech tagging, aligning texts sentence-by-sentence in different languages, syntax analysis, and attaching word sense tags to words in texts to disambiguate them in context.
The empirical movement, basing, as it does, linguistic claims on text data, hasanother stream: the use in language processing of large language dictionaries (of single languages and bilingual forms) that became available about ten years ago in electronic forms from publishers' tapes.
These are not textual data in quite the sense above, since they are large sets of intuitions about meaning set out by teamsof lexicographers or dictionary makers.
Sometimes they are actually wrong, but they have nevertheless proved a useful resource for language processing by computer, and lexicons derived from them have played a role in actual working MT and IE systems (5).
What such lexicons lack is a dynamic view of a language; they are inevitably fossilised intuitions.
To use a well known example: dictionaries of English normally tell you that the first, or main, sense of``of``television'' is as a technology or a TV set, although it is mainly used now to mean the medium itself.
Modern texts are thus out of step with dictionaries --even modern ones.
It is this kind of evidence that shows that, for tasks like IE, lexicons must be adapted or``or``tuned'' tothe texts being analysed which has led to a new, more creative wave, in IE research: the need not just to use large textual and lexical resources, but to adapt them as automatically as possible, to enable them to adapt to new domains and corpora, which will mean dealing with obsolescence and with the specialised vocabulary of a domain not encountered before.
As noted above there has been a movement away from theory prescribed modules whose processing is controlled by sets of handcrafted rules towards data-dependent modules whose processing is controlled by rules or parameters acquiring from automatically analysing large text corpora.
These modules include part-of-speech tagging, text-alignment in different languages, syntax analysis, word sense disambiguation and so on.
Aside from the fact that their rules or parameters are acquired automatically, the other striking thing about these modules is their independence: that these tasks can be done relatively independently is very surprising to those who believed them all contextually dependent sub-tasks within alarger theory.
These modules have been combined in various ways to perform taskslike IE as well as more traditional ones like machine translation (MT).
The modules can each be evaluated separately --against their specifications .
Recently there has been a move to support this kind of modularisation explicitly through the development of text processing architectures like the TIPSTER architecture (5) and implementations of it like the General Architecture for Text Engineering (GATE) (5;5).
These architectures support rapid addition and interchange of modules and represent a commitment to a modular approach to language engineering.
While language engineering modules can be developed and evaluated independently it is important to keep in mind that they do not in the end do tasksthat real people actually do, unlike MT and IE systems.
One can call the formerìntermediate' tasks and the latter real or final tasks --and it is really only the latter that can be firmly evaluated against human needs --by people who know what a translation, say, is and what it is for.
The intermediate tasks are evaluated internally to improve performance but are only, in the end, stages on the way to some larger goal.
Moreover, it is not possible to have quite the same level of confidence in them since what is, or is not, a correct syntactic structure for a sentence is clearly more dependent on one's commitments to a linguistic theory of some sort, and such matters are in constant dispute.
What constitutes proper extraction of people's names from texts, or a translation of it, can be assessed by many people with no such subjective commitments. 
In section 2 we reviewed work in IE from an historical perspective, describing efforts in the area in a chronological fashion.
It is also of interest, however, to view IE from the perspective of the application areas in which IE systems have been or are being deployed.
This perspective should help to dispel the view, whichthe MUC evaluations may have unintentionally engendered, that IE is only of interest for military intelligence or financial applications, and to stimulate thinking about the range of potential applications for this growth technology.
The following list is bound to be partial; but it is indicative of the range of areas in which IE technology is already in play.
Finance The MUC-5 joint ventures scenario lead at least thirteen sites to develop IE systems for extracting details of joint ventures from newswire stories (5).
The MUC-6 management succession event scenario is also of potential interest to those working in finance (5).
The COBALT and FACILE projects (5;5) which use IE techniques to help categorise newswire stories of relevance to stock traders also operate in this area.
A number of companies have expressed interest to the authors in competitor intelligence systems that will enable them to track ventures in which their competitors are engaged, as reported in newswires.
Military intelligence Medicine Sager's early work (5) illustrated the possibility of gathering information from patient discharge summaries and radiology reports.
Work by Lehnert also applied IE in a medical domain (5).
We have discussed applications of IE with local medical informatics experts and they confirm the need for applications to help in the classification of patient records and discharge summaries to assist in public health research and in medical treatment auditing.
Law The NAVILEX project aims to use IE techniques to support intelligent retrieval from legal texts (5).
It follows on from the NOMOS project which also applied`shallow applied`shallow' NLP techniques to extract information from legal texts to assist in retrieval (5).
Police The POETIC project developed an IE system for extracting information about road traffic incidents from policècommand and control' incident logs (5).
The AVENTINUS project is working to build tools to assist police in criminal investigations relating to drug trafficking (5;5).
Technology/product tracking One of the two MUC-5 extraction scenarios was microelectronics products announcements --extracting details about new microelectronic technology from the trade press (5).
Again, industrialists have expressed an interest to us in tracking commodity price changes and factors affecting these changes in the relevant newsfeeds.
Academic research Academic journals and publications are increasingly becoming available on-line and offer a prime, if challenging, source of material for IE technology .
The EMPathIE project in which we are currently involved is exploring the possibility of building an Enzyme and Metabolic Pathways database using IE techniques to fill in templates about enzymes and enzyme activities from electronic versions of relevant biomolecular journals (5).
Cowie's work on wild flower guides (5) and Zarri's work on historical texts (5) are early examples of this sort of work.
Employment The TREE project aims to build a database of employment opportunities from electronic job advertisements (5;5).
Fault Diagnosis The SINTESI project extracts information from reports of car faults (5); the TACITUS system was also employed in analysing engine failure reports (5;5).
Software system requirements specification NLP techniques have been used to assist in the process of deriving formal software specifications from less formal, natural language specifications.
We are currently involved in research to see if this problem can be cast in the form of an IE problem, where the formal specification is viewed as a template which needs to be filled from a natural language specification, supplemented with a dialogue with the user.
Together these applications demonstrate the broad range of projects already undertaken or in progress which utilise IE technology.
Clearly they represent but a tiny fraction of potential applications --which supports our claim to the importanceof IE as a growth text processing technology. 
We hope the foregoing discussion has illuminated the objectives of IE, the as yet brief history of this area of research, the sorts of approaches that are being used, and the areas of application which have been and are being considered.
In concluding we focus on a number of central challenges facing IE in the future.
Combined precision and recall scores for IR systems have rested in the mid-50% range for many years, and it is in this range that current IE systems also find themselves.
While users of IR systems have adapted themselves to these performance levels, it is not clear that for IE applications such levels are acceptable.
Clearly what is tolerable will vary from application to application.
But where IE applications involve building databases over extended periods of time which subsequently form the input to further analysis, noise in the data will seriously compromise its utility.
Cowie and Lehnert (5) suggest that 90% precision will be necessary for IE systems to satisfy information analysts.
Currenthigh precision scores in the MUC scenario extraction tasks are around 70%.
Improvements in both precision and recall are high priority challenges for IE systems.
There are nòmagic bullets' on the horizon, but there is every reason to believe that significant progress can be made as research continues in NLP and asmore lexical and grammatical resources become available.
Currently IE systems are tailored for new applications through a two stage processwhich Information Extraction:Beyond Document Retrievalinvolves first defining a template for the application --identifying the entities, attributes and relations to be captured --and second modifying the lexical,grammatical and conceptual rule-bases that the IE system uses in carrying out its text processing.
Both of these stages typically require the involvement of experts.
The first requires a logical analysis of the information to be captured and the articulation of this analysis in a particular formalism.
Given that the second stage of the customisation is highly dependent on this first stage and will require considerable effort, it is important that this stage be carried out correctly and giventhe current development of the technology this is only probable if the person defining the template has a good grasp of the nature and limits of IE systems.
The second stage of customisation --modifying the lexical, grammatical and conceptual rule-bases that the IE system uses in carrying out its text processing --clearly requires expert knowledge.
If these rule-bases are handcrafted, then those with the knowledge to do the handcrafting --typically computational linguists or NLP experts -must perform the customisation for each new domain.
If the rule-bases are not handcrafted , but acquired from corpora, then the corpora must be carefully selected, perhaps annotated, and the rule acquisition process monitored carefully.
Thus porting IE systems to new domains is a serious bottleneck for state-of-the-art systems.
As a consequence, the development of IE technology that permits users to define the extraction task and then adapts to the new scenario is a major challenge: only with the development of such user-centred, adaptive systems is IE technology likely to become of utility to information gathers other than those who can afford to dedicate months of expensive customisation effort to the task.
Some progress has been made in this direction.
The final MUC-6 scenario task was only given to participants one month before the evaluation in an effort toreward highly portable systems.
SRA have begun developing tools to help users define templates through examples (5).
Morgan et al.
(5) have also experimented with various techniques to allow users to customise the Lolita system for new IE tasks.
IE need not be considered a standalone technology which is of use only for applications in which a structured database is to be created from a text corpus.
There are a number of other technologies with which it might be combined to yield powerful new information gathering capabilities.
Information Retrieval The TIPSTER programme from the very start conceived of IR and IE asnaturally forming two stages of a coupled information gathering effort, referring to them as detection and extraction respectively.
The assumption was that an initial user query would be given to an IR system which from a potentially massive document collection would detect the relevant documents to be passed on to an IE system for the more detailed and computationally intensive analysis that such systems carry out.
While this coupling was initially conceived of in the context of the massive electronic document collections being assembled by governments and other large organisations, the arrival of the WWW has made available a document collection whose size threatens to dwarf anything the TIPSTER convenors conceived of as little as five years ago.
Despite the natural complementarity of IR and IE we are not aware of much practical work which has gone on in this direction as yet.
We have done some preliminary experimental work in using Web search engines to create document collections which are then processed by the LaSIE system, and are encouraged by the results (5;5).
However much more work needs to be done in this area, and no doubt will be.
Aside from this obvious way of combining IR and IE systems, there are otherpossible ways in which the two technologies may be of mutual benefit.
Specifically, for applications where the computational intensiveness of IE systems isnot a drawback, an IE system could be used in conjunction with the indexing component of an IR system in one of a number of ways.
Most obviously, the proper name recognition and classification abilities of an IE system could be harnessed to provide useful (possibly) multi-word, preclassified index terms that would enable searches for, e.g., `Ford' the company, and exclude all references to persons and places named`Fordnamed`Ford'.
But more sophisticated indexing could be developed based on the identification of entities and relations, such as IE systems carry out.
For example, remaining with the management succession scenario, one could index documents according to succession events and roles in them so that one could search for all reports mentioning persons who had resigned from CEO positions in Canadian companies in the last year.
Work on using IE templates for indexing legal documents is implemented in the Navilex system (5); work on usingIE techniques to supplement traditional IR approaches to categorising and filtering news stories is being carried out in the related COBALT and FACILE projects, as mentioned above in section 2.3.
Clearly there are many further potential applications of this nature.
Figure 1showed the NL summarythe LaSIE system generated from the template it had extracted.
This summary was generated using very crude generation techniques.
Given that much more sophisticated NL generation (NLG) capabilities now exist (5), the coupling of IE and NLG should permit more fluid, easy to read summaries to be generated from extracted templates.
Natural Language Generation Our example in Machine Translation The translation of documents may be carried out for many reasons, but if the purpose of the translation is to enable subsequent extraction of information from the text that was previously inaccessible to the information seeker because of the language barrier, then given the difficulty of translation it isworth considering ways in which the information sought could be first extracted and then translated .
That is, rather than performing translation followed by extraction, it may be preferable to perform extraction in the source language followed by translation into the destination language.
Such a coupling of IE and MT technologies is particularly attractive because a template, being regularised provides a much easier information source to translate than a full text.
Some work along these lines has already been carried out (5;5) but we expectmuch more work to be carried out in this area in the near future.
Again, given the sudden availability of multilingual on-line text afforded by the Web, information gatherers will want ways of accessing this information that avoid the overheads of large scale trans- lation.
Data Mining IE systems produce structured data repositories which can be turned into conventional databases to be accessed with conventional database access tools such as SQL query processors.
However, these databases may also be processed bydata mining (DM) or knowledge discovery in database (KDD) tools which seek novel patterns in the data (5).
The significance of coupling IE with DM or KDD techniques is that this will permit hitherto unmined text resources to become the subject of extensive exploration.
As one example, consider the possibilities of extracting information about commodity price changes from financial news reports, building a database of these fluctuations over some historical period and then usingKDD techniques to discover correlations that might give insights into the causes ofthese changes.
Once again, coupling IE with another technology promises powerful new techniques for gathering information from texts.
An important insight, even after accepting our argument that IE is a new, emergent technology, is that what may seem to be wholly separate information technologies are really not so: MT and IE, for example, are just two ways of producing information to meet people's needs and can be combined in differing ways: for example, one could translate a document and then perform IE against the result or vice-versa, which would mean just translating the contents of the resulting templates.
Which of these one chose to do might depend on the relativestrengths of the translation systems available: a simpler one might only be adequateto translate the contents of templates, and so on.
This last observation emphasizesthat the product of an IE system --the filled templates --can be seen either as acompressed, or summarised, text itself, or as a form of data base (with the fillers of the template slots corresponding to conventional database fields).
One can then imagine new, learning, techniques like data mining being done as a subsequent stage on the results of IE itself.
If we think along these lines we see that the first distinction of this paper, between traditional IR and the newer IE, is not totally clear everywhere but can itself become a question of degree.
Suppose parsing systems that produce syntactic and logical representations were so good, as some now believe, that they could process huge corpora in an acceptably short time.
One can then think of the traditional task of computer question answering in two quite different ways.
The old way was to translate a question into a formalised language like SQL and use it to retrieve information from a database --as in `Tell me all the IBM executivesover 40 earning under % 50K a year'.
But with a full parser of large corpora one could now imagine transforming the query to form an IE template and searching the whole text (not a data base) for all examples of such employees --both methods should produce exactly the same result starting from different information sources --a text versus a formalised database.
What we have called an IE template can now be seen as a kind of frozen query that one can reuse many times on a corpus and is therefore only important when one wants stereotypical, repetitive, information back rather than the answer toone-off questions.
Tell me the height of Everest, as a question addressed to a formalised text corpus is then neither IR nor IE but a perfectly reasonable single request for an answer.
`Tell me about fungi', addressed to a text corpus with an IR system, will produce a set of relevant documents but no particular answer.
`Tell me what films my favourite movie critic likes', addressed to the right text corpus, is undoubtedly IE, and will produce an answer also.
The needs and the resources available determine the techniques that are relevant, and those in turn determine what it is to answer a question as opposed to providing information in a broader sense. 
